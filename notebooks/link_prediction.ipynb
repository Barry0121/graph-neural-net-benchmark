{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard pacakges \n",
    "import torch \n",
    "from torch import nn, utils\n",
    "import networkx as nx \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Pytroch Geometric \n",
    "from torch_geometric import utils as gutils\n",
    "from torch_geometric import nn as gnn # import layers \n",
    "from torch_geometric.datasets import Planetoid # import dataset CORA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: Model Training/Evaluation Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_utils: \n",
    "    def __init__(self, dataset, epochs): \n",
    "        # store the data\n",
    "        # TODO: Change the raw dataset to a dataloader object from PyTorch\n",
    "        self.dataset = dataset  \n",
    "        if 'x' in self.dataset:\n",
    "            self.node_features = self.dataset.x\n",
    "        else: \n",
    "            print(\"Input Dataset has no node features.\")\n",
    "        self.edge_index = self.dataset.edge_index\n",
    "        self.node_labels = self.dataset.y \n",
    "\n",
    "        # print some dataset statistics \n",
    "        print(f'Number of nodes: {dataset.num_nodes}')\n",
    "        print(f'Number of edges: {dataset.num_edges}')\n",
    "        print(f'Average node degree: {dataset.num_edges / dataset.num_nodes:.2f}')\n",
    "        if 'train_mask' in dataset:\n",
    "            print(f'Number of training nodes: {dataset.train_mask.sum()}')\n",
    "            print(f'Training node label rate: {int(dataset.train_mask.sum()) / dataset.num_nodes:.2f}')\n",
    "        print(f'Has isolated nodes: {dataset.has_isolated_nodes()}')\n",
    "        print(f'Has self-loops: {dataset.has_self_loops()}')\n",
    "        print(f'Is undirected: {dataset.is_undirected()}')\n",
    "\n",
    "\n",
    "        # training/validation split\n",
    "\n",
    "        # Hyperparameters \n",
    "        self.epochs = epochs\n",
    "        self.train_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.test_loss = 0 \n",
    "        self.validation_acc = []\n",
    "        self.test_acc = 0\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    Utility functions: \n",
    "    - load dataset \n",
    "    - loss function \n",
    "    - optimizer \n",
    "    - train/validation\n",
    "    - test\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize_training(self): \n",
    "        \"\"\" Initialize Training Utilities \"\"\"\n",
    "        pass \n",
    "\n",
    "    def train_step(self): \n",
    "        \"\"\" One Training Step \"\"\"\n",
    "        pass\n",
    "\n",
    "    def test(self): \n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom GCN Layer for weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConvCustom(nn.Module):\n",
    "    def __init__(self, \n",
    "                edge_index,\n",
    "                # node_batch,\n",
    "                input_dim, \n",
    "                output_dim, \n",
    "                random_init=True, \n",
    "                with_bias=False,\n",
    "                device='cuda:0' if torch.cuda.is_available() else 'mps'):\n",
    "        super(GCNConvCustom, self).__init__()\n",
    "        # print(\"layer initialized\")\n",
    "\n",
    "        \"\"\"Metadata\"\"\"\n",
    "        self.device = device # initialize the hosting device\n",
    "        self.with_bias = with_bias\n",
    "\n",
    "        \"\"\"Calculate Matrices\"\"\"\n",
    "        # the adjacency matrix with self-loop \n",
    "        \n",
    "        self.A = gutils.to_dense_adj(edge_index).to(self.device)[0]\n",
    "        self.A_self = self.A + torch.diag(torch.ones(self.A.shape[0], device=self.device))\n",
    "        # print(\"Adj Matrix with self loop: \", self.A)\n",
    "        \n",
    "        # calculate the degree matrix with A after added self loop\n",
    "        self.D = torch.sum(self.A_self, dim=0).to(self.device)  # Note: these are the elements along the diagonal of D\n",
    "        # print(\"Degree Matrix: \", self.D)\n",
    "\n",
    "        # for diagonal matrix, raising it to any power is the same as raising its diagonal elements to that power\n",
    "        # we can just apply the -1/2 power to all element of this degree matrix \n",
    "        # self.D_half_norm = torch.reciprocal(torch.sqrt(self.D)) \n",
    "        # self.D_half_norm = torch.from_numpy(fractional_matrix_power(self.D, -0.5)).to(self.device)\n",
    "        self.D_half_norm = torch.diag(torch.pow(self.D, -0.5))\n",
    "        # print(\"Normalization Matrix: \", self.D_half_norm)\n",
    "\n",
    "        # normalized adjacency matrix\n",
    "        # self.A_s = torch.mm(torch.mm(self.D_half_norm, self.A), self.D_half_norm) \n",
    "        self.A_s = self.D_half_norm @ self.A_self @ self.D_half_norm\n",
    "        self.A_s = self.A_s.to(self.device)\n",
    "        # print(self.A_s.shape)\n",
    "        \n",
    "        # initialize learnable weights\n",
    "        # the weight should have shape of (N , F) where N is the size of the input, and F is the output dimension\n",
    "        self.W, self.b = None, None\n",
    "        if random_init: \n",
    "            self.W = torch.nn.Parameter(\n",
    "                data=(torch.rand(input_dim, output_dim, device=self.device) * 0.01),  # times it by 0.001 to make the weight smaller\n",
    "                requires_grad=True\n",
    "            )\n",
    "            # create trainable a bias term for the layer\n",
    "            self.b = torch.nn.Parameter(\n",
    "                data=(torch.rand(output_dim, 1, device=self.device) * 0.01),\n",
    "                requires_grad=True\n",
    "            )\n",
    "        else: \n",
    "            self.W = torch.nn.Parameter(\n",
    "                data=torch.ones(input_dim, output_dim, device=self.device), \n",
    "                requires_grad=True\n",
    "            )\n",
    "            self.b = torch.nn.Parameter(\n",
    "                data=torch.ones(output_dim, 1, device=self.device), \n",
    "                requires_grad=True\n",
    "            )\n",
    "\n",
    "    def forward(self, H):\n",
    "        if self.with_bias: \n",
    "            return self.A_s @ H @ self.W + self.b.T\n",
    "        else: \n",
    "            return self.A_s @ H @ self.W\n",
    "\n",
    "    def get_adj_matrix(self, with_self=False):\n",
    "        if with_self: \n",
    "            return self.A_self \n",
    "        return self.A\n",
    "    \n",
    "    def get_normalized_adj_matrix(self): \n",
    "        return self.A_s\n",
    "\n",
    "    def get_degree_matrix(self, normalization=False): \n",
    "        if normalization: \n",
    "            return self.D_half_norm\n",
    "        return self.D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporally data reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads Cora dataset, at root location: ../data/raw/Planetoid\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "def loader_cora_torch(filepath=\"../data/raw/Planetoid\", transform=None, batch_size=1, shuffle=False, device='cuda:0' if torch.cuda.is_available() else 'mps'):\n",
    "    \"\"\"Return the CORA dataset\"\"\"\n",
    "    dataset = Planetoid(root=filepath, name='Cora', split='public', num_train_per_class=20, num_val=500, num_test=1000, transform=transform) # return a class of datasets\n",
    "    data = dataset[0]\n",
    "    # print some dataset statistics \n",
    "    print(f'Loads Cora dataset, at root location: {filepath}')\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    return data.edge_index.to(device), data.x.to(device), data.y.to(device)\n",
    "\n",
    "\n",
    "edge_index, node_features, labels = loader_cora_torch(shuffle=True) # load the cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[45.2056, 45.2056, 45.2056, 45.2056, 45.2056],\n",
       "        [33.8967, 33.8967, 33.8967, 33.8967, 33.8967],\n",
       "        [46.2932, 46.2932, 46.2932, 46.2932, 46.2932],\n",
       "        ...,\n",
       "        [10.0000, 10.0000, 10.0000, 10.0000, 10.0000],\n",
       "        [47.9280, 47.9280, 47.9280, 47.9280, 47.9280],\n",
       "        [50.5896, 50.5896, 50.5896, 50.5896, 50.5896]], device='cuda:0',\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test custom GCN Layer \n",
    "layer1 = GCNConvCustom(edge_index=edge_index, input_dim=node_features.shape[0], output_dim=10, random_init=False)\n",
    "output1 = layer1(layer1.get_adj_matrix())\n",
    "layer2 = GCNConvCustom(edge_index=edge_index, input_dim=10, output_dim=5, random_init=False)\n",
    "layer2(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of 2 GCN Convolution Layers Auto-encoder\n",
    "reference: \n",
    "* https://towardsdatascience.com/tutorial-on-variational-graph-auto-encoders-da9333281129 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_AE(nn.Module): \n",
    "    \"\"\"\n",
    "    Graph Convolutional Auto-Encoder\n",
    "\n",
    "    GCN layer implementation from: \n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                edge_index, \n",
    "                input_size, \n",
    "                hidden_size_1,\n",
    "                hidden_size_2,\n",
    "                encoding_size,\n",
    "                device= 'cuda:0' if torch.cuda.is_available() else 'mps'): \n",
    "        super().__init__()\n",
    "        # meta information\n",
    "        self.device = device\n",
    "        self.edge_index = edge_index # same as edge list, replace adjacency matrix \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.encoding_size = encoding_size\n",
    "\n",
    "        # training utilities \n",
    "        self.criterion = None \n",
    "        self.optimizer = None \n",
    "        \n",
    "        # layers \n",
    "        self.GCN_1 = GCNConvCustom(edge_index=self.edge_index, input_dim=self.input_size, output_dim=self.hidden_size_1, random_init=True, device=self.device) \n",
    "        self.GCN_2 = GCNConvCustom(edge_index=self.edge_index, input_dim=self.hidden_size_1, output_dim=self.hidden_size_2, random_init=True, device=self.device)\n",
    "        self.FC = nn.Linear(in_features=self.hidden_size_2, out_features=self.encoding_size, device=self.device)\n",
    "\n",
    "        # activations \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encoder(self, X): \n",
    "        X_hat = self.GCN_1(X) # first layer: lower dimension feature matrix\n",
    "        X_hat = self.relu(X_hat) \n",
    "        H = self.GCN_2(X_hat) # second layer: mean matrix\n",
    "        H = self.relu(H)\n",
    "        Z = self.FC(H)\n",
    "        Z = self.relu(Z) # this activation may or may not be here, doesn't make a difference  \n",
    "        return Z\n",
    "\n",
    "    def decoder(self, Z): \n",
    "        # TODO: we don't want matrix product, but inner product of each encoded vector \n",
    "        Y_inner = torch.mm(Z.T, Z) # calculate inner product of matrix \n",
    "        # Y_inner = Y_inner.reshape((-1)) # flatten the tensor \n",
    "        Y = self.sigmoid(Y_inner) # apply activation \n",
    "        return Y\n",
    "\n",
    "    def forward(self, X): \n",
    "        Z = self.encoder(X)\n",
    "        output = self.decoder(Z)\n",
    "        return output\n",
    "\n",
    "    def set_optimizer(self, optimizer): \n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def set_loss(self, criterion): \n",
    "        self.criterion = criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of 2 GCN Convolution Layers Variational Auto-encoder \n",
    "references: \n",
    "* https://towardsdatascience.com/tutorial-on-variational-graph-auto-encoders-da9333281129 \n",
    "* https://github.com/tkipf/gae/blob/0ebbe9b9a8f496eb12deb9aa6a62e7016b5a5ac3/gae/model.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_VAE(nn.Module): \n",
    "    \"\"\"\n",
    "    Graph Variational Convolutional Auto-Encoder\n",
    "\n",
    "    GCN layer implementation from: \n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                edge_index, \n",
    "                input_size, \n",
    "                hidden_size,\n",
    "                encoding_size,\n",
    "                output_size, \n",
    "                device= 'cuda:0' if torch.cuda.is_available() else 'mps'): \n",
    "        super().__init__()\n",
    "        # meta information\n",
    "        self.device = device\n",
    "        self.edge_index = edge_index # same as edge list, replace adjacency matrix \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.encoding_size = encoding_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # layers \n",
    "        self.GCN_1 = gnn.GCNConv(in_channels=self.input_size, out_channels=self.hidden_size, normalize=False, device=self.device) \n",
    "        self.GCN_2 = gnn.GCNConv(in_channels=self.hidden_size, out_channels=self.encoding_size, normalize=False, device=self.device)\n",
    "\n",
    "        # activations \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encoder(self, X): \n",
    "        X_hat = self.GCN_1(X, self.edge_index) # first layer: lower dimension feature matrix \n",
    "        X_hat = self.relu(X_hat)\n",
    "        Z_mean = self.GCN_2(X_hat, self.edge_index) # second layer: mean matrix\n",
    "        Z_logstd = self.GCN_2(X_hat, self.edge_index) # second layer: natural log of squared standard deviation\n",
    "        Z_std = torch.sqrt(torch.exp(Z_logstd)) # calculate the standard deviation \n",
    "        Z = torch.normal(Z_mean, Z_std) # random sample from normal distribution with the calculated mean and std\n",
    "        return Z\n",
    "\n",
    "    def decoder(self, Z): \n",
    "        Y_inner = torch.mm(Z.T, Z) # calculate inner product of matrix \n",
    "        Y_inner = Y_inner.reshape((-1)) # flatten the tensor \n",
    "        Y = self.sigmoid(Y_inner) # apply activation \n",
    "        return Y\n",
    "\n",
    "    def forward(self, X): \n",
    "        Z = self.encoder(X)\n",
    "        output = self.decoder(Z)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Create two 4-node graphs of your choice by hand on paper with node labels (but no attributes). Begin with a 2-hidden layer GCN with weight matrics initialized at 1. Calculate the feed-forward output of the graph by hand. Compare it to your results using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = nx.Graph()\n",
    "graph.add_nodes_from([1,2,3,4])\n",
    "graph.add_edges_from([(1,2), (2, 3), (1, 4), (4, 2), (3, 1)])\n",
    "nx.draw(graph, labels={i:i for i in range(1, 5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the 2-hidden layer GCN-AE by hand. \n",
    "\n",
    "1. Adjacency Matrix \n",
    "    * Node 1 = [0, 1, 1, 1]\n",
    "    * Node 2 = [1, 0, 1, 1]\n",
    "    * Node 3 = [1, 1, 0, 0]\n",
    "    * Node 4 = [1, 1, 0, 0]\n",
    "$$ \n",
    "\\LARGE{A} = \\begin{pmatrix}\n",
    "            0 & 1 & 1 & 1 \\\\\n",
    "            1 & 0 & 1 & 1 \\\\\n",
    "            1 & 1 & 0 & 0 \\\\\n",
    "            1 & 1 & 0 & 0\n",
    "            \\end{pmatrix} \\\\\n",
    "\n",
    "\\text{ with self loop } = \\begin{pmatrix}\n",
    "            1 & 1 & 1 & 1 \\\\\n",
    "            1 & 1 & 1 & 1 \\\\\n",
    "            1 & 1 & 1 & 0 \\\\\n",
    "            1 & 1 & 0 & 1\n",
    "            \\end{pmatrix} \n",
    "$$ \n",
    "We want to normalize the adjacency matrix by multiplying $D^{-\\frac{1}{2}}$ on each sides: \n",
    "$$\n",
    "\\LARGE{D} = \\begin{pmatrix} \n",
    "            4 & 0 & 0 & 0 \\\\\n",
    "            0 & 4 & 0 & 0 \\\\\n",
    "            0 & 0 & 3 & 0 \\\\\n",
    "            0 & 0 & 0 & 3\n",
    "            \\end{pmatrix} \\\\\n",
    "\\LARGE{D}^{-\\frac{1}{2}} = \\begin{pmatrix} \n",
    "            \\frac{1}{\\sqrt{4}} & 0 & 0 & 0 \\\\\n",
    "            0 & \\frac{1}{\\sqrt{4}} & 0 & 0 \\\\\n",
    "            0 & 0 & \\frac{1}{\\sqrt{3}} & 0 \\\\\n",
    "            0 & 0 & 0 & \\frac{1}{\\sqrt{3}}\n",
    "            \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\LARGE{\\hat{A}} = \\LARGE{D}^{-\\frac{1}{2}} \\LARGE{A} \\LARGE{D}^{-\\frac{1}{2}} \\\\\n",
    "    =   \\begin{pmatrix}\n",
    "        \\frac{1}{4} &  \\frac{1}{4} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} \\\\\n",
    "        \\frac{1}{4} &  \\frac{1}{4} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} \\\\\n",
    "        \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{3} & 0 \\\\\n",
    "        \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & 0 & \\frac{1}{3}\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "2. Feature Matrix\n",
    "    * We didn't specify any node feature, so this is going to be the identity matrix. \n",
    "\n",
    "3. First GCN layer (dim: 4 -> 2)\n",
    "    * Weight Matrix: $$\\LARGE{W_1} = \\begin{pmatrix}\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \n",
    "            \\end{pmatrix}$$\n",
    "    * Full Equation: $$ ReLU(\\hat{A}XW) = ReLU(\n",
    "            \\begin{pmatrix}\n",
    "            \\frac{1}{4} &  \\frac{1}{4} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} \\\\\n",
    "            \\frac{1}{4} &  \\frac{1}{4} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} \\\\\n",
    "            \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{3} & 0 \\\\\n",
    "            \\frac{1}{\\sqrt{3}\\sqrt{4}} & \\frac{1}{\\sqrt{3}\\sqrt{4}} & 0 & \\frac{1}{3}\n",
    "            \\end{pmatrix} \\LARGE{X}\n",
    "            \\begin{pmatrix}\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \\\\\n",
    "            1 & 1 \n",
    "            \\end{pmatrix})$$\n",
    "    * Each node's representation after passing through this layer: \n",
    "        1. [1.07735027, 1.07735027]\n",
    "        2. [1.07735027, 1.07735027]\n",
    "        3. [0.9106836 , 0.9106836]\n",
    "        4. [0.9106836 , 0.9106836]\n",
    "\n",
    "4. Second GCN layer (dim: 2 -> 1)\n",
    "    * Weight Matrix: $$\\LARGE{W_2} = \\begin{pmatrix}\n",
    "            1 \\\\\n",
    "            1 \n",
    "            \\end{pmatrix}$$\n",
    "    * Each node's representation after passing through this layer: \n",
    "        1. [2.12891712]\n",
    "        2. [2.12891712] \n",
    "        3. [1.85113934]\n",
    "        4. [1.85113934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1289],\n",
       "        [2.1289],\n",
       "        [1.8511],\n",
       "        [1.8511]], device='cuda:0', grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the encoding in a 2 layer GCN \n",
    "edge_index = torch.tensor(list(graph.to_directed().edges)).T-1\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'mps'\n",
    "layer1 = GCNConvCustom(edge_index=edge_index, input_dim=4, output_dim=2, random_init=False, with_bias=False)\n",
    "output1 = layer1(torch.eye(4).to(device))\n",
    "layer2 = GCNConvCustom(edge_index=edge_index, input_dim=2, output_dim=1, random_init=False, with_bias=False)\n",
    "layer2(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "Link Prediction with GCN-AE\n",
    "\n",
    "Questions: \n",
    "* What is the loss function for the autoencoder and why is the setup appropriate for link-prediction? \n",
    "    - The idea of using a graph autoencoder for link prediction is to randomly take out part of the graph, and use the remaining portion of the graph to predict the removed portion of the graph. \n",
    "    - This process is similar to the encoding-decoding procedure of an autoencoder. The encoding is the model extracting information form the training portion of the graph. The decoding is the model reconstructing the connection of the test portion of the graph. \n",
    "    - The loss function of the autoencoder calculates the reconstruction loss, i.e. how different the reconstruction of the decoder is from the ground truth. One common approach is to use the MSE between the decoded tensor and the ground truth. \n",
    "\n",
    "* What if you wanted to take node attributes into account as well? \n",
    "    - For GCNs, the node attributes can just be passed into it as part of the encoding features, so there shouldn't be a difference in the procedure from the procedure without the node attributes. \n",
    "\n",
    "* How might you change the loss function?\n",
    "    - Although link prediction problem can be understood as a reconstruct problem, it is also a classification problem. For each pair of nodes, there can only be two possibilities, link or no link. This indicates that we can use binary cross-entropy as the loss function, instead of mean square error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads Cora dataset, at root location: ../data/raw/Planetoid\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "\n",
      "            There are a total of 10556. \n",
      "\n",
      "            Train dataset has 5278 edges. \n",
      "\n",
      "            Validation dataset has 2111 edges. \n",
      "\n",
      "            Test dataset has 3167 edges. \n",
      "          \n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::index.Tensor' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m train_matrix\u001b[39m.\u001b[39mto(device), validation_matrix\u001b[39m.\u001b[39mto(device), test_matrix\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m edge_index, node_features, labels \u001b[39m=\u001b[39m loader_cora_torch(shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# load the cora dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m train_subset, validation_subset, test_subset \u001b[39m=\u001b[39m extract_train_test_data(edge_index)\n",
      "\u001b[1;32m/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb Cell 18\u001b[0m in \u001b[0;36mextract_train_test_data\u001b[0;34m(edge_index, node_features, labels, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m randomize_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(num_edges)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m train_indices, val_indices, test_indices \u001b[39m=\u001b[39m randomize_indices[:train_size], randomize_indices[train_size:train_size\u001b[39m+\u001b[39mval_size], randomize_indices[train_size\u001b[39m+\u001b[39mval_size:]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_matrix, validation_matrix, test_matrix \u001b[39m=\u001b[39m edge_index[:, train_indices], edge_index[:, val_indices], edge_index[:, test_indices] \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/barryxue/Desktop/dsc180-graph-neural-net/notebooks/link_prediction.ipynb#X23sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_matrix\u001b[39m.\u001b[39mto(device), validation_matrix\u001b[39m.\u001b[39mto(device), test_matrix\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::index.Tensor' is not current implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# data preparation \n",
    "\"\"\"\n",
    "Strategy: \n",
    "* Randomly sample 50% edges (training), 20% edges (validation), and 30% edges (testing). \n",
    "* We want to evaluate against the edges that are uniformly distributed connected to each nodes, \n",
    "    so create a couple of random sampled train, validation, test datasets, evaluate the performance against each. \n",
    "\"\"\"\n",
    "def extract_train_test_data(edge_index, node_features=None, labels=None, device='cuda:0' if torch.cuda.is_available() else 'mps'): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        edge_index: torch.Tensor, shape should be (2, # of edges in graph)\n",
    "        node_features: troch.Tensor, shape should be (# of nodes, feature dimension)\n",
    "        labels: torch.Tensor, shape should eb (# of nodes, )\n",
    "        \n",
    "    Return\n",
    "        datasets: list, contain randomly sampled (train_matrix, validation_matrix, test_matix) dataset; same length as the number of trials specified. \n",
    "            train_matrix: sub-graph created by sampling 50% of the edges from the original graph \n",
    "            validation_matix: sub-graph created by sampling 20% of the edges from the original graph \n",
    "            test_matrix: sub-graph of the rest of the 30% of the original graph\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.shape[1] \n",
    "    train_size, val_size = int(0.5*num_edges), int(0.2*num_edges)\n",
    "    test_size = num_edges - train_size - val_size\n",
    "    print(f\"\"\"\n",
    "            There are a total of {num_edges}. \\n\n",
    "            Train dataset has {train_size} edges. \\n\n",
    "            Validation dataset has {val_size} edges. \\n\n",
    "            Test dataset has {test_size} edges. \n",
    "          \"\"\")\n",
    "   \n",
    "    randomize_indices = torch.randperm(num_edges)\n",
    "    train_indices, val_indices, test_indices = randomize_indices[:train_size], randomize_indices[train_size:train_size+val_size], randomize_indices[train_size+val_size:]\n",
    "    train_matrix, validation_matrix, test_matrix = edge_index[:, train_indices], edge_index[:, val_indices], edge_index[:, test_indices] \n",
    "   \n",
    "    return train_matrix.to(device), validation_matrix.to(device), test_matrix.to(device)\n",
    "    \n",
    "edge_index, node_features, labels = loader_cora_torch(shuffle=True, device='cuda:0' if torch.cuda.is_available() else 'mps') # load the cora dataset\n",
    "train_subset, validation_subset, test_subset = extract_train_test_data(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_AE(edge_index=train_subset, input_size=node_features.shape[0], hidden_size_1=30, hidden_size_2=10, encoding_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.set_optimizer(optimizer=optimizer)\n",
    "model.set_loss(criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = model(torch.eye(node_features.shape[0], device='cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.9881, 1.0000, 1.0000, 0.5000],\n",
       "        [0.5000, 1.0000, 1.0000, 1.0000, 0.5000],\n",
       "        [0.5000, 1.0000, 1.0000, 1.0000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.5000]], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300): \n",
    "    # train \n",
    "    model.train(True)\n",
    "    train_running_loss = 0.0\n",
    "    model.optimizer.zero_grad()\n",
    "    output = model()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ee9abc2f8ed4c2bec59380a88532f6c4409e3142704504d9be0d180fda6aa0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
