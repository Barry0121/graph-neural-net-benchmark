{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference website for overview of GCN: [LINK](http://tkipf.github.io/graph-convolutional-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: NumPy Example of GCN on Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Info:\n",
      " Graph named 'G' with 6 nodes and 7 edges\n",
      "\n",
      "Graph Nodes:  [(0, {'name': 0}), (1, {'name': 1}), (2, {'name': 2}), (3, {'name': 3}), (4, {'name': 4}), (5, {'name': 5})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_81637/3724959495.py:22: DeprecationWarning: info is deprecated and will be removed in version 3.0.\n",
      "\n",
      "  print('Graph Info:\\n', nx.info(G))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "#Initialize the graph\n",
    "G = nx.Graph(name='G')\n",
    "\n",
    "#Create nodes\n",
    "#In this example, the graph will consist of 6 nodes.\n",
    "#Each node is assigned node feature which corresponds to the node name\n",
    "for i in range(6):\n",
    "    G.add_node(i, name=i)\n",
    "\n",
    "\n",
    "#Define the edges and the edges to the graph\n",
    "edges = [(0,1),(0,2),(1,2),(0,3),(3,4),(3,5),(4,5)]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "#See graph info\n",
    "print('Graph Info:\\n', nx.info(G))\n",
    "\n",
    "#Inspect the node features\n",
    "print('\\nGraph Nodes: ', G.nodes.data())\n",
    "\n",
    "#Plot the graph\n",
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  (6, 6)\n",
      "\n",
      "Shape of X:  (6, 1)\n",
      "\n",
      "Adjacency Matrix (A):\n",
      " [[0. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0.]]\n",
      "\n",
      "Node Features Matrix (X):\n",
      " [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_81637/784659161.py:2: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_81637/784659161.py:3: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n"
     ]
    }
   ],
   "source": [
    "# Get the Adjacency Matrix (A) and Node Features Matrix (X) as numpy array\n",
    "A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
    "X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n",
    "X = np.expand_dims(X,axis=1)\n",
    "\n",
    "print('Shape of A: ', A.shape)\n",
    "print('\\nShape of X: ', X.shape)\n",
    "print('\\nAdjacency Matrix (A):\\n', A)\n",
    "print('\\nNode Features Matrix (X):\\n', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of A and X (AX):\n",
      " [[6.]\n",
      " [2.]\n",
      " [1.]\n",
      " [9.]\n",
      " [8.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "# Dot product Adjacency Matrix (A) and Node Features (X)\n",
    "AX = np.dot(A,X)\n",
    "print(\"Dot product of A and X (AX):\\n\", AX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dot product of Adjacency Matrix and Node Features Matrix represents the sum of neighboring node features.\n",
    "#### But, if we think about it more, we will realize that while AX sums up the adjacent node features, it does not take into account the features of the node itself.\n",
    "* So we have to insert some self loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges of G with self-loops:\n",
      " [(0, 1), (0, 2), (0, 3), (0, 0), (1, 2), (1, 1), (2, 2), (3, 4), (3, 5), (3, 3), (4, 5), (4, 4), (5, 5)]\n",
      "Adjacency Matrix of added self-loops G (A_hat):\n",
      " [[1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n",
      "AX:\n",
      " [[ 6.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [12.]\n",
      " [12.]\n",
      " [12.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_81637/3005080699.py:14: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])\n"
     ]
    }
   ],
   "source": [
    "# Add Self Loops\n",
    "G_self_loops = G.copy()\n",
    "\n",
    "self_loops = []\n",
    "for i in range(G.number_of_nodes()):\n",
    "    self_loops.append((i,i))\n",
    "\n",
    "G_self_loops.add_edges_from(self_loops)\n",
    "\n",
    "# Check the edges of G_self_loops after adding the self loops\n",
    "print('Edges of G with self-loops:\\n', G_self_loops.edges)\n",
    "\n",
    "# Get the Adjacency Matrix (A) and Node Features Matrix (X) of added self-lopps graph\n",
    "A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])\n",
    "print('Adjacency Matrix of added self-loops G (A_hat):\\n', A_hat)\n",
    "\n",
    "# Calculate the dot product of A_hat and X (AX)\n",
    "AX = np.dot(A_hat, X)\n",
    "print('AX:\\n', AX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Matrix of added self-loops G (D):  [(0, 5), (1, 4), (2, 4), (3, 5), (4, 4), (5, 4)]\n",
      "Degree Matrix of added self-loops G as numpy array (D):\n",
      " [[5 0 0 0 0 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 0 4 0 0 0]\n",
      " [0 0 0 5 0 0]\n",
      " [0 0 0 0 4 0]\n",
      " [0 0 0 0 0 4]]\n",
      "Inverse of D:\n",
      " [[0.2  0.   0.   0.   0.   0.  ]\n",
      " [0.   0.25 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.2  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.25]]\n",
      "DAX:\n",
      " [[1.2 ]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [2.4 ]\n",
      " [3.  ]\n",
      " [3.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data by finding the dot product with degree matrix\n",
    "#Get the Degree Matrix of the added self-loops graph\n",
    "Deg_Mat = G_self_loops.degree()\n",
    "print('Degree Matrix of added self-loops G (D): ', Deg_Mat)\n",
    "\n",
    "#Convert the Degree Matrix to a N x N matrix where N is the number of nodes\n",
    "D = np.diag([deg for (n,deg) in list(Deg_Mat)])\n",
    "print('Degree Matrix of added self-loops G as numpy array (D):\\n', D)\n",
    "\n",
    "#Find the inverse of Degree Matrix (D)\n",
    "D_inv = np.linalg.inv(D)\n",
    "print('Inverse of D:\\n', D_inv)\n",
    "\n",
    "#Dot product of D and AX for normalization\n",
    "DAX = np.dot(D_inv,AX)\n",
    "print('DAX:\\n', DAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADX:\n",
      " [[1.27082039]\n",
      " [0.75      ]\n",
      " [0.75      ]\n",
      " [2.61246118]\n",
      " [2.92082039]\n",
      " [2.92082039]]\n"
     ]
    }
   ],
   "source": [
    "# Symmetrically-normalization\n",
    "D_half_norm = fractional_matrix_power(D, -0.5)\n",
    "DADX = D_half_norm.dot(A_hat).dot(D_half_norm).dot(X)\n",
    "print('DADX:\\n', DADX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Weights and Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Representation from GCN output:\n",
      " [[0.00027758 0.        ]\n",
      " [0.00017298 0.        ]\n",
      " [0.00017298 0.        ]\n",
      " [0.00053017 0.        ]\n",
      " [0.00054097 0.        ]\n",
      " [0.00054097 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the weights\n",
    "np.random.seed(77777)\n",
    "n_h = 4 #number of neurons in the hidden layer\n",
    "n_y = 2 #number of neurons in the output layer\n",
    "W0 = np.random.randn(X.shape[1],n_h) * 0.01\n",
    "W1 = np.random.randn(n_h,n_y) * 0.01\n",
    "\n",
    "#Implement ReLu as activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "#Build GCN layer\n",
    "#In this function, we implement numpy to simplify\n",
    "def gcn(A,H,W):\n",
    "    I = np.identity(A.shape[0]) #create Identity Matrix of A\n",
    "    A_hat = A + I # add self-loop to A\n",
    "    D = np.diag(np.sum(A_hat, axis=0)) #create Degree Matrix of A\n",
    "    D_half_norm = fractional_matrix_power(D, -0.5) #calculate D to the power of -0.5\n",
    "    eq = D_half_norm.dot(A_hat).dot(D_half_norm).dot(H).dot(W)\n",
    "    return relu(eq)\n",
    "\n",
    "\n",
    "#Do forward propagation\n",
    "H1 = gcn(A,X,W0) # Layer one \n",
    "H2 = gcn(A,H1,W1) # Layer two\n",
    "print('Features Representation from GCN output:\\n', H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_features(H2):\n",
    "    #Plot the features representation\n",
    "    x = H2[:,0]\n",
    "    y = H2[:,1]\n",
    "\n",
    "    size = 1000\n",
    "\n",
    "    plt.scatter(x,y,size)\n",
    "    plt.xlim([np.min(x)*0.9, np.max(x)*1.1])\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.xlabel('Feature Representation Dimension 0')\n",
    "    plt.ylabel('Feature Representation Dimension 1')\n",
    "    plt.title('Feature Representation')\n",
    "\n",
    "    for i,row in enumerate(H2):\n",
    "        str = \"{}\".format(i)\n",
    "        plt.annotate(str, (row[0],row[1]),fontsize=18, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_features(H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement a 1 Layer GCN to Classify Node Label on CORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.linalg import fractional_matrix_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5429, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1050679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1103960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   source\n",
       "0      35     1033\n",
       "1      35   103482\n",
       "2      35   103515\n",
       "3      35  1050679\n",
       "4      35  1103960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify data source  \n",
    "core_fp_edges, core_fp_nodes = \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\"\n",
    "\n",
    "# use the edges to create a graph; store the content to each nodes \n",
    "edges_df = pd.read_csv(core_fp_edges, names=[\"target\", \"source\"], header=None, delimiter='\\t')\n",
    "print(edges_df.shape)\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networkx graph object with edgelist \n",
    "cora_g = nx.from_pandas_edgelist(edges_df)\n",
    "# nx.draw(cora_g, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_1424</th>\n",
       "      <th>word_1425</th>\n",
       "      <th>word_1426</th>\n",
       "      <th>word_1427</th>\n",
       "      <th>word_1428</th>\n",
       "      <th>word_1429</th>\n",
       "      <th>word_1430</th>\n",
       "      <th>word_1431</th>\n",
       "      <th>word_1432</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37879</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_0  word_1  word_2  word_3  word_4  word_5  word_6  word_7  \\\n",
       "31336         0       0       0       0       0       0       0       0   \n",
       "1061127       0       0       0       0       0       0       0       0   \n",
       "1106406       0       0       0       0       0       0       0       0   \n",
       "13195         0       0       0       0       0       0       0       0   \n",
       "37879         0       0       0       0       0       0       0       0   \n",
       "\n",
       "         word_8  word_9  ...  word_1424  word_1425  word_1426  word_1427  \\\n",
       "31336         0       0  ...          0          0          1          0   \n",
       "1061127       0       0  ...          0          1          0          0   \n",
       "1106406       0       0  ...          0          0          0          0   \n",
       "13195         0       0  ...          0          0          0          0   \n",
       "37879         0       0  ...          0          0          0          0   \n",
       "\n",
       "         word_1428  word_1429  word_1430  word_1431  word_1432  \\\n",
       "31336            0          0          0          0          0   \n",
       "1061127          0          0          0          0          0   \n",
       "1106406          0          0          0          0          0   \n",
       "13195            0          0          0          0          0   \n",
       "37879            0          0          0          0          0   \n",
       "\n",
       "                        subject  \n",
       "31336           Neural_Networks  \n",
       "1061127           Rule_Learning  \n",
       "1106406  Reinforcement_Learning  \n",
       "13195    Reinforcement_Learning  \n",
       "37879     Probabilistic_Methods  \n",
       "\n",
       "[5 rows x 1434 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the features of each paper \n",
    "columns = [f\"word_{i}\" for i in range(1433)] + [\"subject\"]\n",
    "features_df = pd.read_csv(core_fp_nodes,names=columns, delimiter='\\t')\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genetic_Algorithms'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign features to each nodes\n",
    "nx.set_node_attributes(cora_g, features_df['subject'].to_dict(), \"subject\")\n",
    "nx.get_node_attributes(cora_g, \"subject\")[1033] # check node subject assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch alternative data source: torch-geometric\n",
    "\n",
    "- Install with: conda install pyg -c pyg\n",
    "- Load data: \n",
    "    ```{python} \n",
    "    from torch_geometric.datasets import Planetoid \n",
    "    dataset = Planetoid(root='~/somewhere/Cora', name='Cora')\n",
    "    ```\n",
    "- Visualize with: \n",
    "    ```{python}\n",
    "    import networkx as nx \n",
    "    from torch_geometric.utils import to_networkx \n",
    "    G = to_networkx(data, to_undirected=True) nx.draw(G)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a GCN layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using cuda:0. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"PyTorch is using {device}. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 nodes in the list  [1033, 35, 103482, 103515, 1050679]  out of  2708  number of nodes. \n"
     ]
    }
   ],
   "source": [
    "# create a nodelist to retrieve node number from index \n",
    "nodelist = list(cora_g.nodes) \n",
    "print(\"First 5 nodes in the list \", nodelist[:5], \" out of \", len(nodelist), \" number of nodes. \") # this is corresponding to the adjacency matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the adjacency matrix from established graph \n",
    "A = nx.to_numpy_matrix(cora_g, dtype=np.float32)  # unweighted adj matrix \n",
    "A = torch.from_numpy(A) # change it to a tensor object \n",
    "A = A.to(device)\n",
    "\n",
    "# encode the labesl labels \n",
    "categories = dict(zip(['Case_Based', \n",
    "                        'Genetic_Algorithms',\n",
    "                        'Neural_Networks',\n",
    "                        'Probabilistic_Methods',\n",
    "                        'Reinforcement_Learning',\n",
    "                        'Rule_Learning',\n",
    "                        'Theory'\n",
    "                    ], list(range(7))))\n",
    "labels = features_df.loc[nodelist]['subject'] # get the actual label \n",
    "labels = torch.tensor([categories[cat] for cat in labels]).unsqueeze(dim=1)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# ohe = OneHotEncoder()\n",
    "# ohe.fit(labels.reset_index()) \n",
    "# print(\"Onehot encoded categories: \\n\", ohe.categories_[0], \"\\n\", ohe.categories_[1])\n",
    "# onehot_labels = ohe.transform(labels.reset_index()).todense()\n",
    "# onehot_labels = [torch.from_numpy(lst) for lst in onehot_labels]\n",
    "\n",
    "# creates the training and testing dataset \n",
    "# train_size = int(A.shape[0] * 0.5)\n",
    "# validation_size = int(A.shape[0] * 0.8) - train_size\n",
    "# train_nodes, train_labels = A[:train_size], onehot_labels[:train_size]\n",
    "# validation_nodes, validation_labels = A[train_size:validation_size+train_size], onehot_labels[train_size:validation_size+train_size]\n",
    "# test_nodes, test_labels = A[validation_size+train_size:], onehot_labels[validation_size+train_size:]\n",
    "\n",
    "# creates the training and testing dataset (without ohe)\n",
    "train_size = int(A.shape[0] * 0.5)\n",
    "validation_size = int(A.shape[0] * 0.8) - train_size\n",
    "train_nodes, train_labels = A[:train_size], labels[:train_size]\n",
    "validation_nodes, validation_labels = A[train_size:validation_size+train_size], labels[train_size:validation_size+train_size]\n",
    "test_nodes, test_labels = A[validation_size+train_size:], labels[validation_size+train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1354\n",
      "True 812\n",
      "True 542\n"
     ]
    }
   ],
   "source": [
    "# check training and testing datasets\n",
    "print(len(train_nodes) == len(train_labels), len(train_labels))\n",
    "print(len(validation_nodes) == len(validation_labels), len(validation_nodes))\n",
    "print(len(test_nodes) == len(test_labels), len(test_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are some legacy code for dataloading in pytorch, might be useful later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data in a custom dataset \n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class NodeVLabelDataset(Dataset):\n",
    "#     def __init__(self, cora_cites_fp, cora_content_fp):\n",
    "#         # setup the graph \n",
    "#         self.cora_cites_fp, self.cora_content_fp = cora_cites_fp, cora_content_fp\n",
    "#         self.edges_df = pd.read_csv(self.cora_cites_fp, names=[\"target\", \"source\"], header=None, delimiter='\\t')\n",
    "#         self.graph = nx.from_pandas_edgelist(self.edges_df)       \n",
    "\n",
    "#         # create the adjacency matrix \n",
    "#         self.A = nx.to_numpy_matrix(self.graph, dtype=np.float32)  # unweighted adj matrix \n",
    "#         self.A = torch.from_numpy(A) # change it to a tensor object  \n",
    "#         self.nodelist = list(self.graph.nodes)  # a list of nodes \n",
    "\n",
    "#         # get the label subjects \n",
    "#         columns = [f\"word_{i}\" for i in range(1433)] + [\"subject\"]\n",
    "#         self.features_df = pd.read_csv(self.cora_content_fp, names=columns, delimiter='\\t')\n",
    "#         self.labels = features_df.loc[nodelist]['subject']  # list of labels\n",
    "        \n",
    "#         # encode the labesl labels \n",
    "#         self.categories = dict(zip(['Case_Based', \n",
    "#                                 'Genetic_Algorithms',\n",
    "#                                 'Neural_Networks',\n",
    "#                                 'Probabilistic_Methods',\n",
    "#                                 'Reinforcement_Learning',\n",
    "#                                 'Rule_Learning',\n",
    "#                                 'Theory'\n",
    "#                             ], list(range(7))))\n",
    "#         self.labels = self.labels.apply(lambda x: self.categories[x])\n",
    "#         self.labels = nn.functional.one_hot(self.labels, num_classes=len(self.categories.items()))\n",
    "        \n",
    "\n",
    "#     def draw(self, with_labels=True, font_weight='bold'): \n",
    "#         \"\"\"Create a visualization of the graph\"\"\"\n",
    "#         nx.draw(self.graph, with_labels=with_labels, font_weight=font_weight)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.A)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.A[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the training and testing data loader\n",
    "# train_dataset = NodeVLabelDataset( \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\")\n",
    "# test_dataset = NodeVLabelDataset( \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\")\n",
    "\n",
    "# training_loader = torch.utils.data.DataLoader(, batch_size=4, shuffle=True, num_workers=2)\n",
    "# testing_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GCN layer definition\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, A, input_dim, output_dim, device):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        print(\"layer initialized\")\n",
    "        self.device = device # initialize the hosting device\n",
    "\n",
    "        self.A = A.to(self.device) # the adjacency matrix \n",
    "        self.I = torch.eye(self.A.shape[0], device=self.device) # create identity matrix with the same shape as A \n",
    "        self.A =  self.A + self.I   # add self loop to A \n",
    "        # print(\"Adj Matrix with self loop: \", self.A)\n",
    "        self.D = torch.diag(torch.sum(self.A, dim=0)).cpu()  # calculate the degree matrix with A after added self loop\n",
    "        # print(\"Degree Matrix: \", self.D)\n",
    "        # for diagonal matrix, raising it to any power is the same as raising its diagonal elements to that power\n",
    "        # we can just apply the -1/2 power to all element of this degree matrix \n",
    "        # self.D_half_norm = torch.reciprocal(torch.sqrt(self.D)) \n",
    "        self.D_half_norm = torch.from_numpy(fractional_matrix_power(self.D, -0.5)).to(self.device)\n",
    "        # print(\"Normalization Matrix: \", self.D_half_norm)\n",
    "        self.A_s = torch.mm(torch.mm(self.D_half_norm, self.A), self.D_half_norm).to(self.device) # normalized adjacency matrix\n",
    "\n",
    "        # initialize the weight matrix for this layer \n",
    "        # the weight should have shape of (N , F) where N is the size of the input, and F is the output dimension\n",
    "        self.W = torch.nn.Parameter(\n",
    "            data=(torch.rand(input_dim, output_dim, device=self.device) * 0.01),  # times it by 0.001 to make the weight smaller\n",
    "            requires_grad=True, # weight should be trainable \n",
    "        )\n",
    "        # create trainable a bias term for the layer\n",
    "        self.b = torch.nn.Parameter(\n",
    "            data=(torch.rand(output_dim, 1, device=self.device) * 0.01),\n",
    "            requires_grad=True, # bias should be trainable \n",
    "        )\n",
    "\n",
    "        # print(self.W.get_device(), self.b.get_device())\n",
    "\n",
    "    def forward(self, H):\n",
    "        # print(H.get_device())\n",
    "        return torch.mm(torch.mm(self.A_s, H.unsqueeze(dim=1)).T, self.W).T + self.b\n",
    "\n",
    "# create a 1 layer classification model with softmax output\n",
    "class oneLayerGCN(nn.Module): \n",
    "    def __init__(self, adjacency_matrix, output_size, device) -> None:\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            adjacency_matrix: tensor, a tensor representing the link connection between nodes\n",
    "            output_size: input, the number of target label we have for our prediction\n",
    "        \"\"\"\n",
    "        super(oneLayerGCN, self).__init__()\n",
    "        self.device=device\n",
    "        self.A = adjacency_matrix\n",
    "        self.output_size = output_size\n",
    "        self.hidden_conv_layer = GCNLayer(A=self.A, input_dim=self.A.shape[0], output_dim=self.output_size, device=self.device)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, X): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            X: tensor, the link feature of the nodes in the dataset\n",
    "        \"\"\"\n",
    "        output = self.hidden_conv_layer(X).T # get the hidden embedding\n",
    "        # print(\"Hidden Layer Output:\", output)\n",
    "        output = self.softmax(output)\n",
    "        # print(\"Softmax Output: \", output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer initialized\n"
     ]
    }
   ],
   "source": [
    "# Set up the training parameter \n",
    "model = oneLayerGCN(adjacency_matrix=A, output_size=len(categories.items()), device=device)\n",
    "# print(\"\\nParameters in the one layer GCN: \\n\")\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "model.to(device)\n",
    "\n",
    "# create cross entropy loss calculation, optimization \n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\3425545340.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.834550076842308\n",
      "Training Accuracy for epoch 0:  0.30280649926144754\n",
      "Validation Accuracy for epoch 0:  0.2894088669950739\n",
      "LOSS train 1.834550076842308 valid 1.8806463479995728\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.8220288152694701\n",
      "Training Accuracy for epoch 1:  0.3293943870014771\n",
      "Validation Accuracy for epoch 1:  0.2894088669950739\n",
      "LOSS train 1.8220288152694701 valid 1.8783221244812012\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.8091794267892838\n",
      "Training Accuracy for epoch 2:  0.362629246676514\n",
      "Validation Accuracy for epoch 2:  0.29064039408866993\n",
      "LOSS train 1.8091794267892838 valid 1.876274585723877\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.7966832603216172\n",
      "Training Accuracy for epoch 3:  0.378877400295421\n",
      "Validation Accuracy for epoch 3:  0.29064039408866993\n",
      "LOSS train 1.7966832603216172 valid 1.8743805885314941\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.7851072815656661\n",
      "Training Accuracy for epoch 4:  0.38847858197932056\n",
      "Validation Accuracy for epoch 4:  0.291871921182266\n",
      "LOSS train 1.7851072815656661 valid 1.8726228475570679\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 1.774629139661789\n",
      "Training Accuracy for epoch 5:  0.39217134416543575\n",
      "Validation Accuracy for epoch 5:  0.291871921182266\n",
      "LOSS train 1.774629139661789 valid 1.8710416555404663\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 1.7651401282548904\n",
      "Training Accuracy for epoch 6:  0.397341211225997\n",
      "Validation Accuracy for epoch 6:  0.29679802955665024\n",
      "LOSS train 1.7651401282548904 valid 1.8696693181991577\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 1.7564245429039\n",
      "Training Accuracy for epoch 7:  0.40029542097488924\n",
      "Validation Accuracy for epoch 7:  0.29926108374384236\n",
      "LOSS train 1.7564245429039 valid 1.868520736694336\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 1.748265167593956\n",
      "Training Accuracy for epoch 8:  0.4069423929098966\n",
      "Validation Accuracy for epoch 8:  0.29926108374384236\n",
      "LOSS train 1.748265167593956 valid 1.8676038980484009\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 1.7404845160245896\n",
      "Training Accuracy for epoch 9:  0.41654357459379615\n",
      "Validation Accuracy for epoch 9:  0.30295566502463056\n",
      "LOSS train 1.7404845160245896 valid 1.8669028282165527\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 1.7329575799703598\n",
      "Training Accuracy for epoch 10:  0.4202363367799114\n",
      "Validation Accuracy for epoch 10:  0.30295566502463056\n",
      "LOSS train 1.7329575799703598 valid 1.866399884223938\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 1.7256134464740753\n",
      "Training Accuracy for epoch 11:  0.43057607090103395\n",
      "Validation Accuracy for epoch 11:  0.3066502463054187\n",
      "LOSS train 1.7256134464740753 valid 1.8660595417022705\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 1.7184292159080505\n",
      "Training Accuracy for epoch 12:  0.44091580502215655\n",
      "Validation Accuracy for epoch 12:  0.3078817733990148\n",
      "LOSS train 1.7184292159080505 valid 1.8658363819122314\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 1.711418039560318\n",
      "Training Accuracy for epoch 13:  0.448301329394387\n",
      "Validation Accuracy for epoch 13:  0.3091133004926108\n",
      "LOSS train 1.711418039560318 valid 1.8656837940216064\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 1.7046120647192002\n",
      "Training Accuracy for epoch 14:  0.4601181683899557\n",
      "Validation Accuracy for epoch 14:  0.312807881773399\n",
      "LOSS train 1.7046120647192002 valid 1.8655517101287842\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 1.6980464614629744\n",
      "Training Accuracy for epoch 15:  0.46971935007385524\n",
      "Validation Accuracy for epoch 15:  0.312807881773399\n",
      "LOSS train 1.6980464614629744 valid 1.8654024600982666\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 1.6917485412359237\n",
      "Training Accuracy for epoch 16:  0.4815361890694239\n",
      "Validation Accuracy for epoch 16:  0.31527093596059114\n",
      "LOSS train 1.6917485412359237 valid 1.865201473236084\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 1.6857333180904388\n",
      "Training Accuracy for epoch 17:  0.4874446085672083\n",
      "Validation Accuracy for epoch 17:  0.31527093596059114\n",
      "LOSS train 1.6857333180904388 valid 1.8649379014968872\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 1.6800042392015457\n",
      "Training Accuracy for epoch 18:  0.4948301329394387\n",
      "Validation Accuracy for epoch 18:  0.31773399014778325\n",
      "LOSS train 1.6800042392015457 valid 1.8646012544631958\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 1.6745561453104019\n",
      "Training Accuracy for epoch 19:  0.5051698670605613\n",
      "Validation Accuracy for epoch 19:  0.32019704433497537\n",
      "LOSS train 1.6745561453104019 valid 1.8641917705535889\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 1.6693784694671632\n",
      "Training Accuracy for epoch 20:  0.5096011816838996\n",
      "Validation Accuracy for epoch 20:  0.3226600985221675\n",
      "LOSS train 1.6693784694671632 valid 1.8637217283248901\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 1.6644578223228454\n",
      "Training Accuracy for epoch 21:  0.516248153618907\n",
      "Validation Accuracy for epoch 21:  0.3226600985221675\n",
      "LOSS train 1.6644578223228454 valid 1.8631986379623413\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 1.6597796137332916\n",
      "Training Accuracy for epoch 22:  0.5221565731166913\n",
      "Validation Accuracy for epoch 22:  0.3226600985221675\n",
      "LOSS train 1.6597796137332916 valid 1.8626346588134766\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 1.655329027056694\n",
      "Training Accuracy for epoch 23:  0.5295420974889217\n",
      "Validation Accuracy for epoch 23:  0.32389162561576357\n",
      "LOSS train 1.655329027056694 valid 1.8620367050170898\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 1.651091605424881\n",
      "Training Accuracy for epoch 24:  0.5361890694239291\n",
      "Validation Accuracy for epoch 24:  0.3251231527093596\n",
      "LOSS train 1.651091605424881 valid 1.8614110946655273\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 1.6470534243583679\n",
      "Training Accuracy for epoch 25:  0.5443131462333826\n",
      "Validation Accuracy for epoch 25:  0.3288177339901478\n",
      "LOSS train 1.6470534243583679 valid 1.8607690334320068\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 1.6432013449668885\n",
      "Training Accuracy for epoch 26:  0.5494830132939439\n",
      "Validation Accuracy for epoch 26:  0.33004926108374383\n",
      "LOSS train 1.6432013449668885 valid 1.8601150512695312\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 1.63952288544178\n",
      "Training Accuracy for epoch 27:  0.5561299852289513\n",
      "Validation Accuracy for epoch 27:  0.33251231527093594\n",
      "LOSS train 1.63952288544178 valid 1.8594534397125244\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 1.6360063799619675\n",
      "Training Accuracy for epoch 28:  0.5635155096011817\n",
      "Validation Accuracy for epoch 28:  0.33866995073891626\n",
      "LOSS train 1.6360063799619675 valid 1.8587874174118042\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 1.6326408610343932\n",
      "Training Accuracy for epoch 29:  0.5649926144756278\n",
      "Validation Accuracy for epoch 29:  0.3411330049261084\n",
      "LOSS train 1.6326408610343932 valid 1.8581215143203735\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 1.6294160014390946\n",
      "Training Accuracy for epoch 30:  0.569423929098966\n",
      "Validation Accuracy for epoch 30:  0.34236453201970446\n",
      "LOSS train 1.6294160014390946 valid 1.8574572801589966\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 1.626322172641754\n",
      "Training Accuracy for epoch 31:  0.5709010339734121\n",
      "Validation Accuracy for epoch 31:  0.34236453201970446\n",
      "LOSS train 1.626322172641754 valid 1.8567942380905151\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 1.6233502465486527\n",
      "Training Accuracy for epoch 32:  0.5738552437223042\n",
      "Validation Accuracy for epoch 32:  0.3485221674876847\n",
      "LOSS train 1.6233502465486527 valid 1.8561358451843262\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 1.6204916458129883\n",
      "Training Accuracy for epoch 33:  0.5782865583456426\n",
      "Validation Accuracy for epoch 33:  0.3522167487684729\n",
      "LOSS train 1.6204916458129883 valid 1.855483055114746\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 1.6177382271289826\n",
      "Training Accuracy for epoch 34:  0.5790251107828656\n",
      "Validation Accuracy for epoch 34:  0.35467980295566504\n",
      "LOSS train 1.6177382271289826 valid 1.8548378944396973\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 1.6150822776556015\n",
      "Training Accuracy for epoch 35:  0.5812407680945347\n",
      "Validation Accuracy for epoch 35:  0.35467980295566504\n",
      "LOSS train 1.6150822776556015 valid 1.8541978597640991\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 1.6125164457559586\n",
      "Training Accuracy for epoch 36:  0.585672082717873\n",
      "Validation Accuracy for epoch 36:  0.35714285714285715\n",
      "LOSS train 1.6125164457559586 valid 1.8535645008087158\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 1.6100335731506348\n",
      "Training Accuracy for epoch 37:  0.5841949778434269\n",
      "Validation Accuracy for epoch 37:  0.35960591133004927\n",
      "LOSS train 1.6100335731506348 valid 1.8529399633407593\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 1.6076268184185027\n",
      "Training Accuracy for epoch 38:  0.5864106351550961\n",
      "Validation Accuracy for epoch 38:  0.3608374384236453\n",
      "LOSS train 1.6076268184185027 valid 1.8523228168487549\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 1.6052894541025162\n",
      "Training Accuracy for epoch 39:  0.5878877400295421\n",
      "Validation Accuracy for epoch 39:  0.3620689655172414\n",
      "LOSS train 1.6052894541025162 valid 1.8517132997512817\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 1.603014829158783\n",
      "Training Accuracy for epoch 40:  0.5901033973412112\n",
      "Validation Accuracy for epoch 40:  0.3633004926108374\n",
      "LOSS train 1.603014829158783 valid 1.8511110544204712\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 1.600796395778656\n",
      "Training Accuracy for epoch 41:  0.5937961595273265\n",
      "Validation Accuracy for epoch 41:  0.3633004926108374\n",
      "LOSS train 1.600796395778656 valid 1.8505167961120605\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 1.5986274590492249\n",
      "Training Accuracy for epoch 42:  0.5974889217134417\n",
      "Validation Accuracy for epoch 42:  0.3620689655172414\n",
      "LOSS train 1.5986274590492249 valid 1.849928855895996\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 1.5965012699365615\n",
      "Training Accuracy for epoch 43:  0.5982274741506647\n",
      "Validation Accuracy for epoch 43:  0.3633004926108374\n",
      "LOSS train 1.5965012699365615 valid 1.849351167678833\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 1.594410904288292\n",
      "Training Accuracy for epoch 44:  0.5982274741506647\n",
      "Validation Accuracy for epoch 44:  0.3657635467980296\n",
      "LOSS train 1.594410904288292 valid 1.8487801551818848\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 1.5923491249084472\n",
      "Training Accuracy for epoch 45:  0.5997045790251108\n",
      "Validation Accuracy for epoch 45:  0.3669950738916256\n",
      "LOSS train 1.5923491249084472 valid 1.8482167720794678\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 1.5903083295822145\n",
      "Training Accuracy for epoch 46:  0.6056129985228951\n",
      "Validation Accuracy for epoch 46:  0.3669950738916256\n",
      "LOSS train 1.5903083295822145 valid 1.8476613759994507\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 1.588280576944351\n",
      "Training Accuracy for epoch 47:  0.6070901033973413\n",
      "Validation Accuracy for epoch 47:  0.3694581280788177\n",
      "LOSS train 1.588280576944351 valid 1.847113013267517\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 1.5862574125528335\n",
      "Training Accuracy for epoch 48:  0.6093057607090103\n",
      "Validation Accuracy for epoch 48:  0.3694581280788177\n",
      "LOSS train 1.5862574125528335 valid 1.8465726375579834\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 1.5842299419641495\n",
      "Training Accuracy for epoch 49:  0.6115214180206795\n",
      "Validation Accuracy for epoch 49:  0.3694581280788177\n",
      "LOSS train 1.5842299419641495 valid 1.846037745475769\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 1.5821889293193818\n",
      "Training Accuracy for epoch 50:  0.6137370753323486\n",
      "Validation Accuracy for epoch 50:  0.3706896551724138\n",
      "LOSS train 1.5821889293193818 valid 1.8455113172531128\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 1.5801248599290847\n",
      "Training Accuracy for epoch 51:  0.6152141802067946\n",
      "Validation Accuracy for epoch 51:  0.3706896551724138\n",
      "LOSS train 1.5801248599290847 valid 1.8449921607971191\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 1.5780282998085022\n",
      "Training Accuracy for epoch 52:  0.6174298375184638\n",
      "Validation Accuracy for epoch 52:  0.3706896551724138\n",
      "LOSS train 1.5780282998085022 valid 1.844481348991394\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 1.575890258669853\n",
      "Training Accuracy for epoch 53:  0.6211225997045791\n",
      "Validation Accuracy for epoch 53:  0.3706896551724138\n",
      "LOSS train 1.575890258669853 valid 1.8439780473709106\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 1.5737029695510865\n",
      "Training Accuracy for epoch 54:  0.6233382570162481\n",
      "Validation Accuracy for epoch 54:  0.3694581280788177\n",
      "LOSS train 1.5737029695510865 valid 1.8434827327728271\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 1.5714605544805527\n",
      "Training Accuracy for epoch 55:  0.6248153618906942\n",
      "Validation Accuracy for epoch 55:  0.3706896551724138\n",
      "LOSS train 1.5714605544805527 valid 1.842994213104248\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 1.569160034775734\n",
      "Training Accuracy for epoch 56:  0.6270310192023634\n",
      "Validation Accuracy for epoch 56:  0.3706896551724138\n",
      "LOSS train 1.569160034775734 valid 1.8425151109695435\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 1.5668022091388702\n",
      "Training Accuracy for epoch 57:  0.6307237813884786\n",
      "Validation Accuracy for epoch 57:  0.3706896551724138\n",
      "LOSS train 1.5668022091388702 valid 1.8420429229736328\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 1.5643923100233077\n",
      "Training Accuracy for epoch 58:  0.6336779911373708\n",
      "Validation Accuracy for epoch 58:  0.3706896551724138\n",
      "LOSS train 1.5643923100233077 valid 1.8415802717208862\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 1.561940225839615\n",
      "Training Accuracy for epoch 59:  0.6388478581979321\n",
      "Validation Accuracy for epoch 59:  0.3706896551724138\n",
      "LOSS train 1.561940225839615 valid 1.841124415397644\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 1.559460222363472\n",
      "Training Accuracy for epoch 60:  0.6440177252584933\n",
      "Validation Accuracy for epoch 60:  0.3706896551724138\n",
      "LOSS train 1.559460222363472 valid 1.840674877166748\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 1.5569695891141893\n",
      "Training Accuracy for epoch 61:  0.6484490398818316\n",
      "Validation Accuracy for epoch 61:  0.3706896551724138\n",
      "LOSS train 1.5569695891141893 valid 1.8402308225631714\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 1.554487154364586\n",
      "Training Accuracy for epoch 62:  0.6499261447562777\n",
      "Validation Accuracy for epoch 62:  0.3706896551724138\n",
      "LOSS train 1.554487154364586 valid 1.8397914171218872\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 1.552031137228012\n",
      "Training Accuracy for epoch 63:  0.6528803545051699\n",
      "Validation Accuracy for epoch 63:  0.37192118226600984\n",
      "LOSS train 1.552031137228012 valid 1.8393492698669434\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 1.5496175299882888\n",
      "Training Accuracy for epoch 64:  0.6587887740029542\n",
      "Validation Accuracy for epoch 64:  0.37192118226600984\n",
      "LOSS train 1.5496175299882888 valid 1.8389006853103638\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 1.5472586660385133\n",
      "Training Accuracy for epoch 65:  0.6661742983751846\n",
      "Validation Accuracy for epoch 65:  0.3731527093596059\n",
      "LOSS train 1.5472586660385133 valid 1.838447093963623\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 1.5449626845121385\n",
      "Training Accuracy for epoch 66:  0.672821270310192\n",
      "Validation Accuracy for epoch 66:  0.3731527093596059\n",
      "LOSS train 1.5449626845121385 valid 1.8379788398742676\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 1.5427338547706604\n",
      "Training Accuracy for epoch 67:  0.6742983751846381\n",
      "Validation Accuracy for epoch 67:  0.37561576354679804\n",
      "LOSS train 1.5427338547706604 valid 1.8374987840652466\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 1.5405730838775635\n",
      "Training Accuracy for epoch 68:  0.6816838995568686\n",
      "Validation Accuracy for epoch 68:  0.3793103448275862\n",
      "LOSS train 1.5405730838775635 valid 1.8370047807693481\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 1.5384789270162582\n",
      "Training Accuracy for epoch 69:  0.6846381093057607\n",
      "Validation Accuracy for epoch 69:  0.3842364532019704\n",
      "LOSS train 1.5384789270162582 valid 1.836493968963623\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 1.5364484620094299\n",
      "Training Accuracy for epoch 70:  0.689807976366322\n",
      "Validation Accuracy for epoch 70:  0.3854679802955665\n",
      "LOSS train 1.5364484620094299 valid 1.8359653949737549\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 1.5344780422449111\n",
      "Training Accuracy for epoch 71:  0.6927621861152142\n",
      "Validation Accuracy for epoch 71:  0.3866995073891626\n",
      "LOSS train 1.5344780422449111 valid 1.835429072380066\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 1.532563812494278\n",
      "Training Accuracy for epoch 72:  0.6942392909896603\n",
      "Validation Accuracy for epoch 72:  0.3879310344827586\n",
      "LOSS train 1.532563812494278 valid 1.834877610206604\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 1.5307020547389985\n",
      "Training Accuracy for epoch 73:  0.6964549483013294\n",
      "Validation Accuracy for epoch 73:  0.3916256157635468\n",
      "LOSS train 1.5307020547389985 valid 1.8343170881271362\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 1.5288893880844117\n",
      "Training Accuracy for epoch 74:  0.6964549483013294\n",
      "Validation Accuracy for epoch 74:  0.39285714285714285\n",
      "LOSS train 1.5288893880844117 valid 1.8337472677230835\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 1.5271227167844772\n",
      "Training Accuracy for epoch 75:  0.6994091580502215\n",
      "Validation Accuracy for epoch 75:  0.39532019704433496\n",
      "LOSS train 1.5271227167844772 valid 1.8331738710403442\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 1.5253994302749634\n",
      "Training Accuracy for epoch 76:  0.7001477104874446\n",
      "Validation Accuracy for epoch 76:  0.39655172413793105\n",
      "LOSS train 1.5253994302749634 valid 1.832592248916626\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 1.52371715760231\n",
      "Training Accuracy for epoch 77:  0.7016248153618907\n",
      "Validation Accuracy for epoch 77:  0.39655172413793105\n",
      "LOSS train 1.52371715760231 valid 1.8320096731185913\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 1.5220738681554795\n",
      "Training Accuracy for epoch 78:  0.7038404726735599\n",
      "Validation Accuracy for epoch 78:  0.4027093596059113\n",
      "LOSS train 1.5220738681554795 valid 1.8314220905303955\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 1.5204677611589432\n",
      "Training Accuracy for epoch 79:  0.7038404726735599\n",
      "Validation Accuracy for epoch 79:  0.4051724137931034\n",
      "LOSS train 1.5204677611589432 valid 1.8308360576629639\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 1.5188972322940826\n",
      "Training Accuracy for epoch 80:  0.7045790251107829\n",
      "Validation Accuracy for epoch 80:  0.40763546798029554\n",
      "LOSS train 1.5188972322940826 valid 1.8302470445632935\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 1.517360850095749\n",
      "Training Accuracy for epoch 81:  0.7060561299852289\n",
      "Validation Accuracy for epoch 81:  0.4125615763546798\n",
      "LOSS train 1.517360850095749 valid 1.829660177230835\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 1.5158573105335236\n",
      "Training Accuracy for epoch 82:  0.706794682422452\n",
      "Validation Accuracy for epoch 82:  0.41502463054187194\n",
      "LOSS train 1.5158573105335236 valid 1.829074740409851\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 1.5143854308128357\n",
      "Training Accuracy for epoch 83:  0.706794682422452\n",
      "Validation Accuracy for epoch 83:  0.41502463054187194\n",
      "LOSS train 1.5143854308128357 valid 1.8284885883331299\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 1.512944062113762\n",
      "Training Accuracy for epoch 84:  0.707533234859675\n",
      "Validation Accuracy for epoch 84:  0.41625615763546797\n",
      "LOSS train 1.512944062113762 valid 1.827906847000122\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 1.511532187104225\n",
      "Training Accuracy for epoch 85:  0.7090103397341211\n",
      "Validation Accuracy for epoch 85:  0.41748768472906406\n",
      "LOSS train 1.511532187104225 valid 1.8273262977600098\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 1.5101488147974014\n",
      "Training Accuracy for epoch 86:  0.7119645494830132\n",
      "Validation Accuracy for epoch 86:  0.41748768472906406\n",
      "LOSS train 1.5101488147974014 valid 1.8267467021942139\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 1.5087930321693421\n",
      "Training Accuracy for epoch 87:  0.7134416543574594\n",
      "Validation Accuracy for epoch 87:  0.4187192118226601\n",
      "LOSS train 1.5087930321693421 valid 1.826172113418579\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 1.5074639490842818\n",
      "Training Accuracy for epoch 88:  0.7141802067946824\n",
      "Validation Accuracy for epoch 88:  0.41995073891625617\n",
      "LOSS train 1.5074639490842818 valid 1.8255962133407593\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 1.506160735964775\n",
      "Training Accuracy for epoch 89:  0.7149187592319055\n",
      "Validation Accuracy for epoch 89:  0.41995073891625617\n",
      "LOSS train 1.506160735964775 valid 1.825025200843811\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 1.504882614850998\n",
      "Training Accuracy for epoch 90:  0.7178729689807977\n",
      "Validation Accuracy for epoch 90:  0.4211822660098522\n",
      "LOSS train 1.504882614850998 valid 1.824456810951233\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 1.5036288030147553\n",
      "Training Accuracy for epoch 91:  0.7186115214180206\n",
      "Validation Accuracy for epoch 91:  0.4236453201970443\n",
      "LOSS train 1.5036288030147553 valid 1.823891043663025\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 1.5023986006975174\n",
      "Training Accuracy for epoch 92:  0.7186115214180206\n",
      "Validation Accuracy for epoch 92:  0.4273399014778325\n",
      "LOSS train 1.5023986006975174 valid 1.8233299255371094\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 1.5011913158893586\n",
      "Training Accuracy for epoch 93:  0.7200886262924667\n",
      "Validation Accuracy for epoch 93:  0.4273399014778325\n",
      "LOSS train 1.5011913158893586 valid 1.822771668434143\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 1.50000627887249\n",
      "Training Accuracy for epoch 94:  0.7223042836041359\n",
      "Validation Accuracy for epoch 94:  0.4273399014778325\n",
      "LOSS train 1.50000627887249 valid 1.8222156763076782\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 1.49884288585186\n",
      "Training Accuracy for epoch 95:  0.723781388478582\n",
      "Validation Accuracy for epoch 95:  0.42980295566502463\n",
      "LOSS train 1.49884288585186 valid 1.8216630220413208\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 1.4977005405426025\n",
      "Training Accuracy for epoch 96:  0.725258493353028\n",
      "Validation Accuracy for epoch 96:  0.43103448275862066\n",
      "LOSS train 1.4977005405426025 valid 1.8211133480072021\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 1.496578650712967\n",
      "Training Accuracy for epoch 97:  0.725258493353028\n",
      "Validation Accuracy for epoch 97:  0.43226600985221675\n",
      "LOSS train 1.496578650712967 valid 1.8205680847167969\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 1.495476710319519\n",
      "Training Accuracy for epoch 98:  0.7267355982274741\n",
      "Validation Accuracy for epoch 98:  0.43596059113300495\n",
      "LOSS train 1.495476710319519 valid 1.8200256824493408\n",
      "EPOCH 100:\n",
      "  batch 1000 loss: 1.4943941969871521\n",
      "Training Accuracy for epoch 99:  0.7274741506646972\n",
      "Validation Accuracy for epoch 99:  0.43596059113300495\n",
      "LOSS train 1.4943941969871521 valid 1.8194876909255981\n",
      "EPOCH 101:\n",
      "  batch 1000 loss: 1.4933306189775466\n",
      "Training Accuracy for epoch 100:  0.7282127031019202\n",
      "Validation Accuracy for epoch 100:  0.437192118226601\n",
      "LOSS train 1.4933306189775466 valid 1.8189518451690674\n",
      "EPOCH 102:\n",
      "  batch 1000 loss: 1.492285494208336\n",
      "Training Accuracy for epoch 101:  0.7304283604135894\n",
      "Validation Accuracy for epoch 101:  0.4408866995073892\n",
      "LOSS train 1.492285494208336 valid 1.8184194564819336\n",
      "EPOCH 103:\n",
      "  batch 1000 loss: 1.491258379817009\n",
      "Training Accuracy for epoch 102:  0.7304283604135894\n",
      "Validation Accuracy for epoch 102:  0.4421182266009852\n",
      "LOSS train 1.491258379817009 valid 1.817891001701355\n",
      "EPOCH 104:\n",
      "  batch 1000 loss: 1.490248815894127\n",
      "Training Accuracy for epoch 103:  0.7304283604135894\n",
      "Validation Accuracy for epoch 103:  0.4421182266009852\n",
      "LOSS train 1.490248815894127 valid 1.8173649311065674\n",
      "EPOCH 105:\n",
      "  batch 1000 loss: 1.489256418824196\n",
      "Training Accuracy for epoch 104:  0.7311669128508124\n",
      "Validation Accuracy for epoch 104:  0.4445812807881773\n",
      "LOSS train 1.489256418824196 valid 1.8168429136276245\n",
      "EPOCH 106:\n",
      "  batch 1000 loss: 1.4882807364463806\n",
      "Training Accuracy for epoch 105:  0.7326440177252584\n",
      "Validation Accuracy for epoch 105:  0.4458128078817734\n",
      "LOSS train 1.4882807364463806 valid 1.8163267374038696\n",
      "EPOCH 107:\n",
      "  batch 1000 loss: 1.4873213748931884\n",
      "Training Accuracy for epoch 106:  0.7326440177252584\n",
      "Validation Accuracy for epoch 106:  0.4458128078817734\n",
      "LOSS train 1.4873213748931884 valid 1.8158111572265625\n",
      "EPOCH 108:\n",
      "  batch 1000 loss: 1.4863779537677766\n",
      "Training Accuracy for epoch 107:  0.7333825701624815\n",
      "Validation Accuracy for epoch 107:  0.4458128078817734\n",
      "LOSS train 1.4863779537677766 valid 1.8152997493743896\n",
      "EPOCH 109:\n",
      "  batch 1000 loss: 1.4854500631093979\n",
      "Training Accuracy for epoch 108:  0.7348596750369276\n",
      "Validation Accuracy for epoch 108:  0.4458128078817734\n",
      "LOSS train 1.4854500631093979 valid 1.814792513847351\n",
      "EPOCH 110:\n",
      "  batch 1000 loss: 1.484537350654602\n",
      "Training Accuracy for epoch 109:  0.7348596750369276\n",
      "Validation Accuracy for epoch 109:  0.4482758620689655\n",
      "LOSS train 1.484537350654602 valid 1.814286231994629\n",
      "EPOCH 111:\n",
      "  batch 1000 loss: 1.4836393978595734\n",
      "Training Accuracy for epoch 110:  0.7370753323485968\n",
      "Validation Accuracy for epoch 110:  0.44950738916256155\n",
      "LOSS train 1.4836393978595734 valid 1.8137866258621216\n",
      "EPOCH 112:\n",
      "  batch 1000 loss: 1.4827558821439744\n",
      "Training Accuracy for epoch 111:  0.7370753323485968\n",
      "Validation Accuracy for epoch 111:  0.4482758620689655\n",
      "LOSS train 1.4827558821439744 valid 1.8132904767990112\n",
      "EPOCH 113:\n",
      "  batch 1000 loss: 1.4818864164352417\n",
      "Training Accuracy for epoch 112:  0.7370753323485968\n",
      "Validation Accuracy for epoch 112:  0.44950738916256155\n",
      "LOSS train 1.4818864164352417 valid 1.81279456615448\n",
      "EPOCH 114:\n",
      "  batch 1000 loss: 1.4810306614637374\n",
      "Training Accuracy for epoch 113:  0.7385524372230429\n",
      "Validation Accuracy for epoch 113:  0.45073891625615764\n",
      "LOSS train 1.4810306614637374 valid 1.8123043775558472\n",
      "EPOCH 115:\n",
      "  batch 1000 loss: 1.4801882574558258\n",
      "Training Accuracy for epoch 114:  0.7385524372230429\n",
      "Validation Accuracy for epoch 114:  0.45197044334975367\n",
      "LOSS train 1.4801882574558258 valid 1.8118155002593994\n",
      "EPOCH 116:\n",
      "  batch 1000 loss: 1.4793588466644287\n",
      "Training Accuracy for epoch 115:  0.7392909896602659\n",
      "Validation Accuracy for epoch 115:  0.45197044334975367\n",
      "LOSS train 1.4793588466644287 valid 1.8113313913345337\n",
      "EPOCH 117:\n",
      "  batch 1000 loss: 1.4785421199798583\n",
      "Training Accuracy for epoch 116:  0.7407680945347119\n",
      "Validation Accuracy for epoch 116:  0.45320197044334976\n",
      "LOSS train 1.4785421199798583 valid 1.8108503818511963\n",
      "EPOCH 118:\n",
      "  batch 1000 loss: 1.4777377123832702\n",
      "Training Accuracy for epoch 117:  0.7407680945347119\n",
      "Validation Accuracy for epoch 117:  0.4544334975369458\n",
      "LOSS train 1.4777377123832702 valid 1.810370922088623\n",
      "EPOCH 119:\n",
      "  batch 1000 loss: 1.4769453058242799\n",
      "Training Accuracy for epoch 118:  0.7407680945347119\n",
      "Validation Accuracy for epoch 118:  0.45689655172413796\n",
      "LOSS train 1.4769453058242799 valid 1.8098982572555542\n",
      "EPOCH 120:\n",
      "  batch 1000 loss: 1.4761645838022233\n",
      "Training Accuracy for epoch 119:  0.741506646971935\n",
      "Validation Accuracy for epoch 119:  0.45689655172413796\n",
      "LOSS train 1.4761645838022233 valid 1.809427261352539\n",
      "EPOCH 121:\n",
      "  batch 1000 loss: 1.475395248055458\n",
      "Training Accuracy for epoch 120:  0.742245199409158\n",
      "Validation Accuracy for epoch 120:  0.458128078817734\n",
      "LOSS train 1.475395248055458 valid 1.8089581727981567\n",
      "EPOCH 122:\n",
      "  batch 1000 loss: 1.47463696873188\n",
      "Training Accuracy for epoch 121:  0.742245199409158\n",
      "Validation Accuracy for epoch 121:  0.458128078817734\n",
      "LOSS train 1.47463696873188 valid 1.8084936141967773\n",
      "EPOCH 123:\n",
      "  batch 1000 loss: 1.4738894420862199\n",
      "Training Accuracy for epoch 122:  0.742245199409158\n",
      "Validation Accuracy for epoch 122:  0.458128078817734\n",
      "LOSS train 1.4738894420862199 valid 1.8080312013626099\n",
      "EPOCH 124:\n",
      "  batch 1000 loss: 1.4731523823738097\n",
      "Training Accuracy for epoch 123:  0.742245199409158\n",
      "Validation Accuracy for epoch 123:  0.458128078817734\n",
      "LOSS train 1.4731523823738097 valid 1.8075730800628662\n",
      "EPOCH 125:\n",
      "  batch 1000 loss: 1.4724254902601241\n",
      "Training Accuracy for epoch 124:  0.7429837518463811\n",
      "Validation Accuracy for epoch 124:  0.458128078817734\n",
      "LOSS train 1.4724254902601241 valid 1.8071180582046509\n",
      "EPOCH 126:\n",
      "  batch 1000 loss: 1.4717084980010986\n",
      "Training Accuracy for epoch 125:  0.7429837518463811\n",
      "Validation Accuracy for epoch 125:  0.4618226600985222\n",
      "LOSS train 1.4717084980010986 valid 1.8066654205322266\n",
      "EPOCH 127:\n",
      "  batch 1000 loss: 1.4710010995864868\n",
      "Training Accuracy for epoch 126:  0.7437223042836041\n",
      "Validation Accuracy for epoch 126:  0.4618226600985222\n",
      "LOSS train 1.4710010995864868 valid 1.8062164783477783\n",
      "EPOCH 128:\n",
      "  batch 1000 loss: 1.470303047299385\n",
      "Training Accuracy for epoch 127:  0.7437223042836041\n",
      "Validation Accuracy for epoch 127:  0.4630541871921182\n",
      "LOSS train 1.470303047299385 valid 1.805769443511963\n",
      "EPOCH 129:\n",
      "  batch 1000 loss: 1.4696140670776368\n",
      "Training Accuracy for epoch 128:  0.7437223042836041\n",
      "Validation Accuracy for epoch 128:  0.4630541871921182\n",
      "LOSS train 1.4696140670776368 valid 1.805326223373413\n",
      "EPOCH 130:\n",
      "  batch 1000 loss: 1.4689338799715042\n",
      "Training Accuracy for epoch 129:  0.7437223042836041\n",
      "Validation Accuracy for epoch 129:  0.4642857142857143\n",
      "LOSS train 1.4689338799715042 valid 1.8048864603042603\n",
      "EPOCH 131:\n",
      "  batch 1000 loss: 1.468262222290039\n",
      "Training Accuracy for epoch 130:  0.7437223042836041\n",
      "Validation Accuracy for epoch 130:  0.4630541871921182\n",
      "LOSS train 1.468262222290039 valid 1.8044503927230835\n",
      "EPOCH 132:\n",
      "  batch 1000 loss: 1.4675988664627075\n",
      "Training Accuracy for epoch 131:  0.7444608567208272\n",
      "Validation Accuracy for epoch 131:  0.4630541871921182\n",
      "LOSS train 1.4675988664627075 valid 1.8040152788162231\n",
      "EPOCH 133:\n",
      "  batch 1000 loss: 1.4669435431957245\n",
      "Training Accuracy for epoch 132:  0.7451994091580503\n",
      "Validation Accuracy for epoch 132:  0.4630541871921182\n",
      "LOSS train 1.4669435431957245 valid 1.803585171699524\n",
      "EPOCH 134:\n",
      "  batch 1000 loss: 1.4662960073947906\n",
      "Training Accuracy for epoch 133:  0.7459379615952733\n",
      "Validation Accuracy for epoch 133:  0.46551724137931033\n",
      "LOSS train 1.4662960073947906 valid 1.8031575679779053\n",
      "EPOCH 135:\n",
      "  batch 1000 loss: 1.4656559995412826\n",
      "Training Accuracy for epoch 134:  0.7459379615952733\n",
      "Validation Accuracy for epoch 134:  0.4667487684729064\n",
      "LOSS train 1.4656559995412826 valid 1.802732229232788\n",
      "EPOCH 136:\n",
      "  batch 1000 loss: 1.4650233075618744\n",
      "Training Accuracy for epoch 135:  0.7459379615952733\n",
      "Validation Accuracy for epoch 135:  0.46798029556650245\n",
      "LOSS train 1.4650233075618744 valid 1.802310824394226\n",
      "EPOCH 137:\n",
      "  batch 1000 loss: 1.4643976677656174\n",
      "Training Accuracy for epoch 136:  0.7459379615952733\n",
      "Validation Accuracy for epoch 136:  0.46798029556650245\n",
      "LOSS train 1.4643976677656174 valid 1.8018932342529297\n",
      "EPOCH 138:\n",
      "  batch 1000 loss: 1.463778855085373\n",
      "Training Accuracy for epoch 137:  0.7459379615952733\n",
      "Validation Accuracy for epoch 137:  0.4667487684729064\n",
      "LOSS train 1.463778855085373 valid 1.8014752864837646\n",
      "EPOCH 139:\n",
      "  batch 1000 loss: 1.4631665852069855\n",
      "Training Accuracy for epoch 138:  0.7466765140324964\n",
      "Validation Accuracy for epoch 138:  0.4667487684729064\n",
      "LOSS train 1.4631665852069855 valid 1.8010648488998413\n",
      "EPOCH 140:\n",
      "  batch 1000 loss: 1.4625606536865234\n",
      "Training Accuracy for epoch 139:  0.7474150664697193\n",
      "Validation Accuracy for epoch 139:  0.4667487684729064\n",
      "LOSS train 1.4625606536865234 valid 1.8006525039672852\n",
      "EPOCH 141:\n",
      "  batch 1000 loss: 1.4619608122110366\n",
      "Training Accuracy for epoch 140:  0.7474150664697193\n",
      "Validation Accuracy for epoch 140:  0.4667487684729064\n",
      "LOSS train 1.4619608122110366 valid 1.800248146057129\n",
      "EPOCH 142:\n",
      "  batch 1000 loss: 1.4613668191432954\n",
      "Training Accuracy for epoch 141:  0.7474150664697193\n",
      "Validation Accuracy for epoch 141:  0.47044334975369456\n",
      "LOSS train 1.4613668191432954 valid 1.7998450994491577\n",
      "EPOCH 143:\n",
      "  batch 1000 loss: 1.4607784014940262\n",
      "Training Accuracy for epoch 142:  0.7481536189069424\n",
      "Validation Accuracy for epoch 142:  0.47044334975369456\n",
      "LOSS train 1.4607784014940262 valid 1.7994439601898193\n",
      "EPOCH 144:\n",
      "  batch 1000 loss: 1.4601953403949737\n",
      "Training Accuracy for epoch 143:  0.7481536189069424\n",
      "Validation Accuracy for epoch 143:  0.47413793103448276\n",
      "LOSS train 1.4601953403949737 valid 1.7990463972091675\n",
      "EPOCH 145:\n",
      "  batch 1000 loss: 1.4596173539161683\n",
      "Training Accuracy for epoch 144:  0.7481536189069424\n",
      "Validation Accuracy for epoch 144:  0.47413793103448276\n",
      "LOSS train 1.4596173539161683 valid 1.7986537218093872\n",
      "EPOCH 146:\n",
      "  batch 1000 loss: 1.459044191122055\n",
      "Training Accuracy for epoch 145:  0.7481536189069424\n",
      "Validation Accuracy for epoch 145:  0.47413793103448276\n",
      "LOSS train 1.459044191122055 valid 1.7982622385025024\n",
      "EPOCH 147:\n",
      "  batch 1000 loss: 1.4584755737781525\n",
      "Training Accuracy for epoch 146:  0.7481536189069424\n",
      "Validation Accuracy for epoch 146:  0.4766009852216749\n",
      "LOSS train 1.4584755737781525 valid 1.797872543334961\n",
      "EPOCH 148:\n",
      "  batch 1000 loss: 1.4579112125635147\n",
      "Training Accuracy for epoch 147:  0.7481536189069424\n",
      "Validation Accuracy for epoch 147:  0.4766009852216749\n",
      "LOSS train 1.4579112125635147 valid 1.7974895238876343\n",
      "EPOCH 149:\n",
      "  batch 1000 loss: 1.4573508368730546\n",
      "Training Accuracy for epoch 148:  0.7488921713441654\n",
      "Validation Accuracy for epoch 148:  0.4766009852216749\n",
      "LOSS train 1.4573508368730546 valid 1.7971090078353882\n",
      "EPOCH 150:\n",
      "  batch 1000 loss: 1.4567941207885742\n",
      "Training Accuracy for epoch 149:  0.7488921713441654\n",
      "Validation Accuracy for epoch 149:  0.4766009852216749\n",
      "LOSS train 1.4567941207885742 valid 1.7967277765274048\n",
      "EPOCH 151:\n",
      "  batch 1000 loss: 1.4562407506704331\n",
      "Training Accuracy for epoch 150:  0.7488921713441654\n",
      "Validation Accuracy for epoch 150:  0.4753694581280788\n",
      "LOSS train 1.4562407506704331 valid 1.7963541746139526\n",
      "EPOCH 152:\n",
      "  batch 1000 loss: 1.4556904082298279\n",
      "Training Accuracy for epoch 151:  0.7511078286558346\n",
      "Validation Accuracy for epoch 151:  0.4753694581280788\n",
      "LOSS train 1.4556904082298279 valid 1.7959814071655273\n",
      "EPOCH 153:\n",
      "  batch 1000 loss: 1.4551427141427993\n",
      "Training Accuracy for epoch 152:  0.7518463810930576\n",
      "Validation Accuracy for epoch 152:  0.4753694581280788\n",
      "LOSS train 1.4551427141427993 valid 1.7956123352050781\n",
      "EPOCH 154:\n",
      "  batch 1000 loss: 1.4545973026752472\n",
      "Training Accuracy for epoch 153:  0.7525849335302807\n",
      "Validation Accuracy for epoch 153:  0.4753694581280788\n",
      "LOSS train 1.4545973026752472 valid 1.7952476739883423\n",
      "EPOCH 155:\n",
      "  batch 1000 loss: 1.4540537796020507\n",
      "Training Accuracy for epoch 154:  0.7525849335302807\n",
      "Validation Accuracy for epoch 154:  0.4753694581280788\n",
      "LOSS train 1.4540537796020507 valid 1.794886827468872\n",
      "EPOCH 156:\n",
      "  batch 1000 loss: 1.4535117077827453\n",
      "Training Accuracy for epoch 155:  0.7533234859675036\n",
      "Validation Accuracy for epoch 155:  0.4766009852216749\n",
      "LOSS train 1.4535117077827453 valid 1.794530987739563\n",
      "EPOCH 157:\n",
      "  batch 1000 loss: 1.4529706017971038\n",
      "Training Accuracy for epoch 156:  0.7540620384047267\n",
      "Validation Accuracy for epoch 156:  0.479064039408867\n",
      "LOSS train 1.4529706017971038 valid 1.7941769361495972\n",
      "EPOCH 158:\n",
      "  batch 1000 loss: 1.4524299947023391\n",
      "Training Accuracy for epoch 157:  0.7540620384047267\n",
      "Validation Accuracy for epoch 157:  0.479064039408867\n",
      "LOSS train 1.4524299947023391 valid 1.7938262224197388\n",
      "EPOCH 159:\n",
      "  batch 1000 loss: 1.4518893311023713\n",
      "Training Accuracy for epoch 158:  0.7540620384047267\n",
      "Validation Accuracy for epoch 158:  0.4802955665024631\n",
      "LOSS train 1.4518893311023713 valid 1.793480396270752\n",
      "EPOCH 160:\n",
      "  batch 1000 loss: 1.451348022699356\n",
      "Training Accuracy for epoch 159:  0.7540620384047267\n",
      "Validation Accuracy for epoch 159:  0.4802955665024631\n",
      "LOSS train 1.451348022699356 valid 1.7931371927261353\n",
      "EPOCH 161:\n",
      "  batch 1000 loss: 1.4508054461479187\n",
      "Training Accuracy for epoch 160:  0.7548005908419497\n",
      "Validation Accuracy for epoch 160:  0.4802955665024631\n",
      "LOSS train 1.4508054461479187 valid 1.7928005456924438\n",
      "EPOCH 162:\n",
      "  batch 1000 loss: 1.4502608830928803\n",
      "Training Accuracy for epoch 161:  0.7548005908419497\n",
      "Validation Accuracy for epoch 161:  0.4802955665024631\n",
      "LOSS train 1.4502608830928803 valid 1.7924652099609375\n",
      "EPOCH 163:\n",
      "  batch 1000 loss: 1.449713604927063\n",
      "Training Accuracy for epoch 162:  0.7562776957163959\n",
      "Validation Accuracy for epoch 162:  0.4802955665024631\n",
      "LOSS train 1.449713604927063 valid 1.7921383380889893\n",
      "EPOCH 164:\n",
      "  batch 1000 loss: 1.4491627877950668\n",
      "Training Accuracy for epoch 163:  0.7562776957163959\n",
      "Validation Accuracy for epoch 163:  0.4802955665024631\n",
      "LOSS train 1.4491627877950668 valid 1.791815996170044\n",
      "EPOCH 165:\n",
      "  batch 1000 loss: 1.4486075390577315\n",
      "Training Accuracy for epoch 164:  0.7562776957163959\n",
      "Validation Accuracy for epoch 164:  0.4802955665024631\n",
      "LOSS train 1.4486075390577315 valid 1.7914975881576538\n",
      "EPOCH 166:\n",
      "  batch 1000 loss: 1.4480468736886978\n",
      "Training Accuracy for epoch 165:  0.7570162481536189\n",
      "Validation Accuracy for epoch 165:  0.4802955665024631\n",
      "LOSS train 1.4480468736886978 valid 1.7911840677261353\n",
      "EPOCH 167:\n",
      "  batch 1000 loss: 1.4474797443151475\n",
      "Training Accuracy for epoch 166:  0.7570162481536189\n",
      "Validation Accuracy for epoch 166:  0.4802955665024631\n",
      "LOSS train 1.4474797443151475 valid 1.7908775806427002\n",
      "EPOCH 168:\n",
      "  batch 1000 loss: 1.4469050234556198\n",
      "Training Accuracy for epoch 167:  0.7570162481536189\n",
      "Validation Accuracy for epoch 167:  0.4827586206896552\n",
      "LOSS train 1.4469050234556198 valid 1.79057776927948\n",
      "EPOCH 169:\n",
      "  batch 1000 loss: 1.446321493268013\n",
      "Training Accuracy for epoch 168:  0.7570162481536189\n",
      "Validation Accuracy for epoch 168:  0.4827586206896552\n",
      "LOSS train 1.446321493268013 valid 1.7902857065200806\n",
      "EPOCH 170:\n",
      "  batch 1000 loss: 1.4457278653383254\n",
      "Training Accuracy for epoch 169:  0.7570162481536189\n",
      "Validation Accuracy for epoch 169:  0.4827586206896552\n",
      "LOSS train 1.4457278653383254 valid 1.790000557899475\n",
      "EPOCH 171:\n",
      "  batch 1000 loss: 1.4451227703094482\n",
      "Training Accuracy for epoch 170:  0.7570162481536189\n",
      "Validation Accuracy for epoch 170:  0.4827586206896552\n",
      "LOSS train 1.4451227703094482 valid 1.7897223234176636\n",
      "EPOCH 172:\n",
      "  batch 1000 loss: 1.4445048391819\n",
      "Training Accuracy for epoch 171:  0.7570162481536189\n",
      "Validation Accuracy for epoch 171:  0.4827586206896552\n",
      "LOSS train 1.4445048391819 valid 1.7894552946090698\n",
      "EPOCH 173:\n",
      "  batch 1000 loss: 1.4438726012706757\n",
      "Training Accuracy for epoch 172:  0.758493353028065\n",
      "Validation Accuracy for epoch 172:  0.4839901477832512\n",
      "LOSS train 1.4438726012706757 valid 1.7891972064971924\n",
      "EPOCH 174:\n",
      "  batch 1000 loss: 1.4432246741056443\n",
      "Training Accuracy for epoch 173:  0.7607090103397341\n",
      "Validation Accuracy for epoch 173:  0.4852216748768473\n",
      "LOSS train 1.4432246741056443 valid 1.7889509201049805\n",
      "EPOCH 175:\n",
      "  batch 1000 loss: 1.442559698343277\n",
      "Training Accuracy for epoch 174:  0.7614475627769571\n",
      "Validation Accuracy for epoch 174:  0.4852216748768473\n",
      "LOSS train 1.442559698343277 valid 1.7887134552001953\n",
      "EPOCH 176:\n",
      "  batch 1000 loss: 1.4418764355182647\n",
      "Training Accuracy for epoch 175:  0.7621861152141802\n",
      "Validation Accuracy for epoch 175:  0.4852216748768473\n",
      "LOSS train 1.4418764355182647 valid 1.7884910106658936\n",
      "EPOCH 177:\n",
      "  batch 1000 loss: 1.4411738547086717\n",
      "Training Accuracy for epoch 176:  0.7636632200886263\n",
      "Validation Accuracy for epoch 176:  0.4852216748768473\n",
      "LOSS train 1.4411738547086717 valid 1.7882825136184692\n",
      "EPOCH 178:\n",
      "  batch 1000 loss: 1.4404512095451354\n",
      "Training Accuracy for epoch 177:  0.7636632200886263\n",
      "Validation Accuracy for epoch 177:  0.4852216748768473\n",
      "LOSS train 1.4404512095451354 valid 1.7880865335464478\n",
      "EPOCH 179:\n",
      "  batch 1000 loss: 1.4397081027030945\n",
      "Training Accuracy for epoch 178:  0.7644017725258493\n",
      "Validation Accuracy for epoch 178:  0.48645320197044334\n",
      "LOSS train 1.4397081027030945 valid 1.787909984588623\n",
      "EPOCH 180:\n",
      "  batch 1000 loss: 1.4389445828199388\n",
      "Training Accuracy for epoch 179:  0.7658788774002954\n",
      "Validation Accuracy for epoch 179:  0.48645320197044334\n",
      "LOSS train 1.4389445828199388 valid 1.787750244140625\n",
      "EPOCH 181:\n",
      "  batch 1000 loss: 1.4381612737178802\n",
      "Training Accuracy for epoch 180:  0.7680945347119645\n",
      "Validation Accuracy for epoch 180:  0.48645320197044334\n",
      "LOSS train 1.4381612737178802 valid 1.78760826587677\n",
      "EPOCH 182:\n",
      "  batch 1000 loss: 1.4373593736886978\n",
      "Training Accuracy for epoch 181:  0.7703101920236337\n",
      "Validation Accuracy for epoch 181:  0.4876847290640394\n",
      "LOSS train 1.4373593736886978 valid 1.7874845266342163\n",
      "EPOCH 183:\n",
      "  batch 1000 loss: 1.4365407673120498\n",
      "Training Accuracy for epoch 182:  0.7717872968980798\n",
      "Validation Accuracy for epoch 182:  0.4876847290640394\n",
      "LOSS train 1.4365407673120498 valid 1.787381887435913\n",
      "EPOCH 184:\n",
      "  batch 1000 loss: 1.4357079904079437\n",
      "Training Accuracy for epoch 183:  0.7725258493353028\n",
      "Validation Accuracy for epoch 183:  0.4876847290640394\n",
      "LOSS train 1.4357079904079437 valid 1.787302017211914\n",
      "EPOCH 185:\n",
      "  batch 1000 loss: 1.434864218711853\n",
      "Training Accuracy for epoch 184:  0.7732644017725259\n",
      "Validation Accuracy for epoch 184:  0.48891625615763545\n",
      "LOSS train 1.434864218711853 valid 1.7872400283813477\n",
      "EPOCH 186:\n",
      "  batch 1000 loss: 1.4340131347179412\n",
      "Training Accuracy for epoch 185:  0.7740029542097489\n",
      "Validation Accuracy for epoch 185:  0.4876847290640394\n",
      "LOSS train 1.4340131347179412 valid 1.7871979475021362\n",
      "EPOCH 187:\n",
      "  batch 1000 loss: 1.4331588929891586\n",
      "Training Accuracy for epoch 186:  0.776218611521418\n",
      "Validation Accuracy for epoch 186:  0.4876847290640394\n",
      "LOSS train 1.4331588929891586 valid 1.7871755361557007\n",
      "EPOCH 188:\n",
      "  batch 1000 loss: 1.4323058450222015\n",
      "Training Accuracy for epoch 187:  0.7769571639586411\n",
      "Validation Accuracy for epoch 187:  0.4876847290640394\n",
      "LOSS train 1.4323058450222015 valid 1.787170648574829\n",
      "EPOCH 189:\n",
      "  batch 1000 loss: 1.4314584033489228\n",
      "Training Accuracy for epoch 188:  0.7799113737075333\n",
      "Validation Accuracy for epoch 188:  0.48891625615763545\n",
      "LOSS train 1.4314584033489228 valid 1.7871816158294678\n",
      "EPOCH 190:\n",
      "  batch 1000 loss: 1.4306207926273347\n",
      "Training Accuracy for epoch 189:  0.7813884785819794\n",
      "Validation Accuracy for epoch 189:  0.49014778325123154\n",
      "LOSS train 1.4306207926273347 valid 1.7872052192687988\n",
      "EPOCH 191:\n",
      "  batch 1000 loss: 1.4297968225479125\n",
      "Training Accuracy for epoch 190:  0.7843426883308715\n",
      "Validation Accuracy for epoch 190:  0.49014778325123154\n",
      "LOSS train 1.4297968225479125 valid 1.7872390747070312\n",
      "EPOCH 192:\n",
      "  batch 1000 loss: 1.4289897410869599\n",
      "Training Accuracy for epoch 191:  0.7850812407680945\n",
      "Validation Accuracy for epoch 191:  0.49014778325123154\n",
      "LOSS train 1.4289897410869599 valid 1.7872775793075562\n",
      "EPOCH 193:\n",
      "  batch 1000 loss: 1.4282020890712739\n",
      "Training Accuracy for epoch 192:  0.7858197932053176\n",
      "Validation Accuracy for epoch 192:  0.49014778325123154\n",
      "LOSS train 1.4282020890712739 valid 1.7873202562332153\n",
      "EPOCH 194:\n",
      "  batch 1000 loss: 1.427435626745224\n",
      "Training Accuracy for epoch 193:  0.7887740029542097\n",
      "Validation Accuracy for epoch 193:  0.49014778325123154\n",
      "LOSS train 1.427435626745224 valid 1.787360429763794\n",
      "EPOCH 195:\n",
      "  batch 1000 loss: 1.4266913384199142\n",
      "Training Accuracy for epoch 194:  0.7887740029542097\n",
      "Validation Accuracy for epoch 194:  0.49014778325123154\n",
      "LOSS train 1.4266913384199142 valid 1.787398099899292\n",
      "EPOCH 196:\n",
      "  batch 1000 loss: 1.4259694850444793\n",
      "Training Accuracy for epoch 195:  0.7895125553914328\n",
      "Validation Accuracy for epoch 195:  0.49137931034482757\n",
      "LOSS train 1.4259694850444793 valid 1.7874256372451782\n",
      "EPOCH 197:\n",
      "  batch 1000 loss: 1.4252696806192398\n",
      "Training Accuracy for epoch 196:  0.7902511078286558\n",
      "Validation Accuracy for epoch 196:  0.49137931034482757\n",
      "LOSS train 1.4252696806192398 valid 1.7874428033828735\n",
      "EPOCH 198:\n",
      "  batch 1000 loss: 1.4245910769701005\n",
      "Training Accuracy for epoch 197:  0.792466765140325\n",
      "Validation Accuracy for epoch 197:  0.49261083743842365\n",
      "LOSS train 1.4245910769701005 valid 1.7874469757080078\n",
      "EPOCH 199:\n",
      "  batch 1000 loss: 1.4239324057102203\n",
      "Training Accuracy for epoch 198:  0.792466765140325\n",
      "Validation Accuracy for epoch 198:  0.49261083743842365\n",
      "LOSS train 1.4239324057102203 valid 1.7874343395233154\n",
      "EPOCH 200:\n",
      "  batch 1000 loss: 1.4232922086715698\n",
      "Training Accuracy for epoch 199:  0.7939438700147711\n",
      "Validation Accuracy for epoch 199:  0.49261083743842365\n",
      "LOSS train 1.4232922086715698 valid 1.7874029874801636\n"
     ]
    }
   ],
   "source": [
    "# define one training epoch \n",
    "def train_one_epoch(model, train_nodes, train_labels, epoch_index, tb_writer, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        epoch_index: index of the epoch \n",
    "        tb_writer: a writer object that can record all the training statistics \n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    true_pairs = 0\n",
    "\n",
    "    # Work thought the entire dataset\n",
    "    for i, (inputs, labels) in enumerate(zip(train_nodes, train_labels)):\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the accuracy, loss and its gradients\n",
    "        # print(\"Output: \", outputs)\n",
    "        # print(\"Label: \", labels)\n",
    "        if torch.argmax(outputs) == labels: \n",
    "            true_pairs += 1\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_nodes) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        \n",
    "    # Generate Accuracy \n",
    "    print(f\"Training Accuracy for epoch {epoch_index}: \", true_pairs/len(train_nodes))\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/GCN_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model=model, \n",
    "        train_nodes=train_nodes, \n",
    "        train_labels=train_labels, \n",
    "        epoch_index=epoch_number, \n",
    "        tb_writer=writer, \n",
    "        loss_fn=loss, \n",
    "        optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    true_pairs = 0\n",
    "    for i, (vdata, vlabel) in enumerate(zip(validation_nodes, validation_labels)):\n",
    "        voutputs = model(vdata)\n",
    "        vloss = loss(voutputs, vlabel)\n",
    "        running_vloss += vloss\n",
    "        if torch.argmax(voutputs) == vlabel: \n",
    "            true_pairs +=1\n",
    "    print(f\"Validation Accuracy for epoch {epoch}: \", true_pairs/len(validation_nodes))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'GCNs/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\3425545340.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for epoch 199:  0.47601476014760147\n",
      "LOSS test 1.8437950611114502 \n"
     ]
    }
   ],
   "source": [
    "# test GCN model on testing dataset\n",
    "model.train(False) # turn training off for all parameters\n",
    "\n",
    "running_tloss = 0.0\n",
    "true_pairs = 0\n",
    "for i, (tdata, tlabel) in enumerate(zip(test_nodes, test_labels)):\n",
    "    toutputs = model(tdata)\n",
    "    tloss = loss(toutputs, tlabel)\n",
    "    running_tloss += tloss\n",
    "    if torch.argmax(toutputs) == tlabel:\n",
    "        true_pairs += 1 \n",
    "print(f\"Test Accuracy for epoch {epoch}: \", true_pairs/len(test_nodes))\n",
    "\n",
    "avg_tloss = running_tloss / len(test_nodes)\n",
    "print('LOSS test {} '.format(avg_tloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the performance of our 1-layer GCN model with a 1-layer FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a 1 layer FCN \n",
    "class FCN(nn.Module): \n",
    "    def __init__(self, adjacency_matrix, output_size, device) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.A = adjacency_matrix.to(self.device)\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer = nn.Linear(in_features=self.A.shape[1], out_features=self.output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, X): \n",
    "        output = self.hidden_layer(X)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "# Set up the training parameter \n",
    "model = FCN(adjacency_matrix=A, output_size=len(categories.items()), device=device).to(device)\n",
    "# print(\"\\nParameters in the one layer GCN: \\n\")\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "# create cross entropy loss calculation, optimization \n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\959668787.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.9392861096858978\n",
      "Training Accuracy for epoch 0:  0.25997045790251105\n",
      "Validation Accuracy for epoch 0:  0.2894088669950739\n",
      "LOSS train 1.9392861096858978 valid 1.9386861324310303\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.9169930337667465\n",
      "Training Accuracy for epoch 1:  0.37740029542097486\n",
      "Validation Accuracy for epoch 1:  0.2894088669950739\n",
      "LOSS train 1.9169930337667465 valid 1.928459882736206\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.887115882396698\n",
      "Training Accuracy for epoch 2:  0.38552437223042835\n",
      "Validation Accuracy for epoch 2:  0.2894088669950739\n",
      "LOSS train 1.887115882396698 valid 1.91608726978302\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.8513731492757797\n",
      "Training Accuracy for epoch 3:  0.3803545051698671\n",
      "Validation Accuracy for epoch 3:  0.2894088669950739\n",
      "LOSS train 1.8513731492757797 valid 1.9042004346847534\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.815835992217064\n",
      "Training Accuracy for epoch 4:  0.3810930576070901\n",
      "Validation Accuracy for epoch 4:  0.2894088669950739\n",
      "LOSS train 1.815835992217064 valid 1.8952863216400146\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 1.785897097468376\n",
      "Training Accuracy for epoch 5:  0.38404726735598227\n",
      "Validation Accuracy for epoch 5:  0.2894088669950739\n",
      "LOSS train 1.785897097468376 valid 1.8895385265350342\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 1.7626608145236968\n",
      "Training Accuracy for epoch 6:  0.3862629246676514\n",
      "Validation Accuracy for epoch 6:  0.2894088669950739\n",
      "LOSS train 1.7626608145236968 valid 1.8860548734664917\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 1.7444472081661224\n",
      "Training Accuracy for epoch 7:  0.3914327917282127\n",
      "Validation Accuracy for epoch 7:  0.2894088669950739\n",
      "LOSS train 1.7444472081661224 valid 1.8840527534484863\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 1.729200345993042\n",
      "Training Accuracy for epoch 8:  0.4010339734121123\n",
      "Validation Accuracy for epoch 8:  0.2894088669950739\n",
      "LOSS train 1.729200345993042 valid 1.8830379247665405\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 1.7155647809505463\n",
      "Training Accuracy for epoch 9:  0.4113737075332349\n",
      "Validation Accuracy for epoch 9:  0.2894088669950739\n",
      "LOSS train 1.7155647809505463 valid 1.882686972618103\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 1.7029872913360595\n",
      "Training Accuracy for epoch 10:  0.42909896602658787\n",
      "Validation Accuracy for epoch 10:  0.2894088669950739\n",
      "LOSS train 1.7029872913360595 valid 1.8827486038208008\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 1.6914142744541167\n",
      "Training Accuracy for epoch 11:  0.4401772525849335\n",
      "Validation Accuracy for epoch 11:  0.2894088669950739\n",
      "LOSS train 1.6914142744541167 valid 1.8830238580703735\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 1.6809070907831192\n",
      "Training Accuracy for epoch 12:  0.4468242245199409\n",
      "Validation Accuracy for epoch 12:  0.2894088669950739\n",
      "LOSS train 1.6809070907831192 valid 1.8833732604980469\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 1.6714326295852662\n",
      "Training Accuracy for epoch 13:  0.45420974889217136\n",
      "Validation Accuracy for epoch 13:  0.2894088669950739\n",
      "LOSS train 1.6714326295852662 valid 1.8837225437164307\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 1.6628659447431564\n",
      "Training Accuracy for epoch 14:  0.4711964549483013\n",
      "Validation Accuracy for epoch 14:  0.2894088669950739\n",
      "LOSS train 1.6628659447431564 valid 1.8840402364730835\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 1.6550625556707381\n",
      "Training Accuracy for epoch 15:  0.48227474150664695\n",
      "Validation Accuracy for epoch 15:  0.29064039408866993\n",
      "LOSS train 1.6550625556707381 valid 1.8843083381652832\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 1.6479030554294587\n",
      "Training Accuracy for epoch 16:  0.49261447562776955\n",
      "Validation Accuracy for epoch 16:  0.291871921182266\n",
      "LOSS train 1.6479030554294587 valid 1.8845118284225464\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 1.6412998433113097\n",
      "Training Accuracy for epoch 17:  0.5007385524372231\n",
      "Validation Accuracy for epoch 17:  0.29310344827586204\n",
      "LOSS train 1.6412998433113097 valid 1.8846367597579956\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 1.6351873321533203\n",
      "Training Accuracy for epoch 18:  0.5140324963072378\n",
      "Validation Accuracy for epoch 18:  0.29679802955665024\n",
      "LOSS train 1.6351873321533203 valid 1.8846678733825684\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 1.6295112171173096\n",
      "Training Accuracy for epoch 19:  0.5265878877400295\n",
      "Validation Accuracy for epoch 19:  0.30049261083743845\n",
      "LOSS train 1.6295112171173096 valid 1.8846005201339722\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 1.6242233935594559\n",
      "Training Accuracy for epoch 20:  0.5361890694239291\n",
      "Validation Accuracy for epoch 20:  0.3066502463054187\n",
      "LOSS train 1.6242233935594559 valid 1.8844432830810547\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 1.6192808117866515\n",
      "Training Accuracy for epoch 21:  0.5428360413589365\n",
      "Validation Accuracy for epoch 21:  0.3066502463054187\n",
      "LOSS train 1.6192808117866515 valid 1.884203314781189\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 1.6146448986530304\n",
      "Training Accuracy for epoch 22:  0.5465288035450517\n",
      "Validation Accuracy for epoch 22:  0.3066502463054187\n",
      "LOSS train 1.6146448986530304 valid 1.883900761604309\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 1.610281134724617\n",
      "Training Accuracy for epoch 23:  0.553175775480059\n",
      "Validation Accuracy for epoch 23:  0.3066502463054187\n",
      "LOSS train 1.610281134724617 valid 1.8835530281066895\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 1.6061580933332442\n",
      "Training Accuracy for epoch 24:  0.5576070901033974\n",
      "Validation Accuracy for epoch 24:  0.3091133004926108\n",
      "LOSS train 1.6061580933332442 valid 1.8831737041473389\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 1.6022466295957565\n",
      "Training Accuracy for epoch 25:  0.5612998522895125\n",
      "Validation Accuracy for epoch 25:  0.31157635467980294\n",
      "LOSS train 1.6022466295957565 valid 1.8827747106552124\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 1.5985193513631821\n",
      "Training Accuracy for epoch 26:  0.5649926144756278\n",
      "Validation Accuracy for epoch 26:  0.31403940886699505\n",
      "LOSS train 1.5985193513631821 valid 1.8823648691177368\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 1.5949500212669372\n",
      "Training Accuracy for epoch 27:  0.5738552437223042\n",
      "Validation Accuracy for epoch 27:  0.31403940886699505\n",
      "LOSS train 1.5949500212669372 valid 1.8819540739059448\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 1.5915132286548614\n",
      "Training Accuracy for epoch 28:  0.5797636632200887\n",
      "Validation Accuracy for epoch 28:  0.31773399014778325\n",
      "LOSS train 1.5915132286548614 valid 1.8815430402755737\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 1.5881838957071304\n",
      "Training Accuracy for epoch 29:  0.5819793205317577\n",
      "Validation Accuracy for epoch 29:  0.31773399014778325\n",
      "LOSS train 1.5881838957071304 valid 1.8811354637145996\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 1.584937028169632\n",
      "Training Accuracy for epoch 30:  0.5841949778434269\n",
      "Validation Accuracy for epoch 30:  0.31896551724137934\n",
      "LOSS train 1.584937028169632 valid 1.8807367086410522\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 1.5817473119497298\n",
      "Training Accuracy for epoch 31:  0.587149187592319\n",
      "Validation Accuracy for epoch 31:  0.32019704433497537\n",
      "LOSS train 1.5817473119497298 valid 1.8803517818450928\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 1.5785889492034912\n",
      "Training Accuracy for epoch 32:  0.5878877400295421\n",
      "Validation Accuracy for epoch 32:  0.32142857142857145\n",
      "LOSS train 1.5785889492034912 valid 1.8799769878387451\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 1.5754355586767197\n",
      "Training Accuracy for epoch 33:  0.5908419497784343\n",
      "Validation Accuracy for epoch 33:  0.32142857142857145\n",
      "LOSS train 1.5754355586767197 valid 1.8796135187149048\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 1.572260499715805\n",
      "Training Accuracy for epoch 34:  0.5930576070901034\n",
      "Validation Accuracy for epoch 34:  0.32142857142857145\n",
      "LOSS train 1.572260499715805 valid 1.8792681694030762\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 1.5690376409292222\n",
      "Training Accuracy for epoch 35:  0.5989660265878878\n",
      "Validation Accuracy for epoch 35:  0.3251231527093596\n",
      "LOSS train 1.5690376409292222 valid 1.8789386749267578\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 1.5657430478334426\n",
      "Training Accuracy for epoch 36:  0.6026587887740029\n",
      "Validation Accuracy for epoch 36:  0.3251231527093596\n",
      "LOSS train 1.5657430478334426 valid 1.8786273002624512\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 1.5623574192523957\n",
      "Training Accuracy for epoch 37:  0.6093057607090103\n",
      "Validation Accuracy for epoch 37:  0.3251231527093596\n",
      "LOSS train 1.5623574192523957 valid 1.87833571434021\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 1.5588696064949035\n",
      "Training Accuracy for epoch 38:  0.6144756277695717\n",
      "Validation Accuracy for epoch 38:  0.3251231527093596\n",
      "LOSS train 1.5588696064949035 valid 1.8780597448349\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 1.555280174255371\n",
      "Training Accuracy for epoch 39:  0.6211225997045791\n",
      "Validation Accuracy for epoch 39:  0.3263546798029557\n",
      "LOSS train 1.555280174255371 valid 1.8778033256530762\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 1.5516043300628661\n",
      "Training Accuracy for epoch 40:  0.6292466765140325\n",
      "Validation Accuracy for epoch 40:  0.3263546798029557\n",
      "LOSS train 1.5516043300628661 valid 1.8775577545166016\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 1.5478720979690552\n",
      "Training Accuracy for epoch 41:  0.638109305760709\n",
      "Validation Accuracy for epoch 41:  0.3251231527093596\n",
      "LOSS train 1.5478720979690552 valid 1.8773229122161865\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 1.5441249502897263\n",
      "Training Accuracy for epoch 42:  0.6454948301329394\n",
      "Validation Accuracy for epoch 42:  0.32389162561576357\n",
      "LOSS train 1.5441249502897263 valid 1.8770841360092163\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 1.5404089562892913\n",
      "Training Accuracy for epoch 43:  0.6558345642540621\n",
      "Validation Accuracy for epoch 43:  0.32389162561576357\n",
      "LOSS train 1.5404089562892913 valid 1.8768377304077148\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 1.5367667195796966\n",
      "Training Accuracy for epoch 44:  0.6691285081240768\n",
      "Validation Accuracy for epoch 44:  0.3263546798029557\n",
      "LOSS train 1.5367667195796966 valid 1.8765720129013062\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 1.5332309677600862\n",
      "Training Accuracy for epoch 45:  0.6772525849335302\n",
      "Validation Accuracy for epoch 45:  0.3275862068965517\n",
      "LOSS train 1.5332309677600862 valid 1.8762801885604858\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 1.529821632027626\n",
      "Training Accuracy for epoch 46:  0.6831610044313147\n",
      "Validation Accuracy for epoch 46:  0.3275862068965517\n",
      "LOSS train 1.529821632027626 valid 1.87595534324646\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 1.5265466763973237\n",
      "Training Accuracy for epoch 47:  0.6868537666174298\n",
      "Validation Accuracy for epoch 47:  0.3312807881773399\n",
      "LOSS train 1.5265466763973237 valid 1.87559974193573\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 1.5234048938751221\n",
      "Training Accuracy for epoch 48:  0.689807976366322\n",
      "Validation Accuracy for epoch 48:  0.3312807881773399\n",
      "LOSS train 1.5234048938751221 valid 1.8752107620239258\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 1.5203894894123078\n",
      "Training Accuracy for epoch 49:  0.6949778434268833\n",
      "Validation Accuracy for epoch 49:  0.33497536945812806\n",
      "LOSS train 1.5203894894123078 valid 1.8747981786727905\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 1.5174908349514007\n",
      "Training Accuracy for epoch 50:  0.6986706056129985\n",
      "Validation Accuracy for epoch 50:  0.33620689655172414\n",
      "LOSS train 1.5174908349514007 valid 1.8743637800216675\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 1.514698345899582\n",
      "Training Accuracy for epoch 51:  0.7031019202363368\n",
      "Validation Accuracy for epoch 51:  0.3374384236453202\n",
      "LOSS train 1.514698345899582 valid 1.87391197681427\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 1.5120015774965285\n",
      "Training Accuracy for epoch 52:  0.7045790251107829\n",
      "Validation Accuracy for epoch 52:  0.3399014778325123\n",
      "LOSS train 1.5120015774965285 valid 1.8734511137008667\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 1.509390701651573\n",
      "Training Accuracy for epoch 53:  0.705317577548006\n",
      "Validation Accuracy for epoch 53:  0.3411330049261084\n",
      "LOSS train 1.509390701651573 valid 1.8729825019836426\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 1.5068565932512283\n",
      "Training Accuracy for epoch 54:  0.7097488921713442\n",
      "Validation Accuracy for epoch 54:  0.3435960591133005\n",
      "LOSS train 1.5068565932512283 valid 1.8725095987319946\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 1.5043907920122146\n",
      "Training Accuracy for epoch 55:  0.7127031019202363\n",
      "Validation Accuracy for epoch 55:  0.3435960591133005\n",
      "LOSS train 1.5043907920122146 valid 1.872036337852478\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 1.5019854296445847\n",
      "Training Accuracy for epoch 56:  0.7141802067946824\n",
      "Validation Accuracy for epoch 56:  0.3472906403940887\n",
      "LOSS train 1.5019854296445847 valid 1.8715661764144897\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 1.4996330662965776\n",
      "Training Accuracy for epoch 57:  0.7178729689807977\n",
      "Validation Accuracy for epoch 57:  0.35098522167487683\n",
      "LOSS train 1.4996330662965776 valid 1.8711001873016357\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 1.4973266466856003\n",
      "Training Accuracy for epoch 58:  0.7215657311669128\n",
      "Validation Accuracy for epoch 58:  0.3522167487684729\n",
      "LOSS train 1.4973266466856003 valid 1.8706386089324951\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 1.4950593823194505\n",
      "Training Accuracy for epoch 59:  0.723781388478582\n",
      "Validation Accuracy for epoch 59:  0.35714285714285715\n",
      "LOSS train 1.4950593823194505 valid 1.8701831102371216\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 1.4928248279094696\n",
      "Training Accuracy for epoch 60:  0.725258493353028\n",
      "Validation Accuracy for epoch 60:  0.3620689655172414\n",
      "LOSS train 1.4928248279094696 valid 1.8697373867034912\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 1.4906169008016585\n",
      "Training Accuracy for epoch 61:  0.7274741506646972\n",
      "Validation Accuracy for epoch 61:  0.3657635467980296\n",
      "LOSS train 1.4906169008016585 valid 1.8692998886108398\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 1.488429888486862\n",
      "Training Accuracy for epoch 62:  0.7304283604135894\n",
      "Validation Accuracy for epoch 62:  0.3645320197044335\n",
      "LOSS train 1.488429888486862 valid 1.8688733577728271\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 1.4862586977481842\n",
      "Training Accuracy for epoch 63:  0.7326440177252584\n",
      "Validation Accuracy for epoch 63:  0.3633004926108374\n",
      "LOSS train 1.4862586977481842 valid 1.868456482887268\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 1.484098897576332\n",
      "Training Accuracy for epoch 64:  0.740029542097489\n",
      "Validation Accuracy for epoch 64:  0.3633004926108374\n",
      "LOSS train 1.484098897576332 valid 1.8680518865585327\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 1.4819471006393432\n",
      "Training Accuracy for epoch 65:  0.741506646971935\n",
      "Validation Accuracy for epoch 65:  0.3657635467980296\n",
      "LOSS train 1.4819471006393432 valid 1.8676605224609375\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 1.4798009532690048\n",
      "Training Accuracy for epoch 66:  0.741506646971935\n",
      "Validation Accuracy for epoch 66:  0.3669950738916256\n",
      "LOSS train 1.4798009532690048 valid 1.8672821521759033\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 1.4776594686508178\n",
      "Training Accuracy for epoch 67:  0.7451994091580503\n",
      "Validation Accuracy for epoch 67:  0.3694581280788177\n",
      "LOSS train 1.4776594686508178 valid 1.8669168949127197\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 1.4755231466293335\n",
      "Training Accuracy for epoch 68:  0.7481536189069424\n",
      "Validation Accuracy for epoch 68:  0.3694581280788177\n",
      "LOSS train 1.4755231466293335 valid 1.8665649890899658\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 1.473394053220749\n",
      "Training Accuracy for epoch 69:  0.7533234859675036\n",
      "Validation Accuracy for epoch 69:  0.3706896551724138\n",
      "LOSS train 1.473394053220749 valid 1.8662266731262207\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 1.4712758840322495\n",
      "Training Accuracy for epoch 70:  0.7548005908419497\n",
      "Validation Accuracy for epoch 70:  0.37192118226600984\n",
      "LOSS train 1.4712758840322495 valid 1.8659006357192993\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 1.4691737245321275\n",
      "Training Accuracy for epoch 71:  0.757754800590842\n",
      "Validation Accuracy for epoch 71:  0.37192118226600984\n",
      "LOSS train 1.4691737245321275 valid 1.865587830543518\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 1.467093838095665\n",
      "Training Accuracy for epoch 72:  0.758493353028065\n",
      "Validation Accuracy for epoch 72:  0.3706896551724138\n",
      "LOSS train 1.467093838095665 valid 1.865286946296692\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 1.4650429971218109\n",
      "Training Accuracy for epoch 73:  0.7621861152141802\n",
      "Validation Accuracy for epoch 73:  0.37438423645320196\n",
      "LOSS train 1.4650429971218109 valid 1.8649938106536865\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 1.4630280386209489\n",
      "Training Accuracy for epoch 74:  0.7614475627769571\n",
      "Validation Accuracy for epoch 74:  0.37561576354679804\n",
      "LOSS train 1.4630280386209489 valid 1.8647081851959229\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 1.4610551469326019\n",
      "Training Accuracy for epoch 75:  0.7621861152141802\n",
      "Validation Accuracy for epoch 75:  0.3768472906403941\n",
      "LOSS train 1.4610551469326019 valid 1.8644262552261353\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 1.4591294345855712\n",
      "Training Accuracy for epoch 76:  0.7629246676514032\n",
      "Validation Accuracy for epoch 76:  0.3793103448275862\n",
      "LOSS train 1.4591294345855712 valid 1.864150047302246\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 1.4572545057535171\n",
      "Training Accuracy for epoch 77:  0.7644017725258493\n",
      "Validation Accuracy for epoch 77:  0.3805418719211823\n",
      "LOSS train 1.4572545057535171 valid 1.863871455192566\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 1.4554324530363083\n",
      "Training Accuracy for epoch 78:  0.7658788774002954\n",
      "Validation Accuracy for epoch 78:  0.3817733990147783\n",
      "LOSS train 1.4554324530363083 valid 1.863596796989441\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 1.4536638391017913\n",
      "Training Accuracy for epoch 79:  0.7666174298375185\n",
      "Validation Accuracy for epoch 79:  0.3830049261083744\n",
      "LOSS train 1.4536638391017913 valid 1.8633170127868652\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 1.4519479391574859\n",
      "Training Accuracy for epoch 80:  0.7673559822747416\n",
      "Validation Accuracy for epoch 80:  0.3830049261083744\n",
      "LOSS train 1.4519479391574859 valid 1.8630348443984985\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 1.4502830238342286\n",
      "Training Accuracy for epoch 81:  0.7688330871491876\n",
      "Validation Accuracy for epoch 81:  0.3854679802955665\n",
      "LOSS train 1.4502830238342286 valid 1.8627474308013916\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 1.4486666921377183\n",
      "Training Accuracy for epoch 82:  0.7703101920236337\n",
      "Validation Accuracy for epoch 82:  0.3866995073891626\n",
      "LOSS train 1.4486666921377183 valid 1.8624578714370728\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 1.4470960818529128\n",
      "Training Accuracy for epoch 83:  0.7725258493353028\n",
      "Validation Accuracy for epoch 83:  0.3879310344827586\n",
      "LOSS train 1.4470960818529128 valid 1.8621629476547241\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 1.4455681231021882\n",
      "Training Accuracy for epoch 84:  0.7725258493353028\n",
      "Validation Accuracy for epoch 84:  0.3891625615763547\n",
      "LOSS train 1.4455681231021882 valid 1.861863613128662\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 1.444079732656479\n",
      "Training Accuracy for epoch 85:  0.7732644017725259\n",
      "Validation Accuracy for epoch 85:  0.3891625615763547\n",
      "LOSS train 1.444079732656479 valid 1.8615609407424927\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 1.4426278961896897\n",
      "Training Accuracy for epoch 86:  0.776218611521418\n",
      "Validation Accuracy for epoch 86:  0.3916256157635468\n",
      "LOSS train 1.4426278961896897 valid 1.8612524271011353\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 1.4412098203897477\n",
      "Training Accuracy for epoch 87:  0.7776957163958641\n",
      "Validation Accuracy for epoch 87:  0.39285714285714285\n",
      "LOSS train 1.4412098203897477 valid 1.8609411716461182\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 1.4398228789567948\n",
      "Training Accuracy for epoch 88:  0.7791728212703102\n",
      "Validation Accuracy for epoch 88:  0.39532019704433496\n",
      "LOSS train 1.4398228789567948 valid 1.8606284856796265\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 1.4384647195339202\n",
      "Training Accuracy for epoch 89:  0.7813884785819794\n",
      "Validation Accuracy for epoch 89:  0.39655172413793105\n",
      "LOSS train 1.4384647195339202 valid 1.8603101968765259\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 1.4371332346200942\n",
      "Training Accuracy for epoch 90:  0.7836041358936484\n",
      "Validation Accuracy for epoch 90:  0.39655172413793105\n",
      "LOSS train 1.4371332346200942 valid 1.859989881515503\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 1.4358265606164933\n",
      "Training Accuracy for epoch 91:  0.7850812407680945\n",
      "Validation Accuracy for epoch 91:  0.39655172413793105\n",
      "LOSS train 1.4358265606164933 valid 1.8596677780151367\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 1.4345430685281753\n",
      "Training Accuracy for epoch 92:  0.7858197932053176\n",
      "Validation Accuracy for epoch 92:  0.39655172413793105\n",
      "LOSS train 1.4345430685281753 valid 1.859342336654663\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 1.4332813801765443\n",
      "Training Accuracy for epoch 93:  0.7865583456425406\n",
      "Validation Accuracy for epoch 93:  0.39655172413793105\n",
      "LOSS train 1.4332813801765443 valid 1.8590188026428223\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 1.4320402537584305\n",
      "Training Accuracy for epoch 94:  0.7872968980797637\n",
      "Validation Accuracy for epoch 94:  0.3977832512315271\n",
      "LOSS train 1.4320402537584305 valid 1.858693242073059\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 1.4308186526298523\n",
      "Training Accuracy for epoch 95:  0.7880354505169868\n",
      "Validation Accuracy for epoch 95:  0.3977832512315271\n",
      "LOSS train 1.4308186526298523 valid 1.8583670854568481\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 1.4296156879663466\n",
      "Training Accuracy for epoch 96:  0.7887740029542097\n",
      "Validation Accuracy for epoch 96:  0.4002463054187192\n",
      "LOSS train 1.4296156879663466 valid 1.858039379119873\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 1.4284305970668794\n",
      "Training Accuracy for epoch 97:  0.7909896602658789\n",
      "Validation Accuracy for epoch 97:  0.4014778325123153\n",
      "LOSS train 1.4284305970668794 valid 1.8577117919921875\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 1.4272626945972442\n",
      "Training Accuracy for epoch 98:  0.7917282127031019\n",
      "Validation Accuracy for epoch 98:  0.4027093596059113\n",
      "LOSS train 1.4272626945972442 valid 1.8573821783065796\n",
      "EPOCH 100:\n",
      "  batch 1000 loss: 1.426111394405365\n",
      "Training Accuracy for epoch 99:  0.793205317577548\n",
      "Validation Accuracy for epoch 99:  0.4039408866995074\n",
      "LOSS train 1.426111394405365 valid 1.8570557832717896\n",
      "EPOCH 101:\n",
      "  batch 1000 loss: 1.424976176738739\n",
      "Training Accuracy for epoch 100:  0.7939438700147711\n",
      "Validation Accuracy for epoch 100:  0.4051724137931034\n",
      "LOSS train 1.424976176738739 valid 1.856726884841919\n",
      "EPOCH 102:\n",
      "  batch 1000 loss: 1.4238565294742584\n",
      "Training Accuracy for epoch 101:  0.7939438700147711\n",
      "Validation Accuracy for epoch 101:  0.4051724137931034\n",
      "LOSS train 1.4238565294742584 valid 1.8564000129699707\n",
      "EPOCH 103:\n",
      "  batch 1000 loss: 1.4227520076036453\n",
      "Training Accuracy for epoch 102:  0.794682422451994\n",
      "Validation Accuracy for epoch 102:  0.4051724137931034\n",
      "LOSS train 1.4227520076036453 valid 1.856072187423706\n",
      "EPOCH 104:\n",
      "  batch 1000 loss: 1.4216621664762497\n",
      "Training Accuracy for epoch 103:  0.7961595273264401\n",
      "Validation Accuracy for epoch 103:  0.4064039408866995\n",
      "LOSS train 1.4216621664762497 valid 1.8557454347610474\n",
      "EPOCH 105:\n",
      "  batch 1000 loss: 1.4205865794420243\n",
      "Training Accuracy for epoch 104:  0.7968980797636632\n",
      "Validation Accuracy for epoch 104:  0.40763546798029554\n",
      "LOSS train 1.4205865794420243 valid 1.8554164171218872\n",
      "EPOCH 106:\n",
      "  batch 1000 loss: 1.4195248186588287\n",
      "Training Accuracy for epoch 105:  0.7983751846381093\n",
      "Validation Accuracy for epoch 105:  0.4088669950738916\n",
      "LOSS train 1.4195248186588287 valid 1.8550904989242554\n",
      "EPOCH 107:\n",
      "  batch 1000 loss: 1.4184764566421508\n",
      "Training Accuracy for epoch 106:  0.7991137370753324\n",
      "Validation Accuracy for epoch 106:  0.4088669950738916\n",
      "LOSS train 1.4184764566421508 valid 1.8547616004943848\n",
      "EPOCH 108:\n",
      "  batch 1000 loss: 1.4174410425424575\n",
      "Training Accuracy for epoch 107:  0.7991137370753324\n",
      "Validation Accuracy for epoch 107:  0.4088669950738916\n",
      "LOSS train 1.4174410425424575 valid 1.8544343709945679\n",
      "EPOCH 109:\n",
      "  batch 1000 loss: 1.4164181510210037\n",
      "Training Accuracy for epoch 108:  0.7991137370753324\n",
      "Validation Accuracy for epoch 108:  0.4100985221674877\n",
      "LOSS train 1.4164181510210037 valid 1.8541063070297241\n",
      "EPOCH 110:\n",
      "  batch 1000 loss: 1.41540731215477\n",
      "Training Accuracy for epoch 109:  0.8005908419497785\n",
      "Validation Accuracy for epoch 109:  0.4100985221674877\n",
      "LOSS train 1.41540731215477 valid 1.85378098487854\n",
      "EPOCH 111:\n",
      "  batch 1000 loss: 1.4144080587625503\n",
      "Training Accuracy for epoch 110:  0.8013293943870015\n",
      "Validation Accuracy for epoch 110:  0.4100985221674877\n",
      "LOSS train 1.4144080587625503 valid 1.8534519672393799\n",
      "EPOCH 112:\n",
      "  batch 1000 loss: 1.413419909477234\n",
      "Training Accuracy for epoch 111:  0.8020679468242246\n",
      "Validation Accuracy for epoch 111:  0.4100985221674877\n",
      "LOSS train 1.413419909477234 valid 1.8531237840652466\n",
      "EPOCH 113:\n",
      "  batch 1000 loss: 1.412442326068878\n",
      "Training Accuracy for epoch 112:  0.8035450516986706\n",
      "Validation Accuracy for epoch 112:  0.41133004926108374\n",
      "LOSS train 1.412442326068878 valid 1.852797508239746\n",
      "EPOCH 114:\n",
      "  batch 1000 loss: 1.4114747661352158\n",
      "Training Accuracy for epoch 113:  0.8050221565731167\n",
      "Validation Accuracy for epoch 113:  0.4100985221674877\n",
      "LOSS train 1.4114747661352158 valid 1.8524689674377441\n",
      "EPOCH 115:\n",
      "  batch 1000 loss: 1.4105166726112366\n",
      "Training Accuracy for epoch 114:  0.8094534711964549\n",
      "Validation Accuracy for epoch 114:  0.4125615763546798\n",
      "LOSS train 1.4105166726112366 valid 1.8521393537521362\n",
      "EPOCH 116:\n",
      "  batch 1000 loss: 1.409567434310913\n",
      "Training Accuracy for epoch 115:  0.8116691285081241\n",
      "Validation Accuracy for epoch 115:  0.41133004926108374\n",
      "LOSS train 1.409567434310913 valid 1.8518118858337402\n",
      "EPOCH 117:\n",
      "  batch 1000 loss: 1.408626421689987\n",
      "Training Accuracy for epoch 116:  0.8131462333825702\n",
      "Validation Accuracy for epoch 116:  0.4125615763546798\n",
      "LOSS train 1.408626421689987 valid 1.8514796495437622\n",
      "EPOCH 118:\n",
      "  batch 1000 loss: 1.4076930122375488\n",
      "Training Accuracy for epoch 117:  0.8146233382570163\n",
      "Validation Accuracy for epoch 117:  0.4125615763546798\n",
      "LOSS train 1.4076930122375488 valid 1.851150393486023\n",
      "EPOCH 119:\n",
      "  batch 1000 loss: 1.4067665821313857\n",
      "Training Accuracy for epoch 118:  0.8153618906942393\n",
      "Validation Accuracy for epoch 118:  0.41379310344827586\n",
      "LOSS train 1.4067665821313857 valid 1.8508198261260986\n",
      "EPOCH 120:\n",
      "  batch 1000 loss: 1.4058465886116027\n",
      "Training Accuracy for epoch 119:  0.8183161004431314\n",
      "Validation Accuracy for epoch 119:  0.41502463054187194\n",
      "LOSS train 1.4058465886116027 valid 1.8504842519760132\n",
      "EPOCH 121:\n",
      "  batch 1000 loss: 1.4049325535297394\n",
      "Training Accuracy for epoch 120:  0.8190546528803545\n",
      "Validation Accuracy for epoch 120:  0.41502463054187194\n",
      "LOSS train 1.4049325535297394 valid 1.8501509428024292\n",
      "EPOCH 122:\n",
      "  batch 1000 loss: 1.4040241981744765\n",
      "Training Accuracy for epoch 121:  0.8197932053175776\n",
      "Validation Accuracy for epoch 121:  0.41502463054187194\n",
      "LOSS train 1.4040241981744765 valid 1.8498133420944214\n",
      "EPOCH 123:\n",
      "  batch 1000 loss: 1.4031214768886566\n",
      "Training Accuracy for epoch 122:  0.8205317577548006\n",
      "Validation Accuracy for epoch 122:  0.41502463054187194\n",
      "LOSS train 1.4031214768886566 valid 1.8494728803634644\n",
      "EPOCH 124:\n",
      "  batch 1000 loss: 1.4022246301174164\n",
      "Training Accuracy for epoch 123:  0.8212703101920237\n",
      "Validation Accuracy for epoch 123:  0.41625615763546797\n",
      "LOSS train 1.4022246301174164 valid 1.8491321802139282\n",
      "EPOCH 125:\n",
      "  batch 1000 loss: 1.4013341572284699\n",
      "Training Accuracy for epoch 124:  0.8220088626292467\n",
      "Validation Accuracy for epoch 124:  0.41748768472906406\n",
      "LOSS train 1.4013341572284699 valid 1.8487865924835205\n",
      "EPOCH 126:\n",
      "  batch 1000 loss: 1.4004507845640182\n",
      "Training Accuracy for epoch 125:  0.8220088626292467\n",
      "Validation Accuracy for epoch 125:  0.41995073891625617\n",
      "LOSS train 1.4004507845640182 valid 1.848439335823059\n",
      "EPOCH 127:\n",
      "  batch 1000 loss: 1.3995752764940261\n",
      "Training Accuracy for epoch 126:  0.8227474150664698\n",
      "Validation Accuracy for epoch 126:  0.41995073891625617\n",
      "LOSS train 1.3995752764940261 valid 1.8480874300003052\n",
      "EPOCH 128:\n",
      "  batch 1000 loss: 1.3987083595991134\n",
      "Training Accuracy for epoch 127:  0.8242245199409158\n",
      "Validation Accuracy for epoch 127:  0.4224137931034483\n",
      "LOSS train 1.3987083595991134 valid 1.8477331399917603\n",
      "EPOCH 129:\n",
      "  batch 1000 loss: 1.3978505084514619\n",
      "Training Accuracy for epoch 128:  0.8242245199409158\n",
      "Validation Accuracy for epoch 128:  0.4224137931034483\n",
      "LOSS train 1.3978505084514619 valid 1.847375750541687\n",
      "EPOCH 130:\n",
      "  batch 1000 loss: 1.3970019507408142\n",
      "Training Accuracy for epoch 129:  0.8242245199409158\n",
      "Validation Accuracy for epoch 129:  0.42610837438423643\n",
      "LOSS train 1.3970019507408142 valid 1.8470147848129272\n",
      "EPOCH 131:\n",
      "  batch 1000 loss: 1.3961626549959183\n",
      "Training Accuracy for epoch 130:  0.8249630723781388\n",
      "Validation Accuracy for epoch 130:  0.42610837438423643\n",
      "LOSS train 1.3961626549959183 valid 1.8466520309448242\n",
      "EPOCH 132:\n",
      "  batch 1000 loss: 1.3953323594331741\n",
      "Training Accuracy for epoch 131:  0.8249630723781388\n",
      "Validation Accuracy for epoch 131:  0.42610837438423643\n",
      "LOSS train 1.3953323594331741 valid 1.846286416053772\n",
      "EPOCH 133:\n",
      "  batch 1000 loss: 1.3945107151269913\n",
      "Training Accuracy for epoch 132:  0.8257016248153619\n",
      "Validation Accuracy for epoch 132:  0.4248768472906404\n",
      "LOSS train 1.3945107151269913 valid 1.84591805934906\n",
      "EPOCH 134:\n",
      "  batch 1000 loss: 1.3936972445249558\n",
      "Training Accuracy for epoch 133:  0.827178729689808\n",
      "Validation Accuracy for epoch 133:  0.4248768472906404\n",
      "LOSS train 1.3936972445249558 valid 1.8455471992492676\n",
      "EPOCH 135:\n",
      "  batch 1000 loss: 1.3928914787769318\n",
      "Training Accuracy for epoch 134:  0.827917282127031\n",
      "Validation Accuracy for epoch 134:  0.42610837438423643\n",
      "LOSS train 1.3928914787769318 valid 1.8451752662658691\n",
      "EPOCH 136:\n",
      "  batch 1000 loss: 1.3920929518938066\n",
      "Training Accuracy for epoch 135:  0.827917282127031\n",
      "Validation Accuracy for epoch 135:  0.4273399014778325\n",
      "LOSS train 1.3920929518938066 valid 1.8448015451431274\n",
      "EPOCH 137:\n",
      "  batch 1000 loss: 1.391301259994507\n",
      "Training Accuracy for epoch 136:  0.829394387001477\n",
      "Validation Accuracy for epoch 136:  0.4273399014778325\n",
      "LOSS train 1.391301259994507 valid 1.8444262742996216\n",
      "EPOCH 138:\n",
      "  batch 1000 loss: 1.390516011953354\n",
      "Training Accuracy for epoch 137:  0.829394387001477\n",
      "Validation Accuracy for epoch 137:  0.4273399014778325\n",
      "LOSS train 1.390516011953354 valid 1.8440495729446411\n",
      "EPOCH 139:\n",
      "  batch 1000 loss: 1.3897368861436843\n",
      "Training Accuracy for epoch 138:  0.8301329394387001\n",
      "Validation Accuracy for epoch 138:  0.4273399014778325\n",
      "LOSS train 1.3897368861436843 valid 1.8436682224273682\n",
      "EPOCH 140:\n",
      "  batch 1000 loss: 1.3889635943174363\n",
      "Training Accuracy for epoch 139:  0.8308714918759232\n",
      "Validation Accuracy for epoch 139:  0.42610837438423643\n",
      "LOSS train 1.3889635943174363 valid 1.8432860374450684\n",
      "EPOCH 141:\n",
      "  batch 1000 loss: 1.3881958904266358\n",
      "Training Accuracy for epoch 140:  0.8316100443131462\n",
      "Validation Accuracy for epoch 140:  0.42610837438423643\n",
      "LOSS train 1.3881958904266358 valid 1.842903733253479\n",
      "EPOCH 142:\n",
      "  batch 1000 loss: 1.3874335551261903\n",
      "Training Accuracy for epoch 141:  0.8316100443131462\n",
      "Validation Accuracy for epoch 141:  0.42610837438423643\n",
      "LOSS train 1.3874335551261903 valid 1.8425171375274658\n",
      "EPOCH 143:\n",
      "  batch 1000 loss: 1.3866763809919358\n",
      "Training Accuracy for epoch 142:  0.8316100443131462\n",
      "Validation Accuracy for epoch 142:  0.42610837438423643\n",
      "LOSS train 1.3866763809919358 valid 1.8421279191970825\n",
      "EPOCH 144:\n",
      "  batch 1000 loss: 1.3859242167472838\n",
      "Training Accuracy for epoch 143:  0.8316100443131462\n",
      "Validation Accuracy for epoch 143:  0.42610837438423643\n",
      "LOSS train 1.3859242167472838 valid 1.8417378664016724\n",
      "EPOCH 145:\n",
      "  batch 1000 loss: 1.3851768944263458\n",
      "Training Accuracy for epoch 144:  0.8316100443131462\n",
      "Validation Accuracy for epoch 144:  0.42610837438423643\n",
      "LOSS train 1.3851768944263458 valid 1.841344952583313\n",
      "EPOCH 146:\n",
      "  batch 1000 loss: 1.3844342353343964\n",
      "Training Accuracy for epoch 145:  0.8323485967503693\n",
      "Validation Accuracy for epoch 145:  0.42610837438423643\n",
      "LOSS train 1.3844342353343964 valid 1.8409507274627686\n",
      "EPOCH 147:\n",
      "  batch 1000 loss: 1.3836961147785187\n",
      "Training Accuracy for epoch 146:  0.8353028064992615\n",
      "Validation Accuracy for epoch 146:  0.42610837438423643\n",
      "LOSS train 1.3836961147785187 valid 1.840552806854248\n",
      "EPOCH 148:\n",
      "  batch 1000 loss: 1.38296235704422\n",
      "Training Accuracy for epoch 147:  0.8360413589364845\n",
      "Validation Accuracy for epoch 147:  0.4273399014778325\n",
      "LOSS train 1.38296235704422 valid 1.8401525020599365\n",
      "EPOCH 149:\n",
      "  batch 1000 loss: 1.3822328104972839\n",
      "Training Accuracy for epoch 148:  0.8382570162481536\n",
      "Validation Accuracy for epoch 148:  0.4273399014778325\n",
      "LOSS train 1.3822328104972839 valid 1.8397504091262817\n",
      "EPOCH 150:\n",
      "  batch 1000 loss: 1.381507295012474\n",
      "Training Accuracy for epoch 149:  0.8382570162481536\n",
      "Validation Accuracy for epoch 149:  0.42857142857142855\n",
      "LOSS train 1.381507295012474 valid 1.8393441438674927\n",
      "EPOCH 151:\n",
      "  batch 1000 loss: 1.380785647392273\n",
      "Training Accuracy for epoch 150:  0.8389955686853766\n",
      "Validation Accuracy for epoch 150:  0.42857142857142855\n",
      "LOSS train 1.380785647392273 valid 1.83893620967865\n",
      "EPOCH 152:\n",
      "  batch 1000 loss: 1.38006766474247\n",
      "Training Accuracy for epoch 151:  0.8389955686853766\n",
      "Validation Accuracy for epoch 151:  0.42857142857142855\n",
      "LOSS train 1.38006766474247 valid 1.8385266065597534\n",
      "EPOCH 153:\n",
      "  batch 1000 loss: 1.3793531495332718\n",
      "Training Accuracy for epoch 152:  0.8389955686853766\n",
      "Validation Accuracy for epoch 152:  0.42857142857142855\n",
      "LOSS train 1.3793531495332718 valid 1.8381116390228271\n",
      "EPOCH 154:\n",
      "  batch 1000 loss: 1.3786418914794922\n",
      "Training Accuracy for epoch 153:  0.8397341211225997\n",
      "Validation Accuracy for epoch 153:  0.42857142857142855\n",
      "LOSS train 1.3786418914794922 valid 1.8376942873001099\n",
      "EPOCH 155:\n",
      "  batch 1000 loss: 1.3779336900711059\n",
      "Training Accuracy for epoch 154:  0.8412112259970458\n",
      "Validation Accuracy for epoch 154:  0.42857142857142855\n",
      "LOSS train 1.3779336900711059 valid 1.837274193763733\n",
      "EPOCH 156:\n",
      "  batch 1000 loss: 1.3772282935380935\n",
      "Training Accuracy for epoch 155:  0.8412112259970458\n",
      "Validation Accuracy for epoch 155:  0.42857142857142855\n",
      "LOSS train 1.3772282935380935 valid 1.836850643157959\n",
      "EPOCH 157:\n",
      "  batch 1000 loss: 1.3765254992246627\n",
      "Training Accuracy for epoch 156:  0.8426883308714919\n",
      "Validation Accuracy for epoch 156:  0.42857142857142855\n",
      "LOSS train 1.3765254992246627 valid 1.836423635482788\n",
      "EPOCH 158:\n",
      "  batch 1000 loss: 1.3758250963687897\n",
      "Training Accuracy for epoch 157:  0.844903988183161\n",
      "Validation Accuracy for epoch 157:  0.42980295566502463\n",
      "LOSS train 1.3758250963687897 valid 1.8359932899475098\n",
      "EPOCH 159:\n",
      "  batch 1000 loss: 1.3751268594264985\n",
      "Training Accuracy for epoch 158:  0.844903988183161\n",
      "Validation Accuracy for epoch 158:  0.42980295566502463\n",
      "LOSS train 1.3751268594264985 valid 1.8355575799942017\n",
      "EPOCH 160:\n",
      "  batch 1000 loss: 1.3744305835962296\n",
      "Training Accuracy for epoch 159:  0.8471196454948301\n",
      "Validation Accuracy for epoch 159:  0.43226600985221675\n",
      "LOSS train 1.3744305835962296 valid 1.8351190090179443\n",
      "EPOCH 161:\n",
      "  batch 1000 loss: 1.3737360898256301\n",
      "Training Accuracy for epoch 160:  0.8471196454948301\n",
      "Validation Accuracy for epoch 160:  0.43226600985221675\n",
      "LOSS train 1.3737360898256301 valid 1.8346744775772095\n",
      "EPOCH 162:\n",
      "  batch 1000 loss: 1.3730431777238845\n",
      "Training Accuracy for epoch 161:  0.8478581979320532\n",
      "Validation Accuracy for epoch 161:  0.43349753694581283\n",
      "LOSS train 1.3730431777238845 valid 1.8342270851135254\n",
      "EPOCH 163:\n",
      "  batch 1000 loss: 1.372351702928543\n",
      "Training Accuracy for epoch 162:  0.8478581979320532\n",
      "Validation Accuracy for epoch 162:  0.43349753694581283\n",
      "LOSS train 1.372351702928543 valid 1.8337748050689697\n",
      "EPOCH 164:\n",
      "  batch 1000 loss: 1.3716615425348282\n",
      "Training Accuracy for epoch 163:  0.8500738552437223\n",
      "Validation Accuracy for epoch 163:  0.43349753694581283\n",
      "LOSS train 1.3716615425348282 valid 1.8333172798156738\n",
      "EPOCH 165:\n",
      "  batch 1000 loss: 1.370972584247589\n",
      "Training Accuracy for epoch 164:  0.8508124076809453\n",
      "Validation Accuracy for epoch 164:  0.43349753694581283\n",
      "LOSS train 1.370972584247589 valid 1.832851529121399\n",
      "EPOCH 166:\n",
      "  batch 1000 loss: 1.370284716129303\n",
      "Training Accuracy for epoch 165:  0.8508124076809453\n",
      "Validation Accuracy for epoch 165:  0.43349753694581283\n",
      "LOSS train 1.370284716129303 valid 1.8323814868927002\n",
      "EPOCH 167:\n",
      "  batch 1000 loss: 1.3695979145765305\n",
      "Training Accuracy for epoch 166:  0.8545051698670606\n",
      "Validation Accuracy for epoch 166:  0.43472906403940886\n",
      "LOSS train 1.3695979145765305 valid 1.8319065570831299\n",
      "EPOCH 168:\n",
      "  batch 1000 loss: 1.36891213285923\n",
      "Training Accuracy for epoch 167:  0.8552437223042836\n",
      "Validation Accuracy for epoch 167:  0.43472906403940886\n",
      "LOSS train 1.36891213285923 valid 1.8314250707626343\n",
      "EPOCH 169:\n",
      "  batch 1000 loss: 1.3682273468971253\n",
      "Training Accuracy for epoch 168:  0.8559822747415067\n",
      "Validation Accuracy for epoch 168:  0.43472906403940886\n",
      "LOSS train 1.3682273468971253 valid 1.8309379816055298\n",
      "EPOCH 170:\n",
      "  batch 1000 loss: 1.367543587565422\n",
      "Training Accuracy for epoch 169:  0.8574593796159528\n",
      "Validation Accuracy for epoch 169:  0.43472906403940886\n",
      "LOSS train 1.367543587565422 valid 1.8304431438446045\n",
      "EPOCH 171:\n",
      "  batch 1000 loss: 1.3668608392477035\n",
      "Training Accuracy for epoch 170:  0.8581979320531757\n",
      "Validation Accuracy for epoch 170:  0.43472906403940886\n",
      "LOSS train 1.3668608392477035 valid 1.8299424648284912\n",
      "EPOCH 172:\n",
      "  batch 1000 loss: 1.3661791616678238\n",
      "Training Accuracy for epoch 171:  0.8589364844903988\n",
      "Validation Accuracy for epoch 171:  0.43596059113300495\n",
      "LOSS train 1.3661791616678238 valid 1.8294364213943481\n",
      "EPOCH 173:\n",
      "  batch 1000 loss: 1.3654985655546188\n",
      "Training Accuracy for epoch 172:  0.8596750369276218\n",
      "Validation Accuracy for epoch 172:  0.43596059113300495\n",
      "LOSS train 1.3654985655546188 valid 1.8289220333099365\n",
      "EPOCH 174:\n",
      "  batch 1000 loss: 1.3648191124200821\n",
      "Training Accuracy for epoch 173:  0.8604135893648449\n",
      "Validation Accuracy for epoch 173:  0.43596059113300495\n",
      "LOSS train 1.3648191124200821 valid 1.8284037113189697\n",
      "EPOCH 175:\n",
      "  batch 1000 loss: 1.36414082634449\n",
      "Training Accuracy for epoch 174:  0.8611521418020679\n",
      "Validation Accuracy for epoch 174:  0.437192118226601\n",
      "LOSS train 1.36414082634449 valid 1.8278778791427612\n",
      "EPOCH 176:\n",
      "  batch 1000 loss: 1.363463742852211\n",
      "Training Accuracy for epoch 175:  0.8626292466765141\n",
      "Validation Accuracy for epoch 175:  0.437192118226601\n",
      "LOSS train 1.363463742852211 valid 1.8273478746414185\n",
      "EPOCH 177:\n",
      "  batch 1000 loss: 1.3627878679037093\n",
      "Training Accuracy for epoch 176:  0.8641063515509602\n",
      "Validation Accuracy for epoch 176:  0.43842364532019706\n",
      "LOSS train 1.3627878679037093 valid 1.8268115520477295\n",
      "EPOCH 178:\n",
      "  batch 1000 loss: 1.3621132276058197\n",
      "Training Accuracy for epoch 177:  0.8641063515509602\n",
      "Validation Accuracy for epoch 177:  0.43842364532019706\n",
      "LOSS train 1.3621132276058197 valid 1.8262699842453003\n",
      "EPOCH 179:\n",
      "  batch 1000 loss: 1.3614398174285889\n",
      "Training Accuracy for epoch 178:  0.8655834564254062\n",
      "Validation Accuracy for epoch 178:  0.43842364532019706\n",
      "LOSS train 1.3614398174285889 valid 1.825723648071289\n",
      "EPOCH 180:\n",
      "  batch 1000 loss: 1.360767632007599\n",
      "Training Accuracy for epoch 179:  0.8655834564254062\n",
      "Validation Accuracy for epoch 179:  0.4396551724137931\n",
      "LOSS train 1.360767632007599 valid 1.8251765966415405\n",
      "EPOCH 181:\n",
      "  batch 1000 loss: 1.3600966373682022\n",
      "Training Accuracy for epoch 180:  0.8677991137370753\n",
      "Validation Accuracy for epoch 180:  0.4408866995073892\n",
      "LOSS train 1.3600966373682022 valid 1.8246253728866577\n",
      "EPOCH 182:\n",
      "  batch 1000 loss: 1.3594268312454223\n",
      "Training Accuracy for epoch 181:  0.8685376661742984\n",
      "Validation Accuracy for epoch 181:  0.4421182266009852\n",
      "LOSS train 1.3594268312454223 valid 1.8240727186203003\n",
      "EPOCH 183:\n",
      "  batch 1000 loss: 1.3587581832408906\n",
      "Training Accuracy for epoch 182:  0.8692762186115214\n",
      "Validation Accuracy for epoch 182:  0.4421182266009852\n",
      "LOSS train 1.3587581832408906 valid 1.8235180377960205\n",
      "EPOCH 184:\n",
      "  batch 1000 loss: 1.3580906864404678\n",
      "Training Accuracy for epoch 183:  0.8700147710487445\n",
      "Validation Accuracy for epoch 183:  0.4421182266009852\n",
      "LOSS train 1.3580906864404678 valid 1.8229619264602661\n",
      "EPOCH 185:\n",
      "  batch 1000 loss: 1.3574243681430818\n",
      "Training Accuracy for epoch 184:  0.8700147710487445\n",
      "Validation Accuracy for epoch 184:  0.4433497536945813\n",
      "LOSS train 1.3574243681430818 valid 1.8224095106124878\n",
      "EPOCH 186:\n",
      "  batch 1000 loss: 1.3567592706680298\n",
      "Training Accuracy for epoch 185:  0.8707533234859675\n",
      "Validation Accuracy for epoch 185:  0.4445812807881773\n",
      "LOSS train 1.3567592706680298 valid 1.8218556642532349\n",
      "EPOCH 187:\n",
      "  batch 1000 loss: 1.3560955229997635\n",
      "Training Accuracy for epoch 186:  0.8729689807976366\n",
      "Validation Accuracy for epoch 186:  0.4458128078817734\n",
      "LOSS train 1.3560955229997635 valid 1.8213034868240356\n",
      "EPOCH 188:\n",
      "  batch 1000 loss: 1.3554332860708236\n",
      "Training Accuracy for epoch 187:  0.8729689807976366\n",
      "Validation Accuracy for epoch 187:  0.4458128078817734\n",
      "LOSS train 1.3554332860708236 valid 1.8207566738128662\n",
      "EPOCH 189:\n",
      "  batch 1000 loss: 1.354772816181183\n",
      "Training Accuracy for epoch 188:  0.8729689807976366\n",
      "Validation Accuracy for epoch 188:  0.4458128078817734\n",
      "LOSS train 1.354772816181183 valid 1.8202120065689087\n",
      "EPOCH 190:\n",
      "  batch 1000 loss: 1.3541144417524338\n",
      "Training Accuracy for epoch 189:  0.8729689807976366\n",
      "Validation Accuracy for epoch 189:  0.44704433497536944\n",
      "LOSS train 1.3541144417524338 valid 1.8196710348129272\n",
      "EPOCH 191:\n",
      "  batch 1000 loss: 1.3534585584402083\n",
      "Training Accuracy for epoch 190:  0.8737075332348597\n",
      "Validation Accuracy for epoch 190:  0.4482758620689655\n",
      "LOSS train 1.3534585584402083 valid 1.8191345930099487\n",
      "EPOCH 192:\n",
      "  batch 1000 loss: 1.3528056353330613\n",
      "Training Accuracy for epoch 191:  0.8744460856720827\n",
      "Validation Accuracy for epoch 191:  0.44704433497536944\n",
      "LOSS train 1.3528056353330613 valid 1.8186030387878418\n",
      "EPOCH 193:\n",
      "  batch 1000 loss: 1.3521561385393144\n",
      "Training Accuracy for epoch 192:  0.8774002954209749\n",
      "Validation Accuracy for epoch 192:  0.4482758620689655\n",
      "LOSS train 1.3521561385393144 valid 1.8180761337280273\n",
      "EPOCH 194:\n",
      "  batch 1000 loss: 1.351510553598404\n",
      "Training Accuracy for epoch 193:  0.8788774002954209\n",
      "Validation Accuracy for epoch 193:  0.44950738916256155\n",
      "LOSS train 1.351510553598404 valid 1.8175551891326904\n",
      "EPOCH 195:\n",
      "  batch 1000 loss: 1.3508692988157271\n",
      "Training Accuracy for epoch 194:  0.879615952732644\n",
      "Validation Accuracy for epoch 194:  0.44950738916256155\n",
      "LOSS train 1.3508692988157271 valid 1.8170421123504639\n",
      "EPOCH 196:\n",
      "  batch 1000 loss: 1.3502327325344086\n",
      "Training Accuracy for epoch 195:  0.879615952732644\n",
      "Validation Accuracy for epoch 195:  0.44950738916256155\n",
      "LOSS train 1.3502327325344086 valid 1.8165326118469238\n",
      "EPOCH 197:\n",
      "  batch 1000 loss: 1.349601131439209\n",
      "Training Accuracy for epoch 196:  0.879615952732644\n",
      "Validation Accuracy for epoch 196:  0.45073891625615764\n",
      "LOSS train 1.349601131439209 valid 1.8160347938537598\n",
      "EPOCH 198:\n",
      "  batch 1000 loss: 1.3489746917486192\n",
      "Training Accuracy for epoch 197:  0.8788774002954209\n",
      "Validation Accuracy for epoch 197:  0.45073891625615764\n",
      "LOSS train 1.3489746917486192 valid 1.8155395984649658\n",
      "EPOCH 199:\n",
      "  batch 1000 loss: 1.3483534842729568\n",
      "Training Accuracy for epoch 198:  0.8788774002954209\n",
      "Validation Accuracy for epoch 198:  0.44950738916256155\n",
      "LOSS train 1.3483534842729568 valid 1.8150482177734375\n",
      "EPOCH 200:\n",
      "  batch 1000 loss: 1.3477375297546386\n",
      "Training Accuracy for epoch 199:  0.880354505169867\n",
      "Validation Accuracy for epoch 199:  0.44950738916256155\n",
      "LOSS train 1.3477375297546386 valid 1.8145668506622314\n"
     ]
    }
   ],
   "source": [
    "# define one training epoch \n",
    "def train_one_epoch(model, train_nodes, train_labels, epoch_index, tb_writer, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        epoch_index: index of the epoch \n",
    "        tb_writer: a writer object that can record all the training statistics \n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    true_pairs = 0\n",
    "    # Work thought the entire dataset\n",
    "    for i, (inputs, labels) in enumerate(zip(train_nodes, train_labels)):\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        # print(\"Output: \", outputs)\n",
    "        # print(\"Label: \", labels)\n",
    "        loss = loss_fn(outputs.unsqueeze(0), labels)\n",
    "        loss.backward()\n",
    "        if torch.argmax(outputs) == labels: \n",
    "            true_pairs += 1\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_nodes) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    print(f\"Training Accuracy for epoch {epoch_index}: \", true_pairs/len(train_nodes))\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/FCN_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model=model, \n",
    "        train_nodes=train_nodes, \n",
    "        train_labels=train_labels, \n",
    "        epoch_index=epoch_number, \n",
    "        tb_writer=writer, \n",
    "        loss_fn=loss, \n",
    "        optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    true_pairs = 0\n",
    "    for i, (vdata, vlabel) in enumerate(zip(validation_nodes, validation_labels)):\n",
    "        voutputs = model(vdata)\n",
    "        vloss = loss(voutputs.unsqueeze(0), vlabel)\n",
    "        running_vloss += vloss\n",
    "        if torch.argmax(voutputs) == vlabel: \n",
    "            true_pairs += 1\n",
    "\n",
    "    print(f\"Validation Accuracy for epoch {epoch}: \", true_pairs/len(validation_nodes))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'FCNs/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\959668787.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.46494464944649444\n",
      "LOSS test 1.8706679344177246 \n"
     ]
    }
   ],
   "source": [
    "# testing evaluation \n",
    "model.train(False) # turn training off for all parameters\n",
    "\n",
    "running_tloss = 0.0\n",
    "true_pairs = 0\n",
    "for i, (tdata, tlabel) in enumerate(zip(test_nodes, test_labels)):\n",
    "    toutputs = model(tdata)\n",
    "    tloss = loss(toutputs.unsqueeze(0), tlabel)\n",
    "    running_tloss += tloss\n",
    "    if torch.argmax(toutputs) == tlabel: \n",
    "        true_pairs += 1\n",
    "print(f\"Test Accuracy: \", true_pairs/len(test_nodes))\n",
    "\n",
    "\n",
    "avg_tloss = running_tloss / len(test_nodes)\n",
    "print('LOSS test {} '.format(avg_tloss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ee9abc2f8ed4c2bec59380a88532f6c4409e3142704504d9be0d180fda6aa0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
