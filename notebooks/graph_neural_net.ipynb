{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference website for overview of GCN: [LINK](http://tkipf.github.io/graph-convolutional-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: NumPy Example of GCN on Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Info:\n",
      " Graph named 'G' with 6 nodes and 7 edges\n",
      "\n",
      "Graph Nodes:  [(0, {'name': 0}), (1, {'name': 1}), (2, {'name': 2}), (3, {'name': 3}), (4, {'name': 4}), (5, {'name': 5})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_21353/3724959495.py:22: DeprecationWarning: info is deprecated and will be removed in version 3.0.\n",
      "\n",
      "  print('Graph Info:\\n', nx.info(G))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb4AAAEuCAYAAADx63eqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvmElEQVR4nO3deVhTd9428DthMYggVnHfRqwEVwSlVG3Frbi3aDKtldduLs9op2PVqhXo2ILW6rx2plMdX2trp7ZWm+Auat0QN6QiCCqLuIIKsqiAkJDlvH9YeaTghiQn5Nyf6+o1Njk5uRnn4p7vWX5HJgiCACIiIomQix2AiIjImlh8REQkKSw+IiKSFBYfERFJCouPiIgkhcVHRESSwuIjIiJJYfEREZGksPiIiEhSWHxERCQpLD4iIpIUFh8REUkKi4+IiCSFxUdERJLC4iMiIklh8RERkaSw+IiISFJYfEREJCksPiIikhQWHxERSQqLj4iIJIXFR0REkuIodgAiIjEVlOqhTcxBem4xinVGuCscoWzpDrV/WzRt1EDseGQBMkEQBLFDEBFZ2+ns21gRm4VDmfkAAL3RXPmewlEOAUCQtyemD+yMXu08xAlJFsHiIyLJ+TH+MhbFpENnNOFRvwFlMkDh6ICwkUqEBna0Wj6yLB7qJCJJuVd6aSg3mB+7rSAA5QYTFsWkAQDLz05w4iMiyTidfRtvfBOPcoOpyuu5P82HPvtMldecmrVH68krK//dxckBG6cGomdbD2tEJQvixEdEkrEiNgs6o+mh77v1GVv5Z4dGz1V5T2c0YWVsFlaF9rFYPrIOFh8RSUJBqR6HMvMfeU7vuaFTH/qeIAAHM/JRWKrn1Z71HIuPiCRBm5jz2G2yv3wdAODc0gseQW+jQasuVd6XAdCeysG0l70sEZGshDewE5EkpOcWV7ll4UFyZxe4ePVFQ5+X4ODuCd2VFNzc+AlMpbeqbKczmpF+o8QaccmCOPERkSQU64wPfc9T9QlkMhkAQDAZcO3/TYOp+CZ0V1Pg2nXgH/ZjsGhOsjxOfEQkCe6Kmv9/vtmgg6m0qOYPyar/inRXONVlLBIBJz4ikgRlS3c0cMytdrjTfPcOrn0zDYoOveDo7gn9tXSYim9C7uoBRYeeVbZVOMqhbOVmzdhkAZz4iEgSVP5ta3xd7uKGRt0Hw1h0DXfPHICp7DZcng9EizcWwaFh4yrb6vQVaHIrA7z9uX7jDexEJBlT153E3rS8R97S8DAyGdDVzYArP4XDxcUF4eHhGDt2bOW5Qao/OPERkWTMCOoMhaNDrT6rcHTA5/8nCCkpKfj444+xcOFC+Pr6QqPRwGx+/PJnZDs48RGRpDzNWp33uTjJETbSp8panYIgYOfOnYiMjERpaSnCwsLw+uuvw8GhdsVK1sPiIyLJ+TH+Mv6+JQUmyGq8cvO+J3k6gyAI2Lt3LyIjI5GXl4cFCxZg4sSJcHLi1Z+2ioc6iUhyXmhqQMmWzxD0fFM0cJRD4Vj1V6HCUY4GjnIEd22BjVMDH/lUBplMhldeeQVxcXFYvXo1fvjhB3h7e2P16tXQ6/UW/kmoNjjxEZHkjBs3DgEBAZg/fz4KS/XQnspB+o0SFOsMcFc4QdnKDSq/2j+B/ejRo4iMjMS5c+cwd+5cTJ48GQqFoo5/CqotFh8RSUpcXBwmTZqE9PR0i5dRQkICoqKicPLkScyZMwfTpk2Dq6urRb+THo+HOolIMsxmM2bNmoXFixdbZQILCAjAtm3bEBMTg2PHjsHLywtLlixBSQnX+xQTi4+IJGP9+vWQy+V44403rPq9vr6+0Gq12L9/P1JSUtCpUyd89tlnuH37tlVz0D0sPiKShPLycixYsADLly+HXC7Or75u3bph/fr1OHr0KC5evIjOnTsjIiIChYWFouSRKhYfEUnCl19+iYCAAAwYMEDsKOjSpQu+//57JCQkIC8vD126dMG8efNw8+ZNsaNJAouPiOxeXl4eli9fji+++ELsKFV06tQJq1evRlJSEu7evQulUokPP/wQ169fFzuaXWPxEZHd++STTzBp0iR4ednmk9Pbt2+Pr7/+GmfOnAEAdO/eHTNmzMDVq1dFTmafWHxEZNfOnDmDzZs3Izw8XOwoj9W6dWt8+eWXSE9PR6NGjdC7d29MmTIFFy9eFDuaXWHxEZFd++ijj7BgwQI899xzYkd5Ys2bN8cXX3yBzMxMtGzZEgEBAXjrrbeQkZEhdjS7wOIjIru1Z88eZGVlYfr06WJHqZWmTZsiMjISWVlZ6Ny5MwYMGIAJEybg7NmzYker11h8RGSXTCYT5syZg6VLl8LZ2VnsOM/Ew8MDERERuHjxInx9fTFkyBCoVCokJyeLHa1eYvERkV367rvv0KRJE7z22mtiR6kzbm5umDdvHi5cuID+/ftj5MiRGDt2LH777Texo9UrXKuTiOxOSUkJvL29sW3bNvTp00fsOBaj0+nw7bffYsmSJejWrRsiIiLQv39/sWPZPE58RGR3li5disGDB9t16QGAQqHAjBkzkJWVhXHjxiE0NBSDBw/GwYMHwZnm4TjxEZFdyc7Ohq+vL5KSktC+fXux41iVwWDATz/9hMWLF6N58+aIiIjAK6+8AplMJnY0m8LiIyK7MmnSJLRr1w6LFi0SO4pojEYjfvnlF0RFRcHNzQ3h4eEYPXo0C/B3LD4ishuJiYkYPXo0MjMz4ebmJnYc0ZnNZmzatAmRkZGQy+UIDw9HSEiIaIt02woWHxHZBUEQMGjQIEyYMAHTpk0TO45NMZvN2L59OyIjI6HT6RAeHg61Wg0HBwexo4mCxUdEdmHr1q1YsGABTp8+DUdHR7Hj2CRBELB7925ERkaiqKgICxYswJtvvim5/75YfERU71VUVKB79+746quvMHz4cLHj2DxBEHDgwAFERkYiOzsbH3/8MSZNmvRUN/oXlOqhTcxBem4xinVGuCscoWzpDrV/WzRt1MCC6Z8di4+I6r2vvvoKO3fuxJ49e8SOUu8cPnwYkZGRyMjIwLx58/Duu+9CoVA8dPvT2bexIjYLhzLzAQB6o7nyPYWjHAKAIG9PTB/YGb3aeVg4fe2w+IioXrt16xa8vb2xf/9+9OjRQ+w49VZ8fDyioqKQlJSEjz76CFOnTkXDhg2rbPNj/GUsikmHzmjCo5pDJgMUjg4IG6lEaGBHywavBWlf2kNE9d6iRYvw6quvsvSeUWBgIHbs2IHt27cjLi4OnTp1wtKlS1FaWgrgfumlodzw6NIDAEEAyg0mLIpJw4/xly0f/ilx4iOieuvixYsICAjAmTNn0LJlS7Hj2JXU1FRERUXh4MGDmDBjPvYYfaB74LDmfXfPHULBtmUAALc+Y/Hc0KlV3ndxcsDGqYHo2dbDGrGfCCc+Iqq35s+fj5kzZ7L0LKBHjx7YuHEjDh06hCO3XFFeYay2jbG4AEV7VgLyh98WoTOasDI2y5JRnxqLj4jqpaNHj+L48eOYNWuW2FHsmme7Tih1aw/ZH256FwQBhTuXw8GtKRp693vo5wUBOJiRj8JSvaWjPjEWHxHVO4IgYPbs2Vi0aFG1CzCobmkTc2p8veS3rdDlnEOzMXMgc3j0bRAyANpTNe9HDCw+Iqp3Nm7cCIPBgNDQULGj2L303OIqtywAQEX+Zdw69F94vBQK5xadHrsPndGM9Bsllor41KR1uz4R1Xs6nQ4ff/wx1q5dK/k1J62hWFf93F5ZxjHAZITuair02WdRcfMSAKD8/AnccnRGk6C3a9iPwdJRnxiLj4jqla+++gq9evVCUFCQ2FHsniAIqCi9U9MbAAToLiZWedl4Jw/6a+k17std4WSBhLXD2xmIqN7Iz8+Hj48Pjh07hi5duogdxy6ZzWacOHECGo0GWq0WTj1HAj1GwoSHX7lZsONL3D2zv8bbGYB7K7p8OKwLpr3sZcnoT4zHCYio3li4cCHefPNNll4dM5vNOHLkCGbOnIkOHTpg8uTJcHd3R0xMDOJ//iccHZ9tWhMAqPza1k3YOsCJj4jqhbS0NLz88stIS0tDs2bNxI5T75lMJhw9ehQajQbR0dFo2rQp1Go1VCoVunbtWmXbqetOYm9a3mNXbKmJTAYEd22BVaF96ij5s+M5PiKqF+bOnYv58+ez9J6ByWRCXFwctFotNm3ahBYtWkClUuHAgQNQKpUP/dyMoM44fL4A5QbTU3+nwtEB04M6P0vsOsfiIyKbt3//fpw7dw5arVbsKPWO0WjEoUOHKsuuTZs2UKvVOHTo0BMfMu7VzgNhI5W/r9VZfdmyh3FxkiNspNKmlisDWHxEZONMJhNmz56NJUuWoEED237Om60wGAyIjY2FRqPBli1b0KFDB6hUKhw7dgxeXrW7wOT+Uxbs4ekMLD4ismk//PADXF1doVKpxI5i0wwGAw4cOACNRoOtW7eiU6dOUKvVOHHiBP70pz/VyXeEBnZEz7YeWBmbhYMZ+ZABVRauvv88vkHenpge1NnmJr37eHELEdmsu3fvwtvbG1qtFoGBgWLHsTkVFRXYt28ftFottm7dii5dukCtVmP8+PHo0KGDRb+7sFQP7akcpN8oQbHOAHeFE5St3KDy4xPYiYhq7dNPP0V6ejp+/vlnsaPYDL1ej71790Kj0WD79u3o2rUrVCoVxo8fj3bt2okdr15g8RGRTbp+/Tp69OiBxMREdOzYUew4otLpdNizZw+0Wi127NiBHj16QK1WY9y4cWjTpo3Y8eodFh8R2aR3330Xnp6e+OKLL8SOIory8nLs3r0bGo0GMTEx6N27N1QqFcaNG4dWrVqJHa9eY/ERkc1JTk5GcHAwMjMz0bhxY7HjWE1ZWRl27doFjUaD3bt3w9/fH2q1GiEhIWjRooXY8ewGi4+IbIogCBg2bBhCQkIwY8YMseNY3N27d7Fz505otVrs2bMHAQEBUKvVeO2119C8eXOx49kl3s5ARDYlJiYG165dw9Sp1Rc7thclJSXYuXMnNBoN9u3bhxdffBEqlQorV67kyjRWwImPiGyGwWBAz549sWzZMowePVrsOHWquLgY27dvh1arxYEDB9C/f3+o1WqMHTsWTZs2FTuepHDiIyKb8c0336B169YYNWqU2FHqxJ07d7Bt2zZoNBrExsZi4MCBUKlU+O6779CkSROx40kWJz4isgl37tyBt7c3du/eDV9fX7Hj1NqtW7cqyy4uLg6DBg2CSqXC2LFjJXWhji1j8RGRTZg/fz7y8vKwdu1asaM8taKiImzZsgVarRZHjx7F4MGDoVarMXr0aLi7u4sdj/6AxUdEort8+TL8/f2RkpJSb27ILigowJYtW6DRaBAfH4+hQ4dCrVZj1KhRcHNzEzsePQKLj4hEN2HCBHh7e2PhwoViR3mkmzdvYvPmzdBqtUhISEBwcDBUKhVGjhyJRo0aiR2PnhCLj4hEdeLECYwbNw6ZmZlwdXUVO041eXl52LRpE7RaLRITEzF8+HCo1WqMGDECDRs2FDse1QKLj4hEIwgCBgwYgPfeew/vvvuu2HEq3bhxA5s2bYJGo0FycjJGjRoFlUqF4cOHw8XFRex49Ix4OwMRiSY6Ohp3797FW2+9JXYUXLt2DdHR0dBqtUhNTcXo0aMxa9YsvPLKK1AoFGLHozrEiY+IRKHX69G1a1esXr0aQ4YMESVDdnY2oqOjodFokJaWhrFjx0KlUmHYsGF82rsd48RHRKL4+uuv4ePjY/XSu3LlSmXZZWZm4tVXX0V4eDiGDBkCZ2dnq2YhcXDiIyKrKywshFKpRFxcHHx8fCz+fZcuXYJWq4VWq8WFCxfw2muvQa1WY/DgwXBycrL495NtYfERkdX97W9/g8FgwMqVKy32HRcuXIBWq4VGo8HVq1cREhIClUqFoKAglp3EsfiIyKoyMzPRr18/nDt3rs4fu3P+/PnKsrt27RrGjRsHlUqFgQMHwtGRZ3boHhYfEVlVSEgIAgMDMW/evDrZX0ZGBjQaDbRaLfLy8jBu3Dio1Wq89NJLcHBwqJPvIPvC4iOiOldQqoc2MQfpucUo1hnhrnCEsqU72lRk469T3kZ6evoz3SJw7ty5ysmuqKgI48ePh0qlQv/+/Vl29FgsPiKqM6ezb2NFbBYOZeYDAPRGc+V7Ckc5dHo9ujUBFocGoVc7jyferyAIOHv2bOVkd+fOHahUKqjVarz44ouQy+V1/aOQHWPxEVGd+DH+MhbFpENnNOFRv1VkMkDh6ICwkUqEBnZ86HaCICA1NbWy7MrKyqBSqaBSqfDCCy+w7KjWWHxE9MzulV4ayg33JryCHcuhu5wMU3kx5M4N4dyyM5oMfAvOLb0qP+PiJEfYSJ8q5ScIApKTkysPY1ZUVECtVkOlUiEgIAAymczaPxrZIRYfET2T09m38cY38Sg3mCpfy/1pPhzcmkLeoCF0V1JgLLoGB3dPtJ1e9Vl7Lk4O2DDlBRhvXqyc7ARBqDyM6e/vz7KjOsfiI6JnMnXdSexNy3vo4U19bhZyv58JyORoP2cTZA4P3FYgCEDOaTid+L5ysuvduzfLjiyKN7YQUa0VlOpxKDO/xtIrTtwOQ0E2dFdOAwDcA16rWnoAIJPBqYMvjv8nFc3cuBA0WQfPDhNRrWkTcx76Xln6UZQmxdw7zOnWDA3adK1xOwe5HNFJ1ywVkagaFh8R1Vp6bnGVWxYe1HLiErSfswme48JhKi1C/pbPYbydV207ndGM9Bsllo5KVInFR0S1VqwzVnvNbNBDMN+70EXm6AyXTv6QOSsAswnGO9WL795+DBbNSfQgnuMjolpzV1T/FVJxPQMF2/+BBu26Qa5oBH32WQj6MsgbNoZzC68a9gK4K7hoNFkPi4+Iak3Z0h0NHHOrHO50cGsKxyatobuUDHNFORwauqOhcgAa938DcoVrtX0oHOVQtnKzZmySON7OQES1VlCqR78vDqDiIef5nkQDRzmOzRuMpo34xHOyDp7jI6Jau3n1AoScVECoXfHJZMAgb0+WHlkVi4+InpogCFi1ahUGDhyIUL/mUDjX7qyJwtEB04M613E6okfjOT4ieioFBQWYPHkyrl69iiNHjsDb2xtef1ir80ncW6tTiZ5tPSwXlqgGnPiI6Int27cPvr6+eP7553H8+HF4e3sDAEIDOyJspA9cnBzwuNXGZLJ7a3T+cYFqImvhxS1E9FgVFRUICwvDzz//jO+//x5Dhw6tcbuUnNtYGZuFgxn5kOHezen3KRzlEHDvnN70oM6c9Eg0LD4ieqT09HS8+eabaN++PdasWYNmzZo99jOFpXpoT+Ug/UYJinUGuCucoGzlBpVfW17IQqJj8RFRjQRBwJo1a7BgwQJERkZi2rRpfGoC2QVe3EJE1RQWFmLKlCm4ePEi4uLi4OPjI3YkojrDi1uIqIoDBw7A19cXHTt2xIkTJ1h6ZHc48RERgHsXsHzyySdYt24d1q5di1deeUXsSEQWweIjImRmZuLNN99Eq1atkJycDE9PT7EjEVkMD3USSZggCPjuu+/Qv39/vPPOO9i2bRtLj+weJz4iibp16xamTp2KjIwMxMbGolu3bmJHIrIKTnxEEnTo0CH06tULbdq0QUJCAkuPJIUTH5GEGAwGLFy4EGvXrsW3336LESNGiB2JyOpYfEQSkZWVhYkTJ6Jp06ZISkpCixYtxI5EJAoe6iSyc4Ig4Pvvv8eLL76I0NBQ7Ny5k6VHksaJj8iO3b59G9OmTcPZs2dx4MAB9OjRQ+xIRKLjxEdkpw4fPgxfX180b94cv/32G0uP6Hec+IjsjMFgwGeffYY1a9ZgzZo1GDVqlNiRiGwKi4/Ijly8eBETJ05E48aNkZSUhJYtW4odicjm8FAnkR0QBAHr1q3DCy+8gNdffx0xMTEsPaKH4MRHVM/duXMHf/nLX3D69Gns27cPvXr1EjsSkU3jxEdUjx09ehS+vr7w8PDAb7/9xtIjegKc+IjqIaPRiKioKKxatQqrV6/G2LFjxY5EVG+w+IjqmcuXL2PixIlwdXVFUlISWrVqJXYkonqFhzqJ6pH169cjICAA48ePx+7du1l6RLXAiY+oHiguLsaMGTNw8uRJ7NmzB7179xY7ElG9xYmPyMYdP34cvr6+cHV1RWJiIkuP6Blx4iOyUSaTCYsXL8bXX3+NVatWISQkROxIRHaBxUdkg65cuYLQ0FA4Ozvj1KlTaNOmjdiRiOwGD3US2ZgNGzagb9++GDt2LPbu3cvSI6pjnPiIbERJSQn++te/4vjx49i1axf8/f3FjkRklzjxEdmAhIQE9O7dG05OTjh16hRLj8iCOPERichkMmHJkiX46quvsHLlSowfP17sSER2j8VHJJLs7GyEhoZCLpfj5MmTaNeundiRiCSBhzqJRKDRaODv748RI0Zg3759LD0iK+LER2RFpaWl+OCDD3D48GHs3LkTffv2FTsSkeRw4iOykpMnT8LPzw8AkJSUxNIjEgknPiILM5lMWLZsGZYvX46vv/4af/7zn8WORCRpLD4iC8rJycGkSZNgNBpx8uRJtG/fXuxIRJLHQ51EFrJp0yb4+/tjyJAhOHjwIEuPyEZw4iOqY3fv3sXMmTNx4MABbN26FYGBgWJHIqIHcOIjqkOnTp2Cn58fKioqkJSUxNIjskEsPqI6YDabsWzZMgwfPhyffvop/vvf/8Ld3V3sWERUAx7qJHpG169fx6RJk6DT6ZCQkICOHTuKHYmIHoETH9Ez2Lp1K/z8/PDyyy8jNjaWpUdUD3DiI6qFsrIyzJo1C7/++is2b96MF198UexIRPSEOPERPaXk5GT4+/ujtLQUSUlJLD2ieobFR/SEzGYzli9fjmHDhiE8PBw//vgjGjduLHYsInpKPNRJ9ARu3LiBt99+GyUlJThx4gQ6deokdiQiqiVOfESPsX37dvTu3RuBgYGIi4tj6RHVc5z4iB6ivLwcc+bMwc6dO6HVajFgwACxIxFRHeDER1SDlJQU9OnTB0VFRUhOTmbpEdkRFh/RAwRBwL/+9S8MGTIE8+bNw/r16+Hh4SF2LCKqQzzUSfS7vLw8vP3227h16xbi4+Ph5eUldiQisgBOfEQAYmJi4OvrC39/fxw+fJilR2THOPGRpOl0OsydOxdbt27Fxo0b8fLLL4sdiYgsjBMfSdaZM2fQt29f5OXlITk5maVHJBEsPpIcQRDw9ddfY9CgQZg1axY2bNiAJk2aiB2LiKyEhzpJUm7evIl3330XeXl5OHbsGJ5//nmxIxGRlXHiI8nYvXs3fH190bNnT5YekYRx4iO7p9Pp8PHHHyM6Ohrr169HUFCQ2JGISEQsPrJr586dw4QJE/D8888jOTkZzz33nNiRiEhkPNRJdkkQBPznP//BwIED8cEHH0Cj0bD0iAgAJz6yQ/n5+Zg8eTJycnJw5MgReHt7ix2JiGwIJz6yK3v37oWvry+USiWOHz/O0iOiajjxkV3Q6/UICwvDhg0b8MMPP2DIkCFiRyIiG8Xio3ovPT0dEyZMQMeOHXH69Gk0bdpU7EhEZMNYfGQTCkr10CbmID23GMU6I9wVjlC2dIfavy2aNmpQ42cEQcDq1asRHh6ORYsWYcqUKZDJZFZOTkT1jUwQBEHsECRdp7NvY0VsFg5l5gMA9EZz5XsKRzkEAEHenpg+sDN6tfOofK+wsBCTJ0/G5cuX8fPPP0OpVFo5ORHVV7y4hUTzY/xlvPFNPPam5UFvNFcpPQDQ/f7ar+fy8MY38fgx/jIAYP/+/fD19YWXlxfi4+NZekT0VDjxkSh+jL+MRTFpKDeYH7/x7xROcnTVpyPhp/+LtWvXYtiwYRZMSET2isVHVnc6+zbe+CYe5QZTldcFYwVuHfgOd9MPQ6goh3MLLzQZMhkNWv/vLQkyswHfh/bCwB5/snZsIrITPNRJVrciNgs6o6na60X7VqPk1A44uHrA5flA6K+lI29DOExld/53Iwcn/Hy60IppicjesPjIqgpK9TiUmY8/Hmcw3b2N0pR9gEyOFm8sguerc+HaLQhCRTlKEndUbicIwMGMfBSW6q2cnIjsBYuPrEqbmFPj64aCq4DZCAd3Tzi4egAAnFt2BgBU3LxUZVsZAO2pmvdDRPQ4LD6yqvTc4mpXbwKA6e4tAIDcWVH5muz3P99/7z6d0Yz0GyUWTElE9ozFR1ZVrDPW+LqDaxMAgLlCV/ma8Puf779XdT8GC6QjIilg8ZFVuStqXizIqVk7QO4IU3F+5YSnv5EJAHBuXv0KTneFk+VCEpFd45JlZDW3bt1CaU4mZGYFBHnV/+k5uDZBox5DUHp6D/J+DoOTZweUpR2BzNkFbv6jq2yrcJRD2crNmtGJyI5w4iOLKioqwnfffYcRI0agQ4cOyD2+BQ4ODjVu22ToVDTyGwXT3dsoy4xHgzbeaPH6Z3Bo2LjKdgIAlV9bK6QnInvEG9ipzhUUFGDLli3QaDSIj4/HsGHDoFKpMGrUKLi5uWHqupPYm5ZX7ZaGJyGTAcFdW2BVaJ+6D05EksBDnVQn8vPzsXnzZmg0GiQkJCA4OBjvvfceoqOj0ahRoyrbzgjqjMPnC6qt3PIkFI4OmB7Uua5iE5EEceKjWsvLy8OmTZug1WqRmJiI4cOHQ61WY/jw4XB1dX3kZ2uzVqeLkxxhI30QGtjxGZMTkZSx+Oip5ObmIjo6GlqtFklJSRg5cmRl2bm4uDzVvn6Mv4xPt52BwSwAsoefbpbJ7k16YSOVLD0iemYsPnqs69evV5ZdSkoKRo0aBbVajeDgYCgUisfv4CHKysrg/eIwBL77d6QUmCDDvZvT77v/PL5B3p6YHtQZPdt6PPPPQkTE4qMa5eTkIDo6GhqNBmfPnsWYMWOgVqvxyiuvoEGDmp+I/rSioqKQmpqKjRs3orBUD+2pHKTfKEGxzgB3hROUrdyg8nv4E9iJiGqDxUeVrl69Wll2GRkZGDt2LFQqFYYOHVpnZXffjRs30L17d5w8eRJ/+hMfMURE1sPik7jLly9Xll1WVhZeffVVqNVqDB48GM7Ozhb73ilTpsDDwwPLli2z2HcQEdWExSdBly5dglarhUajwaVLl/Daa69BrVZj0KBBcHKy/FJgKSkpGDZsGDIyMuDh4WHx7yMiehCLTyIuXLgAjUYDrVaLq1evIiQkBGq1GgMHDrRK2d0nCAKCg4MxduxYvP/++1b7XiKi+1h8duz8+fPQaDTQaDS4fv06xo0bB7VajZdffhmOjuKsXbBr1y58+OGHSE1NtWrhEhHdx+KzMxkZGZVld/PmTYwfPx4qlQovvfTSQ9fItBaj0YhevXphyZIlGDNmjKhZiEi6uGSZHTh37lzlObuioiKMHz8e//73v9G/f3/Ry+5Ba9asQYsWLTB69OjHb0xEZCGc+OohQRBw9uzZyrK7c+cOVCoVVCoV+vXrB7nc9h66UVxcDG9vb8TExKB3795ixyEiCWPx1ROCICA1NbWy7O7evQuVSgW1Wo0XXnjBJsvuQQsWLMCNGzewdu1asaMQkcSx+GyYIAg4ffp05dWYer2+suwCAgIgk8nEjvhErly5Aj8/P6SkpKBNmzZixyEiiWPx2RhBEJCUlFRZdiaTqbLs+vTpU2/K7kETJ07E888/j4ULF4odhYiIxWcLBEFAYmJiZdnJZDKo1WqoVCr4+fnVy7K7LyEhASEhIcjMzHzso4qIiKyBV3WKRBAEJCQkQKvVQqvVwsnJCWq1GlqtFr6+vvW67O4TBAGzZs1CZGQkS4+IbAaLz4rMZjNOnDhRWXYuLi5Qq9XYunUrevToYRdl96BNmzahtLQUb731lthRiIgq8VCnhZnNZhw/fryy7Nzc3KBWq6FWq9GtWze7K7v79Ho9unbtitWrV2PIkCFixyEiqsSJzwLMZjOOHj0KjUaD6OhoPPfcc1CpVNizZw+6du0qdjyrWLFiBXx8fFh6RGRzOPHVEZPJhCNHjkCj0WDTpk3w9PSsvBpTqVSKHc+qCgsLoVQqERcXBx8fH7HjEBFVweJ7BkajEYcPH64su1atWlVejdmlSxex44nmb3/7G4xGI1asWCF2FCKianio8ykZjUbExsZCq9Vi8+bNaNu2LdRqNY4cOYLOnTuLHU90mZmZ+Omnn5CWliZ2FCKiGrH4noDBYMDBgweh1WqxZcsWdOjQAWq1GsePH0enTp3EjmdT5s2bh7lz58LT01PsKERENWLxPYTBYMD+/fsry87LywtqtRoJCQno2LGj2PFs0qFDh5CcnIyff/5Z7ChERA9Vb87xFZTqoU3MQXpuMYp1RrgrHKFs6Q61f1s0bdSgTr6joqIC+/btg0ajwbZt2+Dt7Q2VSoXx48ejQ4cOdfId9spsNqNv37746KOP8MYbb4gdh4jooWy++E5n38aK2CwcyswHAOiN5sr3FI5yCACCvD0xfWBn9Grn8dT71+v12Lt3LzQaDbZv346uXbtCrVZj3LhxaNeuXR39FPZv3bp1WLlyJY4dO2a39yYSkX2w6eL7Mf4yFsWkQ2c04VEpZTJA4eiAsJFKhAZ2fOx+dTodfv31V2g0GuzYsQM9evSoLDs+PeDplZWVwdvbGxs3bkS/fv3EjkNE9Eg2W3z3Si8N5Qbz4zf+nYuTHGEjfWosv/LycuzevRtarRYxMTHo1asX1Go1QkJC0Lp16zpMLj1RUVFISUnBL7/8InYUIqLHssniO519G298E49yg6nyteLftqI0ZS8MBVcBwYzG/SfA46WJ1T7r4uSAjVMD0bOtB8rKyrBr1y5otVrs2rULfn5+lWXXsmVLa/5Idis3Nxfdu3dHQkICr3AlonrBJq/qXBGbBZ3RVOW1itwsyBWN4ODWDKbimw/9rM5gwoIfD6HhqZ+wZ88eBAQEQKVS4V//+heaN29u6eiSExERgXfeeYelR0T1hs0VX0GpHocy86ud02s2ZjYA4GZ0FMofUXwCgLO3gL8NDsaKFSt4P5kFpaamYtu2bcjIyBA7ChHRE5OLHeCPtIk5z7yPBs7OcOkaxNKzIEEQMHv2bERERMDDw0PsOERET8zmii89t7jKLQu1oTOakX6jpI4SUU12796Nq1evYtq0aWJHISJ6KjZXfMU6Yx3tx1An+6HqjEYj5syZg6VLl8LJyUnsOERET8Xmis9dUTenHd0V/IVsKd9++y1atGiBMWPGiB2FiOip2dzFLcqW7mjgmFvtcGfJ6T3QZ59DRd4FAEDZ+XgY79xEwy6BaNjlxSrbKhzlULZys1pmKSkuLsbChQsRExPDFVqIqF6yuYlP5d+2xtf12edw98x+mIrvLV1muHkJd8/sR0XexWrbmgQBKr+a90PPZsmSJQgODkbv3r3FjkJEVCs2eQP71HUnsTct75HLlD2cAOOlRAxAGsLDw9G9e/e6jidZV65cgZ+fH1JSUri0GxHVWzY38QHAjKDOUDg61OqzLk6O0H42BX5+fhg6dCjGjx+PpKSkOk4oTQsWLMD777/P0iOies0mi69XOw+EjVTCxenp4t1bq1OJwC6tMXfuXFy8eBEvvfQSRo8ejTFjxiAhIcFCie1fQkICYmNj8dFHH4kdhYjomdhk8QFAaGBHhI30gYuTAx53DYVMdm+Nzj8uUN2wYUPMnDkTFy5cwIgRI6BSqRAcHIwjR45YNrydEQQBs2bNQmRkJBo1aiR2HCKiZ2KT5/gelJJzGytjs3AwIx8y3Ls5/b77z+Mb5O2J6UGd0bOtxyP3VVFRgf/+97/4/PPP0aFDB0RERGDQoEG8OvExoqOj8dlnn+HUqVNwcKjdIWgiIlth88V3X2GpHtpTOUi/UYJinQHuCicoW7lB5ff0T2A3GAxYv349Fi9ejGbNmiEiIgLBwcEswBro9Xp069YNq1atwtChQ8WOQ0T0zOpN8VmCyWTCL7/8gqioKLi6uiI8PBxjxoxhAT5g+fLlOHDgAHbs2CF2FCKiOiHp4rvPbDZj8+bNiIyMBACEh4dj3LhxkMtt9hSoVRQWFkKpVCIuLg4+Pj5ixyEiqhMsvgcIgoAdO3YgMjISZWVlCAsLw5///GfJnteaOXMmDAYDVqxYIXYUIqI6w+KrgSAI+PXXXxEZGYn8/HwsWLAAb775pqQWZM7MzES/fv2QlpbGxzsRkV1h8T2CIAiIjY3FZ599hitXruDjjz/GW2+9BWdnZ7GjWVxISAgCAwMxb948saMQEdUpFt8TOnLkCCIjI5GWloZ58+bhvffeg0KhEDuWRRw6dAhvv/020tLS7PZnJCLpkvbVG09hwIAB2LNnDzQaDXbv3g0vLy/885//RFlZmdjR6pTZbMasWbPw+eefs/SIyC6x+J7SCy+8gO3bt2PHjh04fPgwOnXqhKVLl6KkxD6e+P7TTz/ByckJr7/+uthRiIgsgoc6n1FqaioWLVqEAwcO4IMPPsBf//pXNG7cWOxYtVJWVgalUokNGzagX79+YschIrIITnzPqEePHtiwYQPi4uKQmZkJLy8vfPLJJygqKhI72lNbvnw5AgMDWXpEZNc48dWxCxcu4PPPP8fmzZsxZcoUzJo1C82bNxc71mPl5uaie/fuSEhIQKdOncSOQ0RkMZz46piXlxfWrFmDU6dOobi4GEqlErNmzcKNGzfEjvZIn3zyCd555x2WHhHZPRafhXTo0AErV65EamoqzGYzunXrhvfffx/Z2dliR6smNTUVW7duRVhYmNhRiIgsjsVnYW3atME///lPpKWloWHDhvD19cW0adNw6dIlsaNVmjNnDsLDw+Hh4SF2FCIii2PxWUmLFi2wdOlSZGRkwNPTE3379sU777yD8+fPi5pr9+7duHz5Mv7nf/5H1BxERNbC4rOyZs2aISoqCufPn0fHjh3Rr18/TJw4EefOnbN6FqPRiNmzZ2PZsmWSWoeUiKSNxSeSJk2a4O9//zsuXLiA7t27Y9CgQVCr1Th9+rTVMnz77bdo3rw5xowZY7XvJCISG29nsBF3797FqlWr8I9//AMBAQGIiIhAnz59LPZ9xcXF8Pb2xs6dO+Hn52ex7yEisjWc+GyEq6srZs+ejYsXL2Lo0KEICQnBiBEjcOzYMYt835IlSxAcHMzSIyLJ4cRno/R6PdauXYslS5ZUrgYzcODAp9pHQake2sQcpOcWo1hnhLvCEcqW7ujXUoZB/foiJSUFbdq0sdBPQERkm1h8Ns5gMGDdunVYvHgxWrdujYiICAwdOhQymeyhnzmdfRsrYrNwKDMfAKA3mivfUzjKoTcY0E5+B//+yxj0audh6R+BiMimsPjqCaPRiA0bNmDRokVo3LgxIiIiMHLkyGoF+GP8ZSyKSYfOaMKj/mZlABRODggbqURoYEeLZicisiUsvnrGZDIhOjoaUVFRcHJyQnh4OF599VXI5fLfSy8N5Qbz43f0OxcnOcJG+rD8iEgyWHz1lNlsxrZt2xAZGQmDwYC3PvwE31xyhe6B0ivc9RX0OWkwFudD5uAE59Zd0GTQu3D27FBlXy5ODtg4NRA923pY+acgIrI+Fl89JwgCdu3ahQ83pUHftAsg/98Lda8sGQ3n1t5w9uyA8sunYbqTBwe3pmgz7RvIHJ0rt5PJgOCuLbAq1HK3TxAR2QpHsQPQs5HJZAh4eQhwXAYYqx7ibBG6DIq2PgAA4+08XFv1HkwlhagouIoGLTtXbicIwMGMfBSW6tG0UQOr5icisjbex2cHtIk5Nb5+v/QAQDAb7/1BJodDo+eqbSsDoD1V836IiOwJi88OpOcWV7ll4Y/MFeUo3PklAMA94DU41lB8OqMZ6TdKLJaRiMhW8FCnHSjWGR/6nqnsDm5qFqLixnk06hUMj6B3HrEfgyXiERHZFBafHXBX1PzXaLxzE3kbI2Asugb3QBWaBL39mP3wCQ1EZP9YfHZA2dIdDRxzqx3uzF03B6bSIji4e0IwVqBo32oAgGvXgWjQ2rvKtgpHOZSt3KyWmYhILDzHZwdU/m1rfN1UWnTvP4vzUXJyW+U/hoLsatsKAFR+Ne+HiMiecOKzA80aNcDALp7Ym5ZXZZmyDvN3PNHnZTJgkLcnb2UgIkngxGcnZgR1hsLRoVafVTg6YHpQ58dvSERkB1h8dqJXOw+EjVTCxenp/krvrdWp5HJlRCQZPNRpR+4vNP1ET2eQ3Zv0+HQGIpIartVph1JybmNlbBYOZuRDhns3p9+ncJRDwL1zetODOnPSIyLJYfHZscJSPbSncpB+owTFOgPcFU5QtnKDyq8tL2QhIsli8RERkaTw4hYiIpIUFh8REUkKi4+IiCSFxUdERJLC4iMiIklh8RERkaSw+IiISFJYfEREJCksPiIikhQWHxERSQqLj4iIJIXFR0REksLiIyIiSWHxERGRpLD4iIhIUlh8REQkKSw+IiKSFBYfERFJCouPiIgkhcVHRESSwuIjIiJJ+f8s6kndSvhGmQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.linalg import fractional_matrix_power\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "#Initialize the graph\n",
    "G = nx.Graph(name='G')\n",
    "\n",
    "#Create nodes\n",
    "#In this example, the graph will consist of 6 nodes.\n",
    "#Each node is assigned node feature which corresponds to the node name\n",
    "for i in range(6):\n",
    "    G.add_node(i, name=i)\n",
    "\n",
    "\n",
    "#Define the edges and the edges to the graph\n",
    "edges = [(0,1),(0,2),(1,2),(0,3),(3,4),(3,5),(4,5)]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "#See graph info\n",
    "print('Graph Info:\\n', nx.info(G))\n",
    "\n",
    "#Inspect the node features\n",
    "print('\\nGraph Nodes: ', G.nodes.data())\n",
    "\n",
    "#Plot the graph\n",
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A:  (6, 6)\n",
      "\n",
      "Shape of X:  (6, 1)\n",
      "\n",
      "Adjacency Matrix (A):\n",
      " [[0. 1. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 1. 1. 0.]]\n",
      "\n",
      "Node Features Matrix (X):\n",
      " [[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_21353/784659161.py:2: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_21353/784659161.py:3: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n"
     ]
    }
   ],
   "source": [
    "# Get the Adjacency Matrix (A) and Node Features Matrix (X) as numpy array\n",
    "A = np.array(nx.attr_matrix(G, node_attr='name')[0])\n",
    "X = np.array(nx.attr_matrix(G, node_attr='name')[1])\n",
    "X = np.expand_dims(X,axis=1)\n",
    "\n",
    "print('Shape of A: ', A.shape)\n",
    "print('\\nShape of X: ', X.shape)\n",
    "print('\\nAdjacency Matrix (A):\\n', A)\n",
    "print('\\nNode Features Matrix (X):\\n', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of A and X (AX):\n",
      " [[6.]\n",
      " [2.]\n",
      " [1.]\n",
      " [9.]\n",
      " [8.]\n",
      " [7.]]\n"
     ]
    }
   ],
   "source": [
    "# Dot product Adjacency Matrix (A) and Node Features (X)\n",
    "AX = np.dot(A,X)\n",
    "print(\"Dot product of A and X (AX):\\n\", AX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dot product of Adjacency Matrix and Node Features Matrix represents the sum of neighboring node features.\n",
    "#### But, if we think about it more, we will realize that while AX sums up the adjacent node features, it does not take into account the features of the node itself.\n",
    "* So we have to insert some self loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges of G with self-loops:\n",
      " [(0, 1), (0, 2), (0, 3), (0, 0), (1, 2), (1, 1), (2, 2), (3, 4), (3, 5), (3, 3), (4, 5), (4, 4), (5, 5)]\n",
      "Adjacency Matrix of added self-loops G (A_hat):\n",
      " [[1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n",
      "AX:\n",
      " [[ 6.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [12.]\n",
      " [12.]\n",
      " [12.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bd/3fcn_cld06z1y5f68_qm162m0000gn/T/ipykernel_21353/3005080699.py:14: FutureWarning: attr_matrix will return an numpy.ndarray instead of a numpy.matrix in NetworkX 3.0.\n",
      "  A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])\n"
     ]
    }
   ],
   "source": [
    "# Add Self Loops\n",
    "G_self_loops = G.copy()\n",
    "\n",
    "self_loops = []\n",
    "for i in range(G.number_of_nodes()):\n",
    "    self_loops.append((i,i))\n",
    "\n",
    "G_self_loops.add_edges_from(self_loops)\n",
    "\n",
    "# Check the edges of G_self_loops after adding the self loops\n",
    "print('Edges of G with self-loops:\\n', G_self_loops.edges)\n",
    "\n",
    "# Get the Adjacency Matrix (A) and Node Features Matrix (X) of added self-lopps graph\n",
    "A_hat = np.array(nx.attr_matrix(G_self_loops, node_attr='name')[0])\n",
    "print('Adjacency Matrix of added self-loops G (A_hat):\\n', A_hat)\n",
    "\n",
    "# Calculate the dot product of A_hat and X (AX)\n",
    "AX = np.dot(A_hat, X)\n",
    "print('AX:\\n', AX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree Matrix of added self-loops G (D):  [(0, 5), (1, 4), (2, 4), (3, 5), (4, 4), (5, 4)]\n",
      "Degree Matrix of added self-loops G as numpy array (D):\n",
      " [[5 0 0 0 0 0]\n",
      " [0 4 0 0 0 0]\n",
      " [0 0 4 0 0 0]\n",
      " [0 0 0 5 0 0]\n",
      " [0 0 0 0 4 0]\n",
      " [0 0 0 0 0 4]]\n",
      "Inverse of D:\n",
      " [[0.2  0.   0.   0.   0.   0.  ]\n",
      " [0.   0.25 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.25 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.2  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.25 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.25]]\n",
      "DAX:\n",
      " [[1.2 ]\n",
      " [0.75]\n",
      " [0.75]\n",
      " [2.4 ]\n",
      " [3.  ]\n",
      " [3.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data by finding the dot product with degree matrix\n",
    "#Get the Degree Matrix of the added self-loops graph\n",
    "Deg_Mat = G_self_loops.degree()\n",
    "print('Degree Matrix of added self-loops G (D): ', Deg_Mat)\n",
    "\n",
    "#Convert the Degree Matrix to a N x N matrix where N is the number of nodes\n",
    "D = np.diag([deg for (n,deg) in list(Deg_Mat)])\n",
    "print('Degree Matrix of added self-loops G as numpy array (D):\\n', D)\n",
    "\n",
    "#Find the inverse of Degree Matrix (D)\n",
    "D_inv = np.linalg.inv(D)\n",
    "print('Inverse of D:\\n', D_inv)\n",
    "\n",
    "#Dot product of D and AX for normalization\n",
    "DAX = np.dot(D_inv,AX)\n",
    "print('DAX:\\n', DAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization from the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DADX:\n",
      " [[1.27082039]\n",
      " [0.75      ]\n",
      " [0.75      ]\n",
      " [2.61246118]\n",
      " [2.92082039]\n",
      " [2.92082039]]\n"
     ]
    }
   ],
   "source": [
    "# Symmetrically-normalization\n",
    "D_half_norm = fractional_matrix_power(D, -0.5)\n",
    "DADX = D_half_norm.dot(A_hat).dot(D_half_norm).dot(X)\n",
    "print('DADX:\\n', DADX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Weights and Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n",
      "[[0.0033846  0.         0.0137304  0.00105133]\n",
      " [0.0020944  0.         0.0084964  0.00065057]\n",
      " [0.0020944  0.         0.0084964  0.00065057]\n",
      " [0.00701221 0.         0.0284466  0.00217814]\n",
      " [0.008097   0.         0.0328473  0.0025151 ]\n",
      " [0.008097   0.         0.0328473  0.0025151 ]]\n",
      "(6, 6)\n",
      "Features Representation from GCN output:\n",
      " [[0.00027758 0.        ]\n",
      " [0.00017298 0.        ]\n",
      " [0.00017298 0.        ]\n",
      " [0.00053017 0.        ]\n",
      " [0.00054097 0.        ]\n",
      " [0.00054097 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the weights\n",
    "np.random.seed(77777)\n",
    "n_h = 4 #number of neurons in the hidden layer\n",
    "n_y = 2 #number of neurons in the output layer\n",
    "W0 = np.random.randn(X.shape[1],n_h) * 0.01\n",
    "W1 = np.random.randn(n_h,n_y) * 0.01\n",
    "\n",
    "#Implement ReLu as activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "#Build GCN layer\n",
    "#In this function, we implement numpy to simplify\n",
    "def gcn(A,H,W):\n",
    "    I = np.identity(A.shape[0]) #create Identity Matrix of A\n",
    "    A_hat = A + I # add self-loop to A\n",
    "    D = np.diag(np.sum(A_hat, axis=0)) #create Degree Matrix of A\n",
    "    D_half_norm = fractional_matrix_power(D, -0.5) #calculate D to the power of -0.5\n",
    "    eq = D_half_norm.dot(A_hat).dot(D_half_norm).dot(H).dot(W)\n",
    "    return relu(eq)\n",
    "\n",
    "\n",
    "#Do forward propagation\n",
    "print(X.shape)\n",
    "H1 = gcn(A,X,W0) # Layer one \n",
    "print(H1)\n",
    "print(A.shape)\n",
    "H2 = gcn(A,H1,W1) # Layer two\n",
    "print('Features Representation from GCN output:\\n', H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_features(H2):\n",
    "    #Plot the features representation\n",
    "    x = H2[:,0]\n",
    "    y = H2[:,1]\n",
    "\n",
    "    size = 1000\n",
    "\n",
    "    plt.scatter(x,y,size)\n",
    "    plt.xlim([np.min(x)*0.9, np.max(x)*1.1])\n",
    "    plt.ylim([-1, 1])\n",
    "    plt.xlabel('Feature Representation Dimension 0')\n",
    "    plt.ylabel('Feature Representation Dimension 1')\n",
    "    plt.title('Feature Representation')\n",
    "\n",
    "    for i,row in enumerate(H2):\n",
    "        str = \"{}\".format(i)\n",
    "        plt.annotate(str, (row[0],row[1]),fontsize=18, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_features(H2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement a 1 Layer GCN to Classify Node Label on CORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.linalg import fractional_matrix_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5429, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1050679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1103960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   source\n",
       "0      35     1033\n",
       "1      35   103482\n",
       "2      35   103515\n",
       "3      35  1050679\n",
       "4      35  1103960"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify data source  \n",
    "core_fp_edges, core_fp_nodes = \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\"\n",
    "\n",
    "# use the edges to create a graph; store the content to each nodes \n",
    "edges_df = pd.read_csv(core_fp_edges, names=[\"target\", \"source\"], header=None, delimiter='\\t')\n",
    "print(edges_df.shape)\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networkx graph object with edgelist \n",
    "cora_g = nx.from_pandas_edgelist(edges_df)\n",
    "# nx.draw(cora_g, with_labels=True, font_weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_0</th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>word_3</th>\n",
       "      <th>word_4</th>\n",
       "      <th>word_5</th>\n",
       "      <th>word_6</th>\n",
       "      <th>word_7</th>\n",
       "      <th>word_8</th>\n",
       "      <th>word_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_1424</th>\n",
       "      <th>word_1425</th>\n",
       "      <th>word_1426</th>\n",
       "      <th>word_1427</th>\n",
       "      <th>word_1428</th>\n",
       "      <th>word_1429</th>\n",
       "      <th>word_1430</th>\n",
       "      <th>word_1431</th>\n",
       "      <th>word_1432</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106406</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37879</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_0  word_1  word_2  word_3  word_4  word_5  word_6  word_7  \\\n",
       "31336         0       0       0       0       0       0       0       0   \n",
       "1061127       0       0       0       0       0       0       0       0   \n",
       "1106406       0       0       0       0       0       0       0       0   \n",
       "13195         0       0       0       0       0       0       0       0   \n",
       "37879         0       0       0       0       0       0       0       0   \n",
       "\n",
       "         word_8  word_9  ...  word_1424  word_1425  word_1426  word_1427  \\\n",
       "31336         0       0  ...          0          0          1          0   \n",
       "1061127       0       0  ...          0          1          0          0   \n",
       "1106406       0       0  ...          0          0          0          0   \n",
       "13195         0       0  ...          0          0          0          0   \n",
       "37879         0       0  ...          0          0          0          0   \n",
       "\n",
       "         word_1428  word_1429  word_1430  word_1431  word_1432  \\\n",
       "31336            0          0          0          0          0   \n",
       "1061127          0          0          0          0          0   \n",
       "1106406          0          0          0          0          0   \n",
       "13195            0          0          0          0          0   \n",
       "37879            0          0          0          0          0   \n",
       "\n",
       "                        subject  \n",
       "31336           Neural_Networks  \n",
       "1061127           Rule_Learning  \n",
       "1106406  Reinforcement_Learning  \n",
       "13195    Reinforcement_Learning  \n",
       "37879     Probabilistic_Methods  \n",
       "\n",
       "[5 rows x 1434 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the features of each paper \n",
    "columns = [f\"word_{i}\" for i in range(1433)] + [\"subject\"]\n",
    "features_df = pd.read_csv(core_fp_nodes,names=columns, delimiter='\\t')\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genetic_Algorithms'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign features to each nodes\n",
    "nx.set_node_attributes(cora_g, features_df['subject'].to_dict(), \"subject\")\n",
    "nx.get_node_attributes(cora_g, \"subject\")[1033] # check node subject assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch alternative data source: torch-geometric\n",
    "\n",
    "- Install with: conda install pyg -c pyg\n",
    "- Load data: \n",
    "    ```{python} \n",
    "    from torch_geometric.datasets import Planetoid \n",
    "    dataset = Planetoid(root='~/somewhere/Cora', name='Cora')\n",
    "    ```\n",
    "- Visualize with: \n",
    "    ```{python}\n",
    "    import networkx as nx \n",
    "    from torch_geometric.utils import to_networkx \n",
    "    G = to_networkx(data, to_undirected=True) nx.draw(G)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a GCN layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using cuda:0. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"PyTorch is using {device}. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 nodes in the list  [1033, 35, 103482, 103515, 1050679]  out of  2708  number of nodes. \n"
     ]
    }
   ],
   "source": [
    "# create a nodelist to retrieve node number from index \n",
    "nodelist = list(cora_g.nodes) \n",
    "print(\"First 5 nodes in the list \", nodelist[:5], \" out of \", len(nodelist), \" number of nodes. \") # this is corresponding to the adjacency matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the adjacency matrix from established graph \n",
    "A = nx.to_numpy_matrix(cora_g, dtype=np.float32)  # unweighted adj matrix \n",
    "A = torch.from_numpy(A) # change it to a tensor object \n",
    "A = A.to(device)\n",
    "\n",
    "# encode the labesl labels \n",
    "categories = dict(zip(['Case_Based', \n",
    "                        'Genetic_Algorithms',\n",
    "                        'Neural_Networks',\n",
    "                        'Probabilistic_Methods',\n",
    "                        'Reinforcement_Learning',\n",
    "                        'Rule_Learning',\n",
    "                        'Theory'\n",
    "                    ], list(range(7))))\n",
    "labels = features_df.loc[nodelist]['subject'] # get the actual label \n",
    "labels = torch.tensor([categories[cat] for cat in labels]).unsqueeze(dim=1)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# ohe = OneHotEncoder()\n",
    "# ohe.fit(labels.reset_index()) \n",
    "# print(\"Onehot encoded categories: \\n\", ohe.categories_[0], \"\\n\", ohe.categories_[1])\n",
    "# onehot_labels = ohe.transform(labels.reset_index()).todense()\n",
    "# onehot_labels = [torch.from_numpy(lst) for lst in onehot_labels]\n",
    "\n",
    "# creates the training and testing dataset \n",
    "# train_size = int(A.shape[0] * 0.5)\n",
    "# validation_size = int(A.shape[0] * 0.8) - train_size\n",
    "# train_nodes, train_labels = A[:train_size], onehot_labels[:train_size]\n",
    "# validation_nodes, validation_labels = A[train_size:validation_size+train_size], onehot_labels[train_size:validation_size+train_size]\n",
    "# test_nodes, test_labels = A[validation_size+train_size:], onehot_labels[validation_size+train_size:]\n",
    "\n",
    "# creates the training and testing dataset (without ohe)\n",
    "train_size = int(A.shape[0] * 0.5)\n",
    "validation_size = int(A.shape[0] * 0.8) - train_size\n",
    "train_nodes, train_labels = A[:train_size], labels[:train_size]\n",
    "validation_nodes, validation_labels = A[train_size:validation_size+train_size], labels[train_size:validation_size+train_size]\n",
    "test_nodes, test_labels = A[validation_size+train_size:], labels[validation_size+train_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1354\n",
      "True 812\n",
      "True 542\n"
     ]
    }
   ],
   "source": [
    "# check training and testing datasets\n",
    "print(len(train_nodes) == len(train_labels), len(train_labels))\n",
    "print(len(validation_nodes) == len(validation_labels), len(validation_nodes))\n",
    "print(len(test_nodes) == len(test_labels), len(test_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here are some legacy code for dataloading in pytorch, might be useful later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data in a custom dataset \n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class NodeVLabelDataset(Dataset):\n",
    "#     def __init__(self, cora_cites_fp, cora_content_fp):\n",
    "#         # setup the graph \n",
    "#         self.cora_cites_fp, self.cora_content_fp = cora_cites_fp, cora_content_fp\n",
    "#         self.edges_df = pd.read_csv(self.cora_cites_fp, names=[\"target\", \"source\"], header=None, delimiter='\\t')\n",
    "#         self.graph = nx.from_pandas_edgelist(self.edges_df)       \n",
    "\n",
    "#         # create the adjacency matrix \n",
    "#         self.A = nx.to_numpy_matrix(self.graph, dtype=np.float32)  # unweighted adj matrix \n",
    "#         self.A = torch.from_numpy(A) # change it to a tensor object  \n",
    "#         self.nodelist = list(self.graph.nodes)  # a list of nodes \n",
    "\n",
    "#         # get the label subjects \n",
    "#         columns = [f\"word_{i}\" for i in range(1433)] + [\"subject\"]\n",
    "#         self.features_df = pd.read_csv(self.cora_content_fp, names=columns, delimiter='\\t')\n",
    "#         self.labels = features_df.loc[nodelist]['subject']  # list of labels\n",
    "        \n",
    "#         # encode the labesl labels \n",
    "#         self.categories = dict(zip(['Case_Based', \n",
    "#                                 'Genetic_Algorithms',\n",
    "#                                 'Neural_Networks',\n",
    "#                                 'Probabilistic_Methods',\n",
    "#                                 'Reinforcement_Learning',\n",
    "#                                 'Rule_Learning',\n",
    "#                                 'Theory'\n",
    "#                             ], list(range(7))))\n",
    "#         self.labels = self.labels.apply(lambda x: self.categories[x])\n",
    "#         self.labels = nn.functional.one_hot(self.labels, num_classes=len(self.categories.items()))\n",
    "        \n",
    "\n",
    "#     def draw(self, with_labels=True, font_weight='bold'): \n",
    "#         \"\"\"Create a visualization of the graph\"\"\"\n",
    "#         nx.draw(self.graph, with_labels=with_labels, font_weight=font_weight)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.A)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.A[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the training and testing data loader\n",
    "# train_dataset = NodeVLabelDataset( \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\")\n",
    "# test_dataset = NodeVLabelDataset( \"../data/raw/cora/cora/cora.cites\", \"../data/raw/cora/cora/cora.content\")\n",
    "\n",
    "# training_loader = torch.utils.data.DataLoader(, batch_size=4, shuffle=True, num_workers=2)\n",
    "# testing_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New GCN layer definition\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
    "    def __init__(self, A, input_dim, output_dim, device):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        print(\"layer initialized\")\n",
    "        self.device = device # initialize the hosting device\n",
    "\n",
    "        self.A = A.to(self.device) # the adjacency matrix \n",
    "        self.I = torch.eye(self.A.shape[0], device=self.device) # create identity matrix with the same shape as A \n",
    "        self.A =  self.A + self.I   # add self loop to A \n",
    "        # print(\"Adj Matrix with self loop: \", self.A)\n",
    "        self.D = torch.diag(torch.sum(self.A, dim=0)).cpu()  # calculate the degree matrix with A after added self loop\n",
    "        # print(\"Degree Matrix: \", self.D)\n",
    "        # for diagonal matrix, raising it to any power is the same as raising its diagonal elements to that power\n",
    "        # we can just apply the -1/2 power to all element of this degree matrix \n",
    "        # self.D_half_norm = torch.reciprocal(torch.sqrt(self.D)) \n",
    "        self.D_half_norm = torch.from_numpy(fractional_matrix_power(self.D, -0.5)).to(self.device)\n",
    "        # print(\"Normalization Matrix: \", self.D_half_norm)\n",
    "        self.A_s = torch.mm(torch.mm(self.D_half_norm, self.A), self.D_half_norm).to(self.device) # normalized adjacency matrix\n",
    "\n",
    "        # initialize the weight matrix for this layer \n",
    "        # the weight should have shape of (N , F) where N is the size of the input, and F is the output dimension\n",
    "        self.W = torch.nn.Parameter(\n",
    "            data=(torch.rand(input_dim, output_dim, device=self.device) * 0.01),  # times it by 0.001 to make the weight smaller\n",
    "            requires_grad=True, # weight should be trainable \n",
    "        )\n",
    "        # create trainable a bias term for the layer\n",
    "        self.b = torch.nn.Parameter(\n",
    "            data=(torch.rand(output_dim, 1, device=self.device) * 0.01),\n",
    "            requires_grad=True, # bias should be trainable \n",
    "        )\n",
    "\n",
    "        # print(self.W.get_device(), self.b.get_device())\n",
    "\n",
    "    def forward(self, H):\n",
    "        # print(H.get_device())\n",
    "        return torch.mm(torch.mm(self.A_s, H.unsqueeze(dim=1)).T, self.W).T + self.b\n",
    "\n",
    "# create a 1 layer classification model with softmax output\n",
    "class oneLayerGCN(nn.Module): \n",
    "    def __init__(self, adjacency_matrix, output_size, device) -> None:\n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            adjacency_matrix: tensor, a tensor representing the link connection between nodes\n",
    "            output_size: input, the number of target label we have for our prediction\n",
    "        \"\"\"\n",
    "        super(oneLayerGCN, self).__init__()\n",
    "        self.device=device\n",
    "        self.A = adjacency_matrix\n",
    "        self.output_size = output_size\n",
    "        self.hidden_conv_layer = GCNLayer(A=self.A, input_dim=self.A.shape[0], output_dim=self.output_size, device=self.device)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, X): \n",
    "        \"\"\"\n",
    "        Parameters: \n",
    "            X: tensor, the link feature of the nodes in the dataset\n",
    "        \"\"\"\n",
    "        output = self.hidden_conv_layer(X).T # get the hidden embedding\n",
    "        # print(\"Hidden Layer Output:\", output)\n",
    "        output = self.softmax(output)\n",
    "        # print(\"Softmax Output: \", output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer initialized\n"
     ]
    }
   ],
   "source": [
    "# Set up the training parameter \n",
    "model = oneLayerGCN(adjacency_matrix=A, output_size=len(categories.items()), device=device)\n",
    "# print(\"\\nParameters in the one layer GCN: \\n\")\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "model.to(device)\n",
    "\n",
    "# create cross entropy loss calculation, optimization \n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\3425545340.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.834550076842308\n",
      "Training Accuracy for epoch 0:  0.30280649926144754\n",
      "Validation Accuracy for epoch 0:  0.2894088669950739\n",
      "LOSS train 1.834550076842308 valid 1.8806463479995728\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.8220288152694701\n",
      "Training Accuracy for epoch 1:  0.3293943870014771\n",
      "Validation Accuracy for epoch 1:  0.2894088669950739\n",
      "LOSS train 1.8220288152694701 valid 1.8783221244812012\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.8091794267892838\n",
      "Training Accuracy for epoch 2:  0.362629246676514\n",
      "Validation Accuracy for epoch 2:  0.29064039408866993\n",
      "LOSS train 1.8091794267892838 valid 1.876274585723877\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.7966832603216172\n",
      "Training Accuracy for epoch 3:  0.378877400295421\n",
      "Validation Accuracy for epoch 3:  0.29064039408866993\n",
      "LOSS train 1.7966832603216172 valid 1.8743805885314941\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.7851072815656661\n",
      "Training Accuracy for epoch 4:  0.38847858197932056\n",
      "Validation Accuracy for epoch 4:  0.291871921182266\n",
      "LOSS train 1.7851072815656661 valid 1.8726228475570679\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 1.774629139661789\n",
      "Training Accuracy for epoch 5:  0.39217134416543575\n",
      "Validation Accuracy for epoch 5:  0.291871921182266\n",
      "LOSS train 1.774629139661789 valid 1.8710416555404663\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 1.7651401282548904\n",
      "Training Accuracy for epoch 6:  0.397341211225997\n",
      "Validation Accuracy for epoch 6:  0.29679802955665024\n",
      "LOSS train 1.7651401282548904 valid 1.8696693181991577\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 1.7564245429039\n",
      "Training Accuracy for epoch 7:  0.40029542097488924\n",
      "Validation Accuracy for epoch 7:  0.29926108374384236\n",
      "LOSS train 1.7564245429039 valid 1.868520736694336\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 1.748265167593956\n",
      "Training Accuracy for epoch 8:  0.4069423929098966\n",
      "Validation Accuracy for epoch 8:  0.29926108374384236\n",
      "LOSS train 1.748265167593956 valid 1.8676038980484009\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 1.7404845160245896\n",
      "Training Accuracy for epoch 9:  0.41654357459379615\n",
      "Validation Accuracy for epoch 9:  0.30295566502463056\n",
      "LOSS train 1.7404845160245896 valid 1.8669028282165527\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 1.7329575799703598\n",
      "Training Accuracy for epoch 10:  0.4202363367799114\n",
      "Validation Accuracy for epoch 10:  0.30295566502463056\n",
      "LOSS train 1.7329575799703598 valid 1.866399884223938\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 1.7256134464740753\n",
      "Training Accuracy for epoch 11:  0.43057607090103395\n",
      "Validation Accuracy for epoch 11:  0.3066502463054187\n",
      "LOSS train 1.7256134464740753 valid 1.8660595417022705\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 1.7184292159080505\n",
      "Training Accuracy for epoch 12:  0.44091580502215655\n",
      "Validation Accuracy for epoch 12:  0.3078817733990148\n",
      "LOSS train 1.7184292159080505 valid 1.8658363819122314\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 1.711418039560318\n",
      "Training Accuracy for epoch 13:  0.448301329394387\n",
      "Validation Accuracy for epoch 13:  0.3091133004926108\n",
      "LOSS train 1.711418039560318 valid 1.8656837940216064\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 1.7046120647192002\n",
      "Training Accuracy for epoch 14:  0.4601181683899557\n",
      "Validation Accuracy for epoch 14:  0.312807881773399\n",
      "LOSS train 1.7046120647192002 valid 1.8655517101287842\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 1.6980464614629744\n",
      "Training Accuracy for epoch 15:  0.46971935007385524\n",
      "Validation Accuracy for epoch 15:  0.312807881773399\n",
      "LOSS train 1.6980464614629744 valid 1.8654024600982666\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 1.6917485412359237\n",
      "Training Accuracy for epoch 16:  0.4815361890694239\n",
      "Validation Accuracy for epoch 16:  0.31527093596059114\n",
      "LOSS train 1.6917485412359237 valid 1.865201473236084\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 1.6857333180904388\n",
      "Training Accuracy for epoch 17:  0.4874446085672083\n",
      "Validation Accuracy for epoch 17:  0.31527093596059114\n",
      "LOSS train 1.6857333180904388 valid 1.8649379014968872\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 1.6800042392015457\n",
      "Training Accuracy for epoch 18:  0.4948301329394387\n",
      "Validation Accuracy for epoch 18:  0.31773399014778325\n",
      "LOSS train 1.6800042392015457 valid 1.8646012544631958\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 1.6745561453104019\n",
      "Training Accuracy for epoch 19:  0.5051698670605613\n",
      "Validation Accuracy for epoch 19:  0.32019704433497537\n",
      "LOSS train 1.6745561453104019 valid 1.8641917705535889\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 1.6693784694671632\n",
      "Training Accuracy for epoch 20:  0.5096011816838996\n",
      "Validation Accuracy for epoch 20:  0.3226600985221675\n",
      "LOSS train 1.6693784694671632 valid 1.8637217283248901\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 1.6644578223228454\n",
      "Training Accuracy for epoch 21:  0.516248153618907\n",
      "Validation Accuracy for epoch 21:  0.3226600985221675\n",
      "LOSS train 1.6644578223228454 valid 1.8631986379623413\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 1.6597796137332916\n",
      "Training Accuracy for epoch 22:  0.5221565731166913\n",
      "Validation Accuracy for epoch 22:  0.3226600985221675\n",
      "LOSS train 1.6597796137332916 valid 1.8626346588134766\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 1.655329027056694\n",
      "Training Accuracy for epoch 23:  0.5295420974889217\n",
      "Validation Accuracy for epoch 23:  0.32389162561576357\n",
      "LOSS train 1.655329027056694 valid 1.8620367050170898\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 1.651091605424881\n",
      "Training Accuracy for epoch 24:  0.5361890694239291\n",
      "Validation Accuracy for epoch 24:  0.3251231527093596\n",
      "LOSS train 1.651091605424881 valid 1.8614110946655273\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 1.6470534243583679\n",
      "Training Accuracy for epoch 25:  0.5443131462333826\n",
      "Validation Accuracy for epoch 25:  0.3288177339901478\n",
      "LOSS train 1.6470534243583679 valid 1.8607690334320068\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 1.6432013449668885\n",
      "Training Accuracy for epoch 26:  0.5494830132939439\n",
      "Validation Accuracy for epoch 26:  0.33004926108374383\n",
      "LOSS train 1.6432013449668885 valid 1.8601150512695312\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 1.63952288544178\n",
      "Training Accuracy for epoch 27:  0.5561299852289513\n",
      "Validation Accuracy for epoch 27:  0.33251231527093594\n",
      "LOSS train 1.63952288544178 valid 1.8594534397125244\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 1.6360063799619675\n",
      "Training Accuracy for epoch 28:  0.5635155096011817\n",
      "Validation Accuracy for epoch 28:  0.33866995073891626\n",
      "LOSS train 1.6360063799619675 valid 1.8587874174118042\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 1.6326408610343932\n",
      "Training Accuracy for epoch 29:  0.5649926144756278\n",
      "Validation Accuracy for epoch 29:  0.3411330049261084\n",
      "LOSS train 1.6326408610343932 valid 1.8581215143203735\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 1.6294160014390946\n",
      "Training Accuracy for epoch 30:  0.569423929098966\n",
      "Validation Accuracy for epoch 30:  0.34236453201970446\n",
      "LOSS train 1.6294160014390946 valid 1.8574572801589966\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 1.626322172641754\n",
      "Training Accuracy for epoch 31:  0.5709010339734121\n",
      "Validation Accuracy for epoch 31:  0.34236453201970446\n",
      "LOSS train 1.626322172641754 valid 1.8567942380905151\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 1.6233502465486527\n",
      "Training Accuracy for epoch 32:  0.5738552437223042\n",
      "Validation Accuracy for epoch 32:  0.3485221674876847\n",
      "LOSS train 1.6233502465486527 valid 1.8561358451843262\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 1.6204916458129883\n",
      "Training Accuracy for epoch 33:  0.5782865583456426\n",
      "Validation Accuracy for epoch 33:  0.3522167487684729\n",
      "LOSS train 1.6204916458129883 valid 1.855483055114746\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 1.6177382271289826\n",
      "Training Accuracy for epoch 34:  0.5790251107828656\n",
      "Validation Accuracy for epoch 34:  0.35467980295566504\n",
      "LOSS train 1.6177382271289826 valid 1.8548378944396973\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 1.6150822776556015\n",
      "Training Accuracy for epoch 35:  0.5812407680945347\n",
      "Validation Accuracy for epoch 35:  0.35467980295566504\n",
      "LOSS train 1.6150822776556015 valid 1.8541978597640991\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 1.6125164457559586\n",
      "Training Accuracy for epoch 36:  0.585672082717873\n",
      "Validation Accuracy for epoch 36:  0.35714285714285715\n",
      "LOSS train 1.6125164457559586 valid 1.8535645008087158\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 1.6100335731506348\n",
      "Training Accuracy for epoch 37:  0.5841949778434269\n",
      "Validation Accuracy for epoch 37:  0.35960591133004927\n",
      "LOSS train 1.6100335731506348 valid 1.8529399633407593\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 1.6076268184185027\n",
      "Training Accuracy for epoch 38:  0.5864106351550961\n",
      "Validation Accuracy for epoch 38:  0.3608374384236453\n",
      "LOSS train 1.6076268184185027 valid 1.8523228168487549\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 1.6052894541025162\n",
      "Training Accuracy for epoch 39:  0.5878877400295421\n",
      "Validation Accuracy for epoch 39:  0.3620689655172414\n",
      "LOSS train 1.6052894541025162 valid 1.8517132997512817\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 1.603014829158783\n",
      "Training Accuracy for epoch 40:  0.5901033973412112\n",
      "Validation Accuracy for epoch 40:  0.3633004926108374\n",
      "LOSS train 1.603014829158783 valid 1.8511110544204712\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 1.600796395778656\n",
      "Training Accuracy for epoch 41:  0.5937961595273265\n",
      "Validation Accuracy for epoch 41:  0.3633004926108374\n",
      "LOSS train 1.600796395778656 valid 1.8505167961120605\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 1.5986274590492249\n",
      "Training Accuracy for epoch 42:  0.5974889217134417\n",
      "Validation Accuracy for epoch 42:  0.3620689655172414\n",
      "LOSS train 1.5986274590492249 valid 1.849928855895996\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 1.5965012699365615\n",
      "Training Accuracy for epoch 43:  0.5982274741506647\n",
      "Validation Accuracy for epoch 43:  0.3633004926108374\n",
      "LOSS train 1.5965012699365615 valid 1.849351167678833\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 1.594410904288292\n",
      "Training Accuracy for epoch 44:  0.5982274741506647\n",
      "Validation Accuracy for epoch 44:  0.3657635467980296\n",
      "LOSS train 1.594410904288292 valid 1.8487801551818848\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 1.5923491249084472\n",
      "Training Accuracy for epoch 45:  0.5997045790251108\n",
      "Validation Accuracy for epoch 45:  0.3669950738916256\n",
      "LOSS train 1.5923491249084472 valid 1.8482167720794678\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 1.5903083295822145\n",
      "Training Accuracy for epoch 46:  0.6056129985228951\n",
      "Validation Accuracy for epoch 46:  0.3669950738916256\n",
      "LOSS train 1.5903083295822145 valid 1.8476613759994507\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 1.588280576944351\n",
      "Training Accuracy for epoch 47:  0.6070901033973413\n",
      "Validation Accuracy for epoch 47:  0.3694581280788177\n",
      "LOSS train 1.588280576944351 valid 1.847113013267517\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 1.5862574125528335\n",
      "Training Accuracy for epoch 48:  0.6093057607090103\n",
      "Validation Accuracy for epoch 48:  0.3694581280788177\n",
      "LOSS train 1.5862574125528335 valid 1.8465726375579834\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 1.5842299419641495\n",
      "Training Accuracy for epoch 49:  0.6115214180206795\n",
      "Validation Accuracy for epoch 49:  0.3694581280788177\n",
      "LOSS train 1.5842299419641495 valid 1.846037745475769\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 1.5821889293193818\n",
      "Training Accuracy for epoch 50:  0.6137370753323486\n",
      "Validation Accuracy for epoch 50:  0.3706896551724138\n",
      "LOSS train 1.5821889293193818 valid 1.8455113172531128\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 1.5801248599290847\n",
      "Training Accuracy for epoch 51:  0.6152141802067946\n",
      "Validation Accuracy for epoch 51:  0.3706896551724138\n",
      "LOSS train 1.5801248599290847 valid 1.8449921607971191\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 1.5780282998085022\n",
      "Training Accuracy for epoch 52:  0.6174298375184638\n",
      "Validation Accuracy for epoch 52:  0.3706896551724138\n",
      "LOSS train 1.5780282998085022 valid 1.844481348991394\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 1.575890258669853\n",
      "Training Accuracy for epoch 53:  0.6211225997045791\n",
      "Validation Accuracy for epoch 53:  0.3706896551724138\n",
      "LOSS train 1.575890258669853 valid 1.8439780473709106\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 1.5737029695510865\n",
      "Training Accuracy for epoch 54:  0.6233382570162481\n",
      "Validation Accuracy for epoch 54:  0.3694581280788177\n",
      "LOSS train 1.5737029695510865 valid 1.8434827327728271\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 1.5714605544805527\n",
      "Training Accuracy for epoch 55:  0.6248153618906942\n",
      "Validation Accuracy for epoch 55:  0.3706896551724138\n",
      "LOSS train 1.5714605544805527 valid 1.842994213104248\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 1.569160034775734\n",
      "Training Accuracy for epoch 56:  0.6270310192023634\n",
      "Validation Accuracy for epoch 56:  0.3706896551724138\n",
      "LOSS train 1.569160034775734 valid 1.8425151109695435\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 1.5668022091388702\n",
      "Training Accuracy for epoch 57:  0.6307237813884786\n",
      "Validation Accuracy for epoch 57:  0.3706896551724138\n",
      "LOSS train 1.5668022091388702 valid 1.8420429229736328\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 1.5643923100233077\n",
      "Training Accuracy for epoch 58:  0.6336779911373708\n",
      "Validation Accuracy for epoch 58:  0.3706896551724138\n",
      "LOSS train 1.5643923100233077 valid 1.8415802717208862\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 1.561940225839615\n",
      "Training Accuracy for epoch 59:  0.6388478581979321\n",
      "Validation Accuracy for epoch 59:  0.3706896551724138\n",
      "LOSS train 1.561940225839615 valid 1.841124415397644\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 1.559460222363472\n",
      "Training Accuracy for epoch 60:  0.6440177252584933\n",
      "Validation Accuracy for epoch 60:  0.3706896551724138\n",
      "LOSS train 1.559460222363472 valid 1.840674877166748\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 1.5569695891141893\n",
      "Training Accuracy for epoch 61:  0.6484490398818316\n",
      "Validation Accuracy for epoch 61:  0.3706896551724138\n",
      "LOSS train 1.5569695891141893 valid 1.8402308225631714\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 1.554487154364586\n",
      "Training Accuracy for epoch 62:  0.6499261447562777\n",
      "Validation Accuracy for epoch 62:  0.3706896551724138\n",
      "LOSS train 1.554487154364586 valid 1.8397914171218872\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 1.552031137228012\n",
      "Training Accuracy for epoch 63:  0.6528803545051699\n",
      "Validation Accuracy for epoch 63:  0.37192118226600984\n",
      "LOSS train 1.552031137228012 valid 1.8393492698669434\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 1.5496175299882888\n",
      "Training Accuracy for epoch 64:  0.6587887740029542\n",
      "Validation Accuracy for epoch 64:  0.37192118226600984\n",
      "LOSS train 1.5496175299882888 valid 1.8389006853103638\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 1.5472586660385133\n",
      "Training Accuracy for epoch 65:  0.6661742983751846\n",
      "Validation Accuracy for epoch 65:  0.3731527093596059\n",
      "LOSS train 1.5472586660385133 valid 1.838447093963623\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 1.5449626845121385\n",
      "Training Accuracy for epoch 66:  0.672821270310192\n",
      "Validation Accuracy for epoch 66:  0.3731527093596059\n",
      "LOSS train 1.5449626845121385 valid 1.8379788398742676\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 1.5427338547706604\n",
      "Training Accuracy for epoch 67:  0.6742983751846381\n",
      "Validation Accuracy for epoch 67:  0.37561576354679804\n",
      "LOSS train 1.5427338547706604 valid 1.8374987840652466\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 1.5405730838775635\n",
      "Training Accuracy for epoch 68:  0.6816838995568686\n",
      "Validation Accuracy for epoch 68:  0.3793103448275862\n",
      "LOSS train 1.5405730838775635 valid 1.8370047807693481\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 1.5384789270162582\n",
      "Training Accuracy for epoch 69:  0.6846381093057607\n",
      "Validation Accuracy for epoch 69:  0.3842364532019704\n",
      "LOSS train 1.5384789270162582 valid 1.836493968963623\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 1.5364484620094299\n",
      "Training Accuracy for epoch 70:  0.689807976366322\n",
      "Validation Accuracy for epoch 70:  0.3854679802955665\n",
      "LOSS train 1.5364484620094299 valid 1.8359653949737549\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 1.5344780422449111\n",
      "Training Accuracy for epoch 71:  0.6927621861152142\n",
      "Validation Accuracy for epoch 71:  0.3866995073891626\n",
      "LOSS train 1.5344780422449111 valid 1.835429072380066\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 1.532563812494278\n",
      "Training Accuracy for epoch 72:  0.6942392909896603\n",
      "Validation Accuracy for epoch 72:  0.3879310344827586\n",
      "LOSS train 1.532563812494278 valid 1.834877610206604\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 1.5307020547389985\n",
      "Training Accuracy for epoch 73:  0.6964549483013294\n",
      "Validation Accuracy for epoch 73:  0.3916256157635468\n",
      "LOSS train 1.5307020547389985 valid 1.8343170881271362\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 1.5288893880844117\n",
      "Training Accuracy for epoch 74:  0.6964549483013294\n",
      "Validation Accuracy for epoch 74:  0.39285714285714285\n",
      "LOSS train 1.5288893880844117 valid 1.8337472677230835\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 1.5271227167844772\n",
      "Training Accuracy for epoch 75:  0.6994091580502215\n",
      "Validation Accuracy for epoch 75:  0.39532019704433496\n",
      "LOSS train 1.5271227167844772 valid 1.8331738710403442\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 1.5253994302749634\n",
      "Training Accuracy for epoch 76:  0.7001477104874446\n",
      "Validation Accuracy for epoch 76:  0.39655172413793105\n",
      "LOSS train 1.5253994302749634 valid 1.832592248916626\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 1.52371715760231\n",
      "Training Accuracy for epoch 77:  0.7016248153618907\n",
      "Validation Accuracy for epoch 77:  0.39655172413793105\n",
      "LOSS train 1.52371715760231 valid 1.8320096731185913\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 1.5220738681554795\n",
      "Training Accuracy for epoch 78:  0.7038404726735599\n",
      "Validation Accuracy for epoch 78:  0.4027093596059113\n",
      "LOSS train 1.5220738681554795 valid 1.8314220905303955\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 1.5204677611589432\n",
      "Training Accuracy for epoch 79:  0.7038404726735599\n",
      "Validation Accuracy for epoch 79:  0.4051724137931034\n",
      "LOSS train 1.5204677611589432 valid 1.8308360576629639\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 1.5188972322940826\n",
      "Training Accuracy for epoch 80:  0.7045790251107829\n",
      "Validation Accuracy for epoch 80:  0.40763546798029554\n",
      "LOSS train 1.5188972322940826 valid 1.8302470445632935\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 1.517360850095749\n",
      "Training Accuracy for epoch 81:  0.7060561299852289\n",
      "Validation Accuracy for epoch 81:  0.4125615763546798\n",
      "LOSS train 1.517360850095749 valid 1.829660177230835\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 1.5158573105335236\n",
      "Training Accuracy for epoch 82:  0.706794682422452\n",
      "Validation Accuracy for epoch 82:  0.41502463054187194\n",
      "LOSS train 1.5158573105335236 valid 1.829074740409851\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 1.5143854308128357\n",
      "Training Accuracy for epoch 83:  0.706794682422452\n",
      "Validation Accuracy for epoch 83:  0.41502463054187194\n",
      "LOSS train 1.5143854308128357 valid 1.8284885883331299\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 1.512944062113762\n",
      "Training Accuracy for epoch 84:  0.707533234859675\n",
      "Validation Accuracy for epoch 84:  0.41625615763546797\n",
      "LOSS train 1.512944062113762 valid 1.827906847000122\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 1.511532187104225\n",
      "Training Accuracy for epoch 85:  0.7090103397341211\n",
      "Validation Accuracy for epoch 85:  0.41748768472906406\n",
      "LOSS train 1.511532187104225 valid 1.8273262977600098\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 1.5101488147974014\n",
      "Training Accuracy for epoch 86:  0.7119645494830132\n",
      "Validation Accuracy for epoch 86:  0.41748768472906406\n",
      "LOSS train 1.5101488147974014 valid 1.8267467021942139\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 1.5087930321693421\n",
      "Training Accuracy for epoch 87:  0.7134416543574594\n",
      "Validation Accuracy for epoch 87:  0.4187192118226601\n",
      "LOSS train 1.5087930321693421 valid 1.826172113418579\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 1.5074639490842818\n",
      "Training Accuracy for epoch 88:  0.7141802067946824\n",
      "Validation Accuracy for epoch 88:  0.41995073891625617\n",
      "LOSS train 1.5074639490842818 valid 1.8255962133407593\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 1.506160735964775\n",
      "Training Accuracy for epoch 89:  0.7149187592319055\n",
      "Validation Accuracy for epoch 89:  0.41995073891625617\n",
      "LOSS train 1.506160735964775 valid 1.825025200843811\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 1.504882614850998\n",
      "Training Accuracy for epoch 90:  0.7178729689807977\n",
      "Validation Accuracy for epoch 90:  0.4211822660098522\n",
      "LOSS train 1.504882614850998 valid 1.824456810951233\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 1.5036288030147553\n",
      "Training Accuracy for epoch 91:  0.7186115214180206\n",
      "Validation Accuracy for epoch 91:  0.4236453201970443\n",
      "LOSS train 1.5036288030147553 valid 1.823891043663025\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 1.5023986006975174\n",
      "Training Accuracy for epoch 92:  0.7186115214180206\n",
      "Validation Accuracy for epoch 92:  0.4273399014778325\n",
      "LOSS train 1.5023986006975174 valid 1.8233299255371094\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 1.5011913158893586\n",
      "Training Accuracy for epoch 93:  0.7200886262924667\n",
      "Validation Accuracy for epoch 93:  0.4273399014778325\n",
      "LOSS train 1.5011913158893586 valid 1.822771668434143\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 1.50000627887249\n",
      "Training Accuracy for epoch 94:  0.7223042836041359\n",
      "Validation Accuracy for epoch 94:  0.4273399014778325\n",
      "LOSS train 1.50000627887249 valid 1.8222156763076782\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 1.49884288585186\n",
      "Training Accuracy for epoch 95:  0.723781388478582\n",
      "Validation Accuracy for epoch 95:  0.42980295566502463\n",
      "LOSS train 1.49884288585186 valid 1.8216630220413208\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 1.4977005405426025\n",
      "Training Accuracy for epoch 96:  0.725258493353028\n",
      "Validation Accuracy for epoch 96:  0.43103448275862066\n",
      "LOSS train 1.4977005405426025 valid 1.8211133480072021\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 1.496578650712967\n",
      "Training Accuracy for epoch 97:  0.725258493353028\n",
      "Validation Accuracy for epoch 97:  0.43226600985221675\n",
      "LOSS train 1.496578650712967 valid 1.8205680847167969\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 1.495476710319519\n",
      "Training Accuracy for epoch 98:  0.7267355982274741\n",
      "Validation Accuracy for epoch 98:  0.43596059113300495\n",
      "LOSS train 1.495476710319519 valid 1.8200256824493408\n",
      "EPOCH 100:\n",
      "  batch 1000 loss: 1.4943941969871521\n",
      "Training Accuracy for epoch 99:  0.7274741506646972\n",
      "Validation Accuracy for epoch 99:  0.43596059113300495\n",
      "LOSS train 1.4943941969871521 valid 1.8194876909255981\n",
      "EPOCH 101:\n",
      "  batch 1000 loss: 1.4933306189775466\n",
      "Training Accuracy for epoch 100:  0.7282127031019202\n",
      "Validation Accuracy for epoch 100:  0.437192118226601\n",
      "LOSS train 1.4933306189775466 valid 1.8189518451690674\n",
      "EPOCH 102:\n",
      "  batch 1000 loss: 1.492285494208336\n",
      "Training Accuracy for epoch 101:  0.7304283604135894\n",
      "Validation Accuracy for epoch 101:  0.4408866995073892\n",
      "LOSS train 1.492285494208336 valid 1.8184194564819336\n",
      "EPOCH 103:\n",
      "  batch 1000 loss: 1.491258379817009\n",
      "Training Accuracy for epoch 102:  0.7304283604135894\n",
      "Validation Accuracy for epoch 102:  0.4421182266009852\n",
      "LOSS train 1.491258379817009 valid 1.817891001701355\n",
      "EPOCH 104:\n",
      "  batch 1000 loss: 1.490248815894127\n",
      "Training Accuracy for epoch 103:  0.7304283604135894\n",
      "Validation Accuracy for epoch 103:  0.4421182266009852\n",
      "LOSS train 1.490248815894127 valid 1.8173649311065674\n",
      "EPOCH 105:\n",
      "  batch 1000 loss: 1.489256418824196\n",
      "Training Accuracy for epoch 104:  0.7311669128508124\n",
      "Validation Accuracy for epoch 104:  0.4445812807881773\n",
      "LOSS train 1.489256418824196 valid 1.8168429136276245\n",
      "EPOCH 106:\n",
      "  batch 1000 loss: 1.4882807364463806\n",
      "Training Accuracy for epoch 105:  0.7326440177252584\n",
      "Validation Accuracy for epoch 105:  0.4458128078817734\n",
      "LOSS train 1.4882807364463806 valid 1.8163267374038696\n",
      "EPOCH 107:\n",
      "  batch 1000 loss: 1.4873213748931884\n",
      "Training Accuracy for epoch 106:  0.7326440177252584\n",
      "Validation Accuracy for epoch 106:  0.4458128078817734\n",
      "LOSS train 1.4873213748931884 valid 1.8158111572265625\n",
      "EPOCH 108:\n",
      "  batch 1000 loss: 1.4863779537677766\n",
      "Training Accuracy for epoch 107:  0.7333825701624815\n",
      "Validation Accuracy for epoch 107:  0.4458128078817734\n",
      "LOSS train 1.4863779537677766 valid 1.8152997493743896\n",
      "EPOCH 109:\n",
      "  batch 1000 loss: 1.4854500631093979\n",
      "Training Accuracy for epoch 108:  0.7348596750369276\n",
      "Validation Accuracy for epoch 108:  0.4458128078817734\n",
      "LOSS train 1.4854500631093979 valid 1.814792513847351\n",
      "EPOCH 110:\n",
      "  batch 1000 loss: 1.484537350654602\n",
      "Training Accuracy for epoch 109:  0.7348596750369276\n",
      "Validation Accuracy for epoch 109:  0.4482758620689655\n",
      "LOSS train 1.484537350654602 valid 1.814286231994629\n",
      "EPOCH 111:\n",
      "  batch 1000 loss: 1.4836393978595734\n",
      "Training Accuracy for epoch 110:  0.7370753323485968\n",
      "Validation Accuracy for epoch 110:  0.44950738916256155\n",
      "LOSS train 1.4836393978595734 valid 1.8137866258621216\n",
      "EPOCH 112:\n",
      "  batch 1000 loss: 1.4827558821439744\n",
      "Training Accuracy for epoch 111:  0.7370753323485968\n",
      "Validation Accuracy for epoch 111:  0.4482758620689655\n",
      "LOSS train 1.4827558821439744 valid 1.8132904767990112\n",
      "EPOCH 113:\n",
      "  batch 1000 loss: 1.4818864164352417\n",
      "Training Accuracy for epoch 112:  0.7370753323485968\n",
      "Validation Accuracy for epoch 112:  0.44950738916256155\n",
      "LOSS train 1.4818864164352417 valid 1.81279456615448\n",
      "EPOCH 114:\n",
      "  batch 1000 loss: 1.4810306614637374\n",
      "Training Accuracy for epoch 113:  0.7385524372230429\n",
      "Validation Accuracy for epoch 113:  0.45073891625615764\n",
      "LOSS train 1.4810306614637374 valid 1.8123043775558472\n",
      "EPOCH 115:\n",
      "  batch 1000 loss: 1.4801882574558258\n",
      "Training Accuracy for epoch 114:  0.7385524372230429\n",
      "Validation Accuracy for epoch 114:  0.45197044334975367\n",
      "LOSS train 1.4801882574558258 valid 1.8118155002593994\n",
      "EPOCH 116:\n",
      "  batch 1000 loss: 1.4793588466644287\n",
      "Training Accuracy for epoch 115:  0.7392909896602659\n",
      "Validation Accuracy for epoch 115:  0.45197044334975367\n",
      "LOSS train 1.4793588466644287 valid 1.8113313913345337\n",
      "EPOCH 117:\n",
      "  batch 1000 loss: 1.4785421199798583\n",
      "Training Accuracy for epoch 116:  0.7407680945347119\n",
      "Validation Accuracy for epoch 116:  0.45320197044334976\n",
      "LOSS train 1.4785421199798583 valid 1.8108503818511963\n",
      "EPOCH 118:\n",
      "  batch 1000 loss: 1.4777377123832702\n",
      "Training Accuracy for epoch 117:  0.7407680945347119\n",
      "Validation Accuracy for epoch 117:  0.4544334975369458\n",
      "LOSS train 1.4777377123832702 valid 1.810370922088623\n",
      "EPOCH 119:\n",
      "  batch 1000 loss: 1.4769453058242799\n",
      "Training Accuracy for epoch 118:  0.7407680945347119\n",
      "Validation Accuracy for epoch 118:  0.45689655172413796\n",
      "LOSS train 1.4769453058242799 valid 1.8098982572555542\n",
      "EPOCH 120:\n",
      "  batch 1000 loss: 1.4761645838022233\n",
      "Training Accuracy for epoch 119:  0.741506646971935\n",
      "Validation Accuracy for epoch 119:  0.45689655172413796\n",
      "LOSS train 1.4761645838022233 valid 1.809427261352539\n",
      "EPOCH 121:\n",
      "  batch 1000 loss: 1.475395248055458\n",
      "Training Accuracy for epoch 120:  0.742245199409158\n",
      "Validation Accuracy for epoch 120:  0.458128078817734\n",
      "LOSS train 1.475395248055458 valid 1.8089581727981567\n",
      "EPOCH 122:\n",
      "  batch 1000 loss: 1.47463696873188\n",
      "Training Accuracy for epoch 121:  0.742245199409158\n",
      "Validation Accuracy for epoch 121:  0.458128078817734\n",
      "LOSS train 1.47463696873188 valid 1.8084936141967773\n",
      "EPOCH 123:\n",
      "  batch 1000 loss: 1.4738894420862199\n",
      "Training Accuracy for epoch 122:  0.742245199409158\n",
      "Validation Accuracy for epoch 122:  0.458128078817734\n",
      "LOSS train 1.4738894420862199 valid 1.8080312013626099\n",
      "EPOCH 124:\n",
      "  batch 1000 loss: 1.4731523823738097\n",
      "Training Accuracy for epoch 123:  0.742245199409158\n",
      "Validation Accuracy for epoch 123:  0.458128078817734\n",
      "LOSS train 1.4731523823738097 valid 1.8075730800628662\n",
      "EPOCH 125:\n",
      "  batch 1000 loss: 1.4724254902601241\n",
      "Training Accuracy for epoch 124:  0.7429837518463811\n",
      "Validation Accuracy for epoch 124:  0.458128078817734\n",
      "LOSS train 1.4724254902601241 valid 1.8071180582046509\n",
      "EPOCH 126:\n",
      "  batch 1000 loss: 1.4717084980010986\n",
      "Training Accuracy for epoch 125:  0.7429837518463811\n",
      "Validation Accuracy for epoch 125:  0.4618226600985222\n",
      "LOSS train 1.4717084980010986 valid 1.8066654205322266\n",
      "EPOCH 127:\n",
      "  batch 1000 loss: 1.4710010995864868\n",
      "Training Accuracy for epoch 126:  0.7437223042836041\n",
      "Validation Accuracy for epoch 126:  0.4618226600985222\n",
      "LOSS train 1.4710010995864868 valid 1.8062164783477783\n",
      "EPOCH 128:\n",
      "  batch 1000 loss: 1.470303047299385\n",
      "Training Accuracy for epoch 127:  0.7437223042836041\n",
      "Validation Accuracy for epoch 127:  0.4630541871921182\n",
      "LOSS train 1.470303047299385 valid 1.805769443511963\n",
      "EPOCH 129:\n",
      "  batch 1000 loss: 1.4696140670776368\n",
      "Training Accuracy for epoch 128:  0.7437223042836041\n",
      "Validation Accuracy for epoch 128:  0.4630541871921182\n",
      "LOSS train 1.4696140670776368 valid 1.805326223373413\n",
      "EPOCH 130:\n",
      "  batch 1000 loss: 1.4689338799715042\n",
      "Training Accuracy for epoch 129:  0.7437223042836041\n",
      "Validation Accuracy for epoch 129:  0.4642857142857143\n",
      "LOSS train 1.4689338799715042 valid 1.8048864603042603\n",
      "EPOCH 131:\n",
      "  batch 1000 loss: 1.468262222290039\n",
      "Training Accuracy for epoch 130:  0.7437223042836041\n",
      "Validation Accuracy for epoch 130:  0.4630541871921182\n",
      "LOSS train 1.468262222290039 valid 1.8044503927230835\n",
      "EPOCH 132:\n",
      "  batch 1000 loss: 1.4675988664627075\n",
      "Training Accuracy for epoch 131:  0.7444608567208272\n",
      "Validation Accuracy for epoch 131:  0.4630541871921182\n",
      "LOSS train 1.4675988664627075 valid 1.8040152788162231\n",
      "EPOCH 133:\n",
      "  batch 1000 loss: 1.4669435431957245\n",
      "Training Accuracy for epoch 132:  0.7451994091580503\n",
      "Validation Accuracy for epoch 132:  0.4630541871921182\n",
      "LOSS train 1.4669435431957245 valid 1.803585171699524\n",
      "EPOCH 134:\n",
      "  batch 1000 loss: 1.4662960073947906\n",
      "Training Accuracy for epoch 133:  0.7459379615952733\n",
      "Validation Accuracy for epoch 133:  0.46551724137931033\n",
      "LOSS train 1.4662960073947906 valid 1.8031575679779053\n",
      "EPOCH 135:\n",
      "  batch 1000 loss: 1.4656559995412826\n",
      "Training Accuracy for epoch 134:  0.7459379615952733\n",
      "Validation Accuracy for epoch 134:  0.4667487684729064\n",
      "LOSS train 1.4656559995412826 valid 1.802732229232788\n",
      "EPOCH 136:\n",
      "  batch 1000 loss: 1.4650233075618744\n",
      "Training Accuracy for epoch 135:  0.7459379615952733\n",
      "Validation Accuracy for epoch 135:  0.46798029556650245\n",
      "LOSS train 1.4650233075618744 valid 1.802310824394226\n",
      "EPOCH 137:\n",
      "  batch 1000 loss: 1.4643976677656174\n",
      "Training Accuracy for epoch 136:  0.7459379615952733\n",
      "Validation Accuracy for epoch 136:  0.46798029556650245\n",
      "LOSS train 1.4643976677656174 valid 1.8018932342529297\n",
      "EPOCH 138:\n",
      "  batch 1000 loss: 1.463778855085373\n",
      "Training Accuracy for epoch 137:  0.7459379615952733\n",
      "Validation Accuracy for epoch 137:  0.4667487684729064\n",
      "LOSS train 1.463778855085373 valid 1.8014752864837646\n",
      "EPOCH 139:\n",
      "  batch 1000 loss: 1.4631665852069855\n",
      "Training Accuracy for epoch 138:  0.7466765140324964\n",
      "Validation Accuracy for epoch 138:  0.4667487684729064\n",
      "LOSS train 1.4631665852069855 valid 1.8010648488998413\n",
      "EPOCH 140:\n",
      "  batch 1000 loss: 1.4625606536865234\n",
      "Training Accuracy for epoch 139:  0.7474150664697193\n",
      "Validation Accuracy for epoch 139:  0.4667487684729064\n",
      "LOSS train 1.4625606536865234 valid 1.8006525039672852\n",
      "EPOCH 141:\n",
      "  batch 1000 loss: 1.4619608122110366\n",
      "Training Accuracy for epoch 140:  0.7474150664697193\n",
      "Validation Accuracy for epoch 140:  0.4667487684729064\n",
      "LOSS train 1.4619608122110366 valid 1.800248146057129\n",
      "EPOCH 142:\n",
      "  batch 1000 loss: 1.4613668191432954\n",
      "Training Accuracy for epoch 141:  0.7474150664697193\n",
      "Validation Accuracy for epoch 141:  0.47044334975369456\n",
      "LOSS train 1.4613668191432954 valid 1.7998450994491577\n",
      "EPOCH 143:\n",
      "  batch 1000 loss: 1.4607784014940262\n",
      "Training Accuracy for epoch 142:  0.7481536189069424\n",
      "Validation Accuracy for epoch 142:  0.47044334975369456\n",
      "LOSS train 1.4607784014940262 valid 1.7994439601898193\n",
      "EPOCH 144:\n",
      "  batch 1000 loss: 1.4601953403949737\n",
      "Training Accuracy for epoch 143:  0.7481536189069424\n",
      "Validation Accuracy for epoch 143:  0.47413793103448276\n",
      "LOSS train 1.4601953403949737 valid 1.7990463972091675\n",
      "EPOCH 145:\n",
      "  batch 1000 loss: 1.4596173539161683\n",
      "Training Accuracy for epoch 144:  0.7481536189069424\n",
      "Validation Accuracy for epoch 144:  0.47413793103448276\n",
      "LOSS train 1.4596173539161683 valid 1.7986537218093872\n",
      "EPOCH 146:\n",
      "  batch 1000 loss: 1.459044191122055\n",
      "Training Accuracy for epoch 145:  0.7481536189069424\n",
      "Validation Accuracy for epoch 145:  0.47413793103448276\n",
      "LOSS train 1.459044191122055 valid 1.7982622385025024\n",
      "EPOCH 147:\n",
      "  batch 1000 loss: 1.4584755737781525\n",
      "Training Accuracy for epoch 146:  0.7481536189069424\n",
      "Validation Accuracy for epoch 146:  0.4766009852216749\n",
      "LOSS train 1.4584755737781525 valid 1.797872543334961\n",
      "EPOCH 148:\n",
      "  batch 1000 loss: 1.4579112125635147\n",
      "Training Accuracy for epoch 147:  0.7481536189069424\n",
      "Validation Accuracy for epoch 147:  0.4766009852216749\n",
      "LOSS train 1.4579112125635147 valid 1.7974895238876343\n",
      "EPOCH 149:\n",
      "  batch 1000 loss: 1.4573508368730546\n",
      "Training Accuracy for epoch 148:  0.7488921713441654\n",
      "Validation Accuracy for epoch 148:  0.4766009852216749\n",
      "LOSS train 1.4573508368730546 valid 1.7971090078353882\n",
      "EPOCH 150:\n",
      "  batch 1000 loss: 1.4567941207885742\n",
      "Training Accuracy for epoch 149:  0.7488921713441654\n",
      "Validation Accuracy for epoch 149:  0.4766009852216749\n",
      "LOSS train 1.4567941207885742 valid 1.7967277765274048\n",
      "EPOCH 151:\n",
      "  batch 1000 loss: 1.4562407506704331\n",
      "Training Accuracy for epoch 150:  0.7488921713441654\n",
      "Validation Accuracy for epoch 150:  0.4753694581280788\n",
      "LOSS train 1.4562407506704331 valid 1.7963541746139526\n",
      "EPOCH 152:\n",
      "  batch 1000 loss: 1.4556904082298279\n",
      "Training Accuracy for epoch 151:  0.7511078286558346\n",
      "Validation Accuracy for epoch 151:  0.4753694581280788\n",
      "LOSS train 1.4556904082298279 valid 1.7959814071655273\n",
      "EPOCH 153:\n",
      "  batch 1000 loss: 1.4551427141427993\n",
      "Training Accuracy for epoch 152:  0.7518463810930576\n",
      "Validation Accuracy for epoch 152:  0.4753694581280788\n",
      "LOSS train 1.4551427141427993 valid 1.7956123352050781\n",
      "EPOCH 154:\n",
      "  batch 1000 loss: 1.4545973026752472\n",
      "Training Accuracy for epoch 153:  0.7525849335302807\n",
      "Validation Accuracy for epoch 153:  0.4753694581280788\n",
      "LOSS train 1.4545973026752472 valid 1.7952476739883423\n",
      "EPOCH 155:\n",
      "  batch 1000 loss: 1.4540537796020507\n",
      "Training Accuracy for epoch 154:  0.7525849335302807\n",
      "Validation Accuracy for epoch 154:  0.4753694581280788\n",
      "LOSS train 1.4540537796020507 valid 1.794886827468872\n",
      "EPOCH 156:\n",
      "  batch 1000 loss: 1.4535117077827453\n",
      "Training Accuracy for epoch 155:  0.7533234859675036\n",
      "Validation Accuracy for epoch 155:  0.4766009852216749\n",
      "LOSS train 1.4535117077827453 valid 1.794530987739563\n",
      "EPOCH 157:\n",
      "  batch 1000 loss: 1.4529706017971038\n",
      "Training Accuracy for epoch 156:  0.7540620384047267\n",
      "Validation Accuracy for epoch 156:  0.479064039408867\n",
      "LOSS train 1.4529706017971038 valid 1.7941769361495972\n",
      "EPOCH 158:\n",
      "  batch 1000 loss: 1.4524299947023391\n",
      "Training Accuracy for epoch 157:  0.7540620384047267\n",
      "Validation Accuracy for epoch 157:  0.479064039408867\n",
      "LOSS train 1.4524299947023391 valid 1.7938262224197388\n",
      "EPOCH 159:\n",
      "  batch 1000 loss: 1.4518893311023713\n",
      "Training Accuracy for epoch 158:  0.7540620384047267\n",
      "Validation Accuracy for epoch 158:  0.4802955665024631\n",
      "LOSS train 1.4518893311023713 valid 1.793480396270752\n",
      "EPOCH 160:\n",
      "  batch 1000 loss: 1.451348022699356\n",
      "Training Accuracy for epoch 159:  0.7540620384047267\n",
      "Validation Accuracy for epoch 159:  0.4802955665024631\n",
      "LOSS train 1.451348022699356 valid 1.7931371927261353\n",
      "EPOCH 161:\n",
      "  batch 1000 loss: 1.4508054461479187\n",
      "Training Accuracy for epoch 160:  0.7548005908419497\n",
      "Validation Accuracy for epoch 160:  0.4802955665024631\n",
      "LOSS train 1.4508054461479187 valid 1.7928005456924438\n",
      "EPOCH 162:\n",
      "  batch 1000 loss: 1.4502608830928803\n",
      "Training Accuracy for epoch 161:  0.7548005908419497\n",
      "Validation Accuracy for epoch 161:  0.4802955665024631\n",
      "LOSS train 1.4502608830928803 valid 1.7924652099609375\n",
      "EPOCH 163:\n",
      "  batch 1000 loss: 1.449713604927063\n",
      "Training Accuracy for epoch 162:  0.7562776957163959\n",
      "Validation Accuracy for epoch 162:  0.4802955665024631\n",
      "LOSS train 1.449713604927063 valid 1.7921383380889893\n",
      "EPOCH 164:\n",
      "  batch 1000 loss: 1.4491627877950668\n",
      "Training Accuracy for epoch 163:  0.7562776957163959\n",
      "Validation Accuracy for epoch 163:  0.4802955665024631\n",
      "LOSS train 1.4491627877950668 valid 1.791815996170044\n",
      "EPOCH 165:\n",
      "  batch 1000 loss: 1.4486075390577315\n",
      "Training Accuracy for epoch 164:  0.7562776957163959\n",
      "Validation Accuracy for epoch 164:  0.4802955665024631\n",
      "LOSS train 1.4486075390577315 valid 1.7914975881576538\n",
      "EPOCH 166:\n",
      "  batch 1000 loss: 1.4480468736886978\n",
      "Training Accuracy for epoch 165:  0.7570162481536189\n",
      "Validation Accuracy for epoch 165:  0.4802955665024631\n",
      "LOSS train 1.4480468736886978 valid 1.7911840677261353\n",
      "EPOCH 167:\n",
      "  batch 1000 loss: 1.4474797443151475\n",
      "Training Accuracy for epoch 166:  0.7570162481536189\n",
      "Validation Accuracy for epoch 166:  0.4802955665024631\n",
      "LOSS train 1.4474797443151475 valid 1.7908775806427002\n",
      "EPOCH 168:\n",
      "  batch 1000 loss: 1.4469050234556198\n",
      "Training Accuracy for epoch 167:  0.7570162481536189\n",
      "Validation Accuracy for epoch 167:  0.4827586206896552\n",
      "LOSS train 1.4469050234556198 valid 1.79057776927948\n",
      "EPOCH 169:\n",
      "  batch 1000 loss: 1.446321493268013\n",
      "Training Accuracy for epoch 168:  0.7570162481536189\n",
      "Validation Accuracy for epoch 168:  0.4827586206896552\n",
      "LOSS train 1.446321493268013 valid 1.7902857065200806\n",
      "EPOCH 170:\n",
      "  batch 1000 loss: 1.4457278653383254\n",
      "Training Accuracy for epoch 169:  0.7570162481536189\n",
      "Validation Accuracy for epoch 169:  0.4827586206896552\n",
      "LOSS train 1.4457278653383254 valid 1.790000557899475\n",
      "EPOCH 171:\n",
      "  batch 1000 loss: 1.4451227703094482\n",
      "Training Accuracy for epoch 170:  0.7570162481536189\n",
      "Validation Accuracy for epoch 170:  0.4827586206896552\n",
      "LOSS train 1.4451227703094482 valid 1.7897223234176636\n",
      "EPOCH 172:\n",
      "  batch 1000 loss: 1.4445048391819\n",
      "Training Accuracy for epoch 171:  0.7570162481536189\n",
      "Validation Accuracy for epoch 171:  0.4827586206896552\n",
      "LOSS train 1.4445048391819 valid 1.7894552946090698\n",
      "EPOCH 173:\n",
      "  batch 1000 loss: 1.4438726012706757\n",
      "Training Accuracy for epoch 172:  0.758493353028065\n",
      "Validation Accuracy for epoch 172:  0.4839901477832512\n",
      "LOSS train 1.4438726012706757 valid 1.7891972064971924\n",
      "EPOCH 174:\n",
      "  batch 1000 loss: 1.4432246741056443\n",
      "Training Accuracy for epoch 173:  0.7607090103397341\n",
      "Validation Accuracy for epoch 173:  0.4852216748768473\n",
      "LOSS train 1.4432246741056443 valid 1.7889509201049805\n",
      "EPOCH 175:\n",
      "  batch 1000 loss: 1.442559698343277\n",
      "Training Accuracy for epoch 174:  0.7614475627769571\n",
      "Validation Accuracy for epoch 174:  0.4852216748768473\n",
      "LOSS train 1.442559698343277 valid 1.7887134552001953\n",
      "EPOCH 176:\n",
      "  batch 1000 loss: 1.4418764355182647\n",
      "Training Accuracy for epoch 175:  0.7621861152141802\n",
      "Validation Accuracy for epoch 175:  0.4852216748768473\n",
      "LOSS train 1.4418764355182647 valid 1.7884910106658936\n",
      "EPOCH 177:\n",
      "  batch 1000 loss: 1.4411738547086717\n",
      "Training Accuracy for epoch 176:  0.7636632200886263\n",
      "Validation Accuracy for epoch 176:  0.4852216748768473\n",
      "LOSS train 1.4411738547086717 valid 1.7882825136184692\n",
      "EPOCH 178:\n",
      "  batch 1000 loss: 1.4404512095451354\n",
      "Training Accuracy for epoch 177:  0.7636632200886263\n",
      "Validation Accuracy for epoch 177:  0.4852216748768473\n",
      "LOSS train 1.4404512095451354 valid 1.7880865335464478\n",
      "EPOCH 179:\n",
      "  batch 1000 loss: 1.4397081027030945\n",
      "Training Accuracy for epoch 178:  0.7644017725258493\n",
      "Validation Accuracy for epoch 178:  0.48645320197044334\n",
      "LOSS train 1.4397081027030945 valid 1.787909984588623\n",
      "EPOCH 180:\n",
      "  batch 1000 loss: 1.4389445828199388\n",
      "Training Accuracy for epoch 179:  0.7658788774002954\n",
      "Validation Accuracy for epoch 179:  0.48645320197044334\n",
      "LOSS train 1.4389445828199388 valid 1.787750244140625\n",
      "EPOCH 181:\n",
      "  batch 1000 loss: 1.4381612737178802\n",
      "Training Accuracy for epoch 180:  0.7680945347119645\n",
      "Validation Accuracy for epoch 180:  0.48645320197044334\n",
      "LOSS train 1.4381612737178802 valid 1.78760826587677\n",
      "EPOCH 182:\n",
      "  batch 1000 loss: 1.4373593736886978\n",
      "Training Accuracy for epoch 181:  0.7703101920236337\n",
      "Validation Accuracy for epoch 181:  0.4876847290640394\n",
      "LOSS train 1.4373593736886978 valid 1.7874845266342163\n",
      "EPOCH 183:\n",
      "  batch 1000 loss: 1.4365407673120498\n",
      "Training Accuracy for epoch 182:  0.7717872968980798\n",
      "Validation Accuracy for epoch 182:  0.4876847290640394\n",
      "LOSS train 1.4365407673120498 valid 1.787381887435913\n",
      "EPOCH 184:\n",
      "  batch 1000 loss: 1.4357079904079437\n",
      "Training Accuracy for epoch 183:  0.7725258493353028\n",
      "Validation Accuracy for epoch 183:  0.4876847290640394\n",
      "LOSS train 1.4357079904079437 valid 1.787302017211914\n",
      "EPOCH 185:\n",
      "  batch 1000 loss: 1.434864218711853\n",
      "Training Accuracy for epoch 184:  0.7732644017725259\n",
      "Validation Accuracy for epoch 184:  0.48891625615763545\n",
      "LOSS train 1.434864218711853 valid 1.7872400283813477\n",
      "EPOCH 186:\n",
      "  batch 1000 loss: 1.4340131347179412\n",
      "Training Accuracy for epoch 185:  0.7740029542097489\n",
      "Validation Accuracy for epoch 185:  0.4876847290640394\n",
      "LOSS train 1.4340131347179412 valid 1.7871979475021362\n",
      "EPOCH 187:\n",
      "  batch 1000 loss: 1.4331588929891586\n",
      "Training Accuracy for epoch 186:  0.776218611521418\n",
      "Validation Accuracy for epoch 186:  0.4876847290640394\n",
      "LOSS train 1.4331588929891586 valid 1.7871755361557007\n",
      "EPOCH 188:\n",
      "  batch 1000 loss: 1.4323058450222015\n",
      "Training Accuracy for epoch 187:  0.7769571639586411\n",
      "Validation Accuracy for epoch 187:  0.4876847290640394\n",
      "LOSS train 1.4323058450222015 valid 1.787170648574829\n",
      "EPOCH 189:\n",
      "  batch 1000 loss: 1.4314584033489228\n",
      "Training Accuracy for epoch 188:  0.7799113737075333\n",
      "Validation Accuracy for epoch 188:  0.48891625615763545\n",
      "LOSS train 1.4314584033489228 valid 1.7871816158294678\n",
      "EPOCH 190:\n",
      "  batch 1000 loss: 1.4306207926273347\n",
      "Training Accuracy for epoch 189:  0.7813884785819794\n",
      "Validation Accuracy for epoch 189:  0.49014778325123154\n",
      "LOSS train 1.4306207926273347 valid 1.7872052192687988\n",
      "EPOCH 191:\n",
      "  batch 1000 loss: 1.4297968225479125\n",
      "Training Accuracy for epoch 190:  0.7843426883308715\n",
      "Validation Accuracy for epoch 190:  0.49014778325123154\n",
      "LOSS train 1.4297968225479125 valid 1.7872390747070312\n",
      "EPOCH 192:\n",
      "  batch 1000 loss: 1.4289897410869599\n",
      "Training Accuracy for epoch 191:  0.7850812407680945\n",
      "Validation Accuracy for epoch 191:  0.49014778325123154\n",
      "LOSS train 1.4289897410869599 valid 1.7872775793075562\n",
      "EPOCH 193:\n",
      "  batch 1000 loss: 1.4282020890712739\n",
      "Training Accuracy for epoch 192:  0.7858197932053176\n",
      "Validation Accuracy for epoch 192:  0.49014778325123154\n",
      "LOSS train 1.4282020890712739 valid 1.7873202562332153\n",
      "EPOCH 194:\n",
      "  batch 1000 loss: 1.427435626745224\n",
      "Training Accuracy for epoch 193:  0.7887740029542097\n",
      "Validation Accuracy for epoch 193:  0.49014778325123154\n",
      "LOSS train 1.427435626745224 valid 1.787360429763794\n",
      "EPOCH 195:\n",
      "  batch 1000 loss: 1.4266913384199142\n",
      "Training Accuracy for epoch 194:  0.7887740029542097\n",
      "Validation Accuracy for epoch 194:  0.49014778325123154\n",
      "LOSS train 1.4266913384199142 valid 1.787398099899292\n",
      "EPOCH 196:\n",
      "  batch 1000 loss: 1.4259694850444793\n",
      "Training Accuracy for epoch 195:  0.7895125553914328\n",
      "Validation Accuracy for epoch 195:  0.49137931034482757\n",
      "LOSS train 1.4259694850444793 valid 1.7874256372451782\n",
      "EPOCH 197:\n",
      "  batch 1000 loss: 1.4252696806192398\n",
      "Training Accuracy for epoch 196:  0.7902511078286558\n",
      "Validation Accuracy for epoch 196:  0.49137931034482757\n",
      "LOSS train 1.4252696806192398 valid 1.7874428033828735\n",
      "EPOCH 198:\n",
      "  batch 1000 loss: 1.4245910769701005\n",
      "Training Accuracy for epoch 197:  0.792466765140325\n",
      "Validation Accuracy for epoch 197:  0.49261083743842365\n",
      "LOSS train 1.4245910769701005 valid 1.7874469757080078\n",
      "EPOCH 199:\n",
      "  batch 1000 loss: 1.4239324057102203\n",
      "Training Accuracy for epoch 198:  0.792466765140325\n",
      "Validation Accuracy for epoch 198:  0.49261083743842365\n",
      "LOSS train 1.4239324057102203 valid 1.7874343395233154\n",
      "EPOCH 200:\n",
      "  batch 1000 loss: 1.4232922086715698\n",
      "Training Accuracy for epoch 199:  0.7939438700147711\n",
      "Validation Accuracy for epoch 199:  0.49261083743842365\n",
      "LOSS train 1.4232922086715698 valid 1.7874029874801636\n"
     ]
    }
   ],
   "source": [
    "# define one training epoch \n",
    "def train_one_epoch(model, train_nodes, train_labels, epoch_index, tb_writer, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        epoch_index: index of the epoch \n",
    "        tb_writer: a writer object that can record all the training statistics \n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    true_pairs = 0\n",
    "\n",
    "    # Work thought the entire dataset\n",
    "    for i, (inputs, labels) in enumerate(zip(train_nodes, train_labels)):\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the accuracy, loss and its gradients\n",
    "        # print(\"Output: \", outputs)\n",
    "        # print(\"Label: \", labels)\n",
    "        if torch.argmax(outputs) == labels: \n",
    "            true_pairs += 1\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_nodes) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        \n",
    "    # Generate Accuracy \n",
    "    print(f\"Training Accuracy for epoch {epoch_index}: \", true_pairs/len(train_nodes))\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/GCN_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model=model, \n",
    "        train_nodes=train_nodes, \n",
    "        train_labels=train_labels, \n",
    "        epoch_index=epoch_number, \n",
    "        tb_writer=writer, \n",
    "        loss_fn=loss, \n",
    "        optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    true_pairs = 0\n",
    "    for i, (vdata, vlabel) in enumerate(zip(validation_nodes, validation_labels)):\n",
    "        voutputs = model(vdata)\n",
    "        vloss = loss(voutputs, vlabel)\n",
    "        running_vloss += vloss\n",
    "        if torch.argmax(voutputs) == vlabel: \n",
    "            true_pairs +=1\n",
    "    print(f\"Validation Accuracy for epoch {epoch}: \", true_pairs/len(validation_nodes))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'GCNs/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\3425545340.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for epoch 199:  0.47601476014760147\n",
      "LOSS test 1.8437950611114502 \n"
     ]
    }
   ],
   "source": [
    "# test GCN model on testing dataset\n",
    "model.train(False) # turn training off for all parameters\n",
    "\n",
    "running_tloss = 0.0\n",
    "true_pairs = 0\n",
    "for i, (tdata, tlabel) in enumerate(zip(test_nodes, test_labels)):\n",
    "    toutputs = model(tdata)\n",
    "    tloss = loss(toutputs, tlabel)\n",
    "    running_tloss += tloss\n",
    "    if torch.argmax(toutputs) == tlabel:\n",
    "        true_pairs += 1 \n",
    "print(f\"Test Accuracy for epoch {epoch}: \", true_pairs/len(test_nodes))\n",
    "\n",
    "avg_tloss = running_tloss / len(test_nodes)\n",
    "print('LOSS test {} '.format(avg_tloss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare the performance of our 1-layer GCN model with a 1-layer FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a 1 layer FCN \n",
    "class FCN(nn.Module): \n",
    "    def __init__(self, adjacency_matrix, output_size, device) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.A = adjacency_matrix.to(self.device)\n",
    "        self.output_size = output_size\n",
    "        self.hidden_layer = nn.Linear(in_features=self.A.shape[1], out_features=self.output_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, X): \n",
    "        output = self.hidden_layer(X)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "# Set up the training parameter \n",
    "model = FCN(adjacency_matrix=A, output_size=len(categories.items()), device=device).to(device)\n",
    "# print(\"\\nParameters in the one layer GCN: \\n\")\n",
    "# for param in model.parameters():\n",
    "#     print(param)\n",
    "\n",
    "# create cross entropy loss calculation, optimization \n",
    "loss = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\959668787.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1000 loss: 1.9392861096858978\n",
      "Training Accuracy for epoch 0:  0.25997045790251105\n",
      "Validation Accuracy for epoch 0:  0.2894088669950739\n",
      "LOSS train 1.9392861096858978 valid 1.9386861324310303\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.9169930337667465\n",
      "Training Accuracy for epoch 1:  0.37740029542097486\n",
      "Validation Accuracy for epoch 1:  0.2894088669950739\n",
      "LOSS train 1.9169930337667465 valid 1.928459882736206\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.887115882396698\n",
      "Training Accuracy for epoch 2:  0.38552437223042835\n",
      "Validation Accuracy for epoch 2:  0.2894088669950739\n",
      "LOSS train 1.887115882396698 valid 1.91608726978302\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.8513731492757797\n",
      "Training Accuracy for epoch 3:  0.3803545051698671\n",
      "Validation Accuracy for epoch 3:  0.2894088669950739\n",
      "LOSS train 1.8513731492757797 valid 1.9042004346847534\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.815835992217064\n",
      "Training Accuracy for epoch 4:  0.3810930576070901\n",
      "Validation Accuracy for epoch 4:  0.2894088669950739\n",
      "LOSS train 1.815835992217064 valid 1.8952863216400146\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 1.785897097468376\n",
      "Training Accuracy for epoch 5:  0.38404726735598227\n",
      "Validation Accuracy for epoch 5:  0.2894088669950739\n",
      "LOSS train 1.785897097468376 valid 1.8895385265350342\n",
      "EPOCH 7:\n",
      "  batch 1000 loss: 1.7626608145236968\n",
      "Training Accuracy for epoch 6:  0.3862629246676514\n",
      "Validation Accuracy for epoch 6:  0.2894088669950739\n",
      "LOSS train 1.7626608145236968 valid 1.8860548734664917\n",
      "EPOCH 8:\n",
      "  batch 1000 loss: 1.7444472081661224\n",
      "Training Accuracy for epoch 7:  0.3914327917282127\n",
      "Validation Accuracy for epoch 7:  0.2894088669950739\n",
      "LOSS train 1.7444472081661224 valid 1.8840527534484863\n",
      "EPOCH 9:\n",
      "  batch 1000 loss: 1.729200345993042\n",
      "Training Accuracy for epoch 8:  0.4010339734121123\n",
      "Validation Accuracy for epoch 8:  0.2894088669950739\n",
      "LOSS train 1.729200345993042 valid 1.8830379247665405\n",
      "EPOCH 10:\n",
      "  batch 1000 loss: 1.7155647809505463\n",
      "Training Accuracy for epoch 9:  0.4113737075332349\n",
      "Validation Accuracy for epoch 9:  0.2894088669950739\n",
      "LOSS train 1.7155647809505463 valid 1.882686972618103\n",
      "EPOCH 11:\n",
      "  batch 1000 loss: 1.7029872913360595\n",
      "Training Accuracy for epoch 10:  0.42909896602658787\n",
      "Validation Accuracy for epoch 10:  0.2894088669950739\n",
      "LOSS train 1.7029872913360595 valid 1.8827486038208008\n",
      "EPOCH 12:\n",
      "  batch 1000 loss: 1.6914142744541167\n",
      "Training Accuracy for epoch 11:  0.4401772525849335\n",
      "Validation Accuracy for epoch 11:  0.2894088669950739\n",
      "LOSS train 1.6914142744541167 valid 1.8830238580703735\n",
      "EPOCH 13:\n",
      "  batch 1000 loss: 1.6809070907831192\n",
      "Training Accuracy for epoch 12:  0.4468242245199409\n",
      "Validation Accuracy for epoch 12:  0.2894088669950739\n",
      "LOSS train 1.6809070907831192 valid 1.8833732604980469\n",
      "EPOCH 14:\n",
      "  batch 1000 loss: 1.6714326295852662\n",
      "Training Accuracy for epoch 13:  0.45420974889217136\n",
      "Validation Accuracy for epoch 13:  0.2894088669950739\n",
      "LOSS train 1.6714326295852662 valid 1.8837225437164307\n",
      "EPOCH 15:\n",
      "  batch 1000 loss: 1.6628659447431564\n",
      "Training Accuracy for epoch 14:  0.4711964549483013\n",
      "Validation Accuracy for epoch 14:  0.2894088669950739\n",
      "LOSS train 1.6628659447431564 valid 1.8840402364730835\n",
      "EPOCH 16:\n",
      "  batch 1000 loss: 1.6550625556707381\n",
      "Training Accuracy for epoch 15:  0.48227474150664695\n",
      "Validation Accuracy for epoch 15:  0.29064039408866993\n",
      "LOSS train 1.6550625556707381 valid 1.8843083381652832\n",
      "EPOCH 17:\n",
      "  batch 1000 loss: 1.6479030554294587\n",
      "Training Accuracy for epoch 16:  0.49261447562776955\n",
      "Validation Accuracy for epoch 16:  0.291871921182266\n",
      "LOSS train 1.6479030554294587 valid 1.8845118284225464\n",
      "EPOCH 18:\n",
      "  batch 1000 loss: 1.6412998433113097\n",
      "Training Accuracy for epoch 17:  0.5007385524372231\n",
      "Validation Accuracy for epoch 17:  0.29310344827586204\n",
      "LOSS train 1.6412998433113097 valid 1.8846367597579956\n",
      "EPOCH 19:\n",
      "  batch 1000 loss: 1.6351873321533203\n",
      "Training Accuracy for epoch 18:  0.5140324963072378\n",
      "Validation Accuracy for epoch 18:  0.29679802955665024\n",
      "LOSS train 1.6351873321533203 valid 1.8846678733825684\n",
      "EPOCH 20:\n",
      "  batch 1000 loss: 1.6295112171173096\n",
      "Training Accuracy for epoch 19:  0.5265878877400295\n",
      "Validation Accuracy for epoch 19:  0.30049261083743845\n",
      "LOSS train 1.6295112171173096 valid 1.8846005201339722\n",
      "EPOCH 21:\n",
      "  batch 1000 loss: 1.6242233935594559\n",
      "Training Accuracy for epoch 20:  0.5361890694239291\n",
      "Validation Accuracy for epoch 20:  0.3066502463054187\n",
      "LOSS train 1.6242233935594559 valid 1.8844432830810547\n",
      "EPOCH 22:\n",
      "  batch 1000 loss: 1.6192808117866515\n",
      "Training Accuracy for epoch 21:  0.5428360413589365\n",
      "Validation Accuracy for epoch 21:  0.3066502463054187\n",
      "LOSS train 1.6192808117866515 valid 1.884203314781189\n",
      "EPOCH 23:\n",
      "  batch 1000 loss: 1.6146448986530304\n",
      "Training Accuracy for epoch 22:  0.5465288035450517\n",
      "Validation Accuracy for epoch 22:  0.3066502463054187\n",
      "LOSS train 1.6146448986530304 valid 1.883900761604309\n",
      "EPOCH 24:\n",
      "  batch 1000 loss: 1.610281134724617\n",
      "Training Accuracy for epoch 23:  0.553175775480059\n",
      "Validation Accuracy for epoch 23:  0.3066502463054187\n",
      "LOSS train 1.610281134724617 valid 1.8835530281066895\n",
      "EPOCH 25:\n",
      "  batch 1000 loss: 1.6061580933332442\n",
      "Training Accuracy for epoch 24:  0.5576070901033974\n",
      "Validation Accuracy for epoch 24:  0.3091133004926108\n",
      "LOSS train 1.6061580933332442 valid 1.8831737041473389\n",
      "EPOCH 26:\n",
      "  batch 1000 loss: 1.6022466295957565\n",
      "Training Accuracy for epoch 25:  0.5612998522895125\n",
      "Validation Accuracy for epoch 25:  0.31157635467980294\n",
      "LOSS train 1.6022466295957565 valid 1.8827747106552124\n",
      "EPOCH 27:\n",
      "  batch 1000 loss: 1.5985193513631821\n",
      "Training Accuracy for epoch 26:  0.5649926144756278\n",
      "Validation Accuracy for epoch 26:  0.31403940886699505\n",
      "LOSS train 1.5985193513631821 valid 1.8823648691177368\n",
      "EPOCH 28:\n",
      "  batch 1000 loss: 1.5949500212669372\n",
      "Training Accuracy for epoch 27:  0.5738552437223042\n",
      "Validation Accuracy for epoch 27:  0.31403940886699505\n",
      "LOSS train 1.5949500212669372 valid 1.8819540739059448\n",
      "EPOCH 29:\n",
      "  batch 1000 loss: 1.5915132286548614\n",
      "Training Accuracy for epoch 28:  0.5797636632200887\n",
      "Validation Accuracy for epoch 28:  0.31773399014778325\n",
      "LOSS train 1.5915132286548614 valid 1.8815430402755737\n",
      "EPOCH 30:\n",
      "  batch 1000 loss: 1.5881838957071304\n",
      "Training Accuracy for epoch 29:  0.5819793205317577\n",
      "Validation Accuracy for epoch 29:  0.31773399014778325\n",
      "LOSS train 1.5881838957071304 valid 1.8811354637145996\n",
      "EPOCH 31:\n",
      "  batch 1000 loss: 1.584937028169632\n",
      "Training Accuracy for epoch 30:  0.5841949778434269\n",
      "Validation Accuracy for epoch 30:  0.31896551724137934\n",
      "LOSS train 1.584937028169632 valid 1.8807367086410522\n",
      "EPOCH 32:\n",
      "  batch 1000 loss: 1.5817473119497298\n",
      "Training Accuracy for epoch 31:  0.587149187592319\n",
      "Validation Accuracy for epoch 31:  0.32019704433497537\n",
      "LOSS train 1.5817473119497298 valid 1.8803517818450928\n",
      "EPOCH 33:\n",
      "  batch 1000 loss: 1.5785889492034912\n",
      "Training Accuracy for epoch 32:  0.5878877400295421\n",
      "Validation Accuracy for epoch 32:  0.32142857142857145\n",
      "LOSS train 1.5785889492034912 valid 1.8799769878387451\n",
      "EPOCH 34:\n",
      "  batch 1000 loss: 1.5754355586767197\n",
      "Training Accuracy for epoch 33:  0.5908419497784343\n",
      "Validation Accuracy for epoch 33:  0.32142857142857145\n",
      "LOSS train 1.5754355586767197 valid 1.8796135187149048\n",
      "EPOCH 35:\n",
      "  batch 1000 loss: 1.572260499715805\n",
      "Training Accuracy for epoch 34:  0.5930576070901034\n",
      "Validation Accuracy for epoch 34:  0.32142857142857145\n",
      "LOSS train 1.572260499715805 valid 1.8792681694030762\n",
      "EPOCH 36:\n",
      "  batch 1000 loss: 1.5690376409292222\n",
      "Training Accuracy for epoch 35:  0.5989660265878878\n",
      "Validation Accuracy for epoch 35:  0.3251231527093596\n",
      "LOSS train 1.5690376409292222 valid 1.8789386749267578\n",
      "EPOCH 37:\n",
      "  batch 1000 loss: 1.5657430478334426\n",
      "Training Accuracy for epoch 36:  0.6026587887740029\n",
      "Validation Accuracy for epoch 36:  0.3251231527093596\n",
      "LOSS train 1.5657430478334426 valid 1.8786273002624512\n",
      "EPOCH 38:\n",
      "  batch 1000 loss: 1.5623574192523957\n",
      "Training Accuracy for epoch 37:  0.6093057607090103\n",
      "Validation Accuracy for epoch 37:  0.3251231527093596\n",
      "LOSS train 1.5623574192523957 valid 1.87833571434021\n",
      "EPOCH 39:\n",
      "  batch 1000 loss: 1.5588696064949035\n",
      "Training Accuracy for epoch 38:  0.6144756277695717\n",
      "Validation Accuracy for epoch 38:  0.3251231527093596\n",
      "LOSS train 1.5588696064949035 valid 1.8780597448349\n",
      "EPOCH 40:\n",
      "  batch 1000 loss: 1.555280174255371\n",
      "Training Accuracy for epoch 39:  0.6211225997045791\n",
      "Validation Accuracy for epoch 39:  0.3263546798029557\n",
      "LOSS train 1.555280174255371 valid 1.8778033256530762\n",
      "EPOCH 41:\n",
      "  batch 1000 loss: 1.5516043300628661\n",
      "Training Accuracy for epoch 40:  0.6292466765140325\n",
      "Validation Accuracy for epoch 40:  0.3263546798029557\n",
      "LOSS train 1.5516043300628661 valid 1.8775577545166016\n",
      "EPOCH 42:\n",
      "  batch 1000 loss: 1.5478720979690552\n",
      "Training Accuracy for epoch 41:  0.638109305760709\n",
      "Validation Accuracy for epoch 41:  0.3251231527093596\n",
      "LOSS train 1.5478720979690552 valid 1.8773229122161865\n",
      "EPOCH 43:\n",
      "  batch 1000 loss: 1.5441249502897263\n",
      "Training Accuracy for epoch 42:  0.6454948301329394\n",
      "Validation Accuracy for epoch 42:  0.32389162561576357\n",
      "LOSS train 1.5441249502897263 valid 1.8770841360092163\n",
      "EPOCH 44:\n",
      "  batch 1000 loss: 1.5404089562892913\n",
      "Training Accuracy for epoch 43:  0.6558345642540621\n",
      "Validation Accuracy for epoch 43:  0.32389162561576357\n",
      "LOSS train 1.5404089562892913 valid 1.8768377304077148\n",
      "EPOCH 45:\n",
      "  batch 1000 loss: 1.5367667195796966\n",
      "Training Accuracy for epoch 44:  0.6691285081240768\n",
      "Validation Accuracy for epoch 44:  0.3263546798029557\n",
      "LOSS train 1.5367667195796966 valid 1.8765720129013062\n",
      "EPOCH 46:\n",
      "  batch 1000 loss: 1.5332309677600862\n",
      "Training Accuracy for epoch 45:  0.6772525849335302\n",
      "Validation Accuracy for epoch 45:  0.3275862068965517\n",
      "LOSS train 1.5332309677600862 valid 1.8762801885604858\n",
      "EPOCH 47:\n",
      "  batch 1000 loss: 1.529821632027626\n",
      "Training Accuracy for epoch 46:  0.6831610044313147\n",
      "Validation Accuracy for epoch 46:  0.3275862068965517\n",
      "LOSS train 1.529821632027626 valid 1.87595534324646\n",
      "EPOCH 48:\n",
      "  batch 1000 loss: 1.5265466763973237\n",
      "Training Accuracy for epoch 47:  0.6868537666174298\n",
      "Validation Accuracy for epoch 47:  0.3312807881773399\n",
      "LOSS train 1.5265466763973237 valid 1.87559974193573\n",
      "EPOCH 49:\n",
      "  batch 1000 loss: 1.5234048938751221\n",
      "Training Accuracy for epoch 48:  0.689807976366322\n",
      "Validation Accuracy for epoch 48:  0.3312807881773399\n",
      "LOSS train 1.5234048938751221 valid 1.8752107620239258\n",
      "EPOCH 50:\n",
      "  batch 1000 loss: 1.5203894894123078\n",
      "Training Accuracy for epoch 49:  0.6949778434268833\n",
      "Validation Accuracy for epoch 49:  0.33497536945812806\n",
      "LOSS train 1.5203894894123078 valid 1.8747981786727905\n",
      "EPOCH 51:\n",
      "  batch 1000 loss: 1.5174908349514007\n",
      "Training Accuracy for epoch 50:  0.6986706056129985\n",
      "Validation Accuracy for epoch 50:  0.33620689655172414\n",
      "LOSS train 1.5174908349514007 valid 1.8743637800216675\n",
      "EPOCH 52:\n",
      "  batch 1000 loss: 1.514698345899582\n",
      "Training Accuracy for epoch 51:  0.7031019202363368\n",
      "Validation Accuracy for epoch 51:  0.3374384236453202\n",
      "LOSS train 1.514698345899582 valid 1.87391197681427\n",
      "EPOCH 53:\n",
      "  batch 1000 loss: 1.5120015774965285\n",
      "Training Accuracy for epoch 52:  0.7045790251107829\n",
      "Validation Accuracy for epoch 52:  0.3399014778325123\n",
      "LOSS train 1.5120015774965285 valid 1.8734511137008667\n",
      "EPOCH 54:\n",
      "  batch 1000 loss: 1.509390701651573\n",
      "Training Accuracy for epoch 53:  0.705317577548006\n",
      "Validation Accuracy for epoch 53:  0.3411330049261084\n",
      "LOSS train 1.509390701651573 valid 1.8729825019836426\n",
      "EPOCH 55:\n",
      "  batch 1000 loss: 1.5068565932512283\n",
      "Training Accuracy for epoch 54:  0.7097488921713442\n",
      "Validation Accuracy for epoch 54:  0.3435960591133005\n",
      "LOSS train 1.5068565932512283 valid 1.8725095987319946\n",
      "EPOCH 56:\n",
      "  batch 1000 loss: 1.5043907920122146\n",
      "Training Accuracy for epoch 55:  0.7127031019202363\n",
      "Validation Accuracy for epoch 55:  0.3435960591133005\n",
      "LOSS train 1.5043907920122146 valid 1.872036337852478\n",
      "EPOCH 57:\n",
      "  batch 1000 loss: 1.5019854296445847\n",
      "Training Accuracy for epoch 56:  0.7141802067946824\n",
      "Validation Accuracy for epoch 56:  0.3472906403940887\n",
      "LOSS train 1.5019854296445847 valid 1.8715661764144897\n",
      "EPOCH 58:\n",
      "  batch 1000 loss: 1.4996330662965776\n",
      "Training Accuracy for epoch 57:  0.7178729689807977\n",
      "Validation Accuracy for epoch 57:  0.35098522167487683\n",
      "LOSS train 1.4996330662965776 valid 1.8711001873016357\n",
      "EPOCH 59:\n",
      "  batch 1000 loss: 1.4973266466856003\n",
      "Training Accuracy for epoch 58:  0.7215657311669128\n",
      "Validation Accuracy for epoch 58:  0.3522167487684729\n",
      "LOSS train 1.4973266466856003 valid 1.8706386089324951\n",
      "EPOCH 60:\n",
      "  batch 1000 loss: 1.4950593823194505\n",
      "Training Accuracy for epoch 59:  0.723781388478582\n",
      "Validation Accuracy for epoch 59:  0.35714285714285715\n",
      "LOSS train 1.4950593823194505 valid 1.8701831102371216\n",
      "EPOCH 61:\n",
      "  batch 1000 loss: 1.4928248279094696\n",
      "Training Accuracy for epoch 60:  0.725258493353028\n",
      "Validation Accuracy for epoch 60:  0.3620689655172414\n",
      "LOSS train 1.4928248279094696 valid 1.8697373867034912\n",
      "EPOCH 62:\n",
      "  batch 1000 loss: 1.4906169008016585\n",
      "Training Accuracy for epoch 61:  0.7274741506646972\n",
      "Validation Accuracy for epoch 61:  0.3657635467980296\n",
      "LOSS train 1.4906169008016585 valid 1.8692998886108398\n",
      "EPOCH 63:\n",
      "  batch 1000 loss: 1.488429888486862\n",
      "Training Accuracy for epoch 62:  0.7304283604135894\n",
      "Validation Accuracy for epoch 62:  0.3645320197044335\n",
      "LOSS train 1.488429888486862 valid 1.8688733577728271\n",
      "EPOCH 64:\n",
      "  batch 1000 loss: 1.4862586977481842\n",
      "Training Accuracy for epoch 63:  0.7326440177252584\n",
      "Validation Accuracy for epoch 63:  0.3633004926108374\n",
      "LOSS train 1.4862586977481842 valid 1.868456482887268\n",
      "EPOCH 65:\n",
      "  batch 1000 loss: 1.484098897576332\n",
      "Training Accuracy for epoch 64:  0.740029542097489\n",
      "Validation Accuracy for epoch 64:  0.3633004926108374\n",
      "LOSS train 1.484098897576332 valid 1.8680518865585327\n",
      "EPOCH 66:\n",
      "  batch 1000 loss: 1.4819471006393432\n",
      "Training Accuracy for epoch 65:  0.741506646971935\n",
      "Validation Accuracy for epoch 65:  0.3657635467980296\n",
      "LOSS train 1.4819471006393432 valid 1.8676605224609375\n",
      "EPOCH 67:\n",
      "  batch 1000 loss: 1.4798009532690048\n",
      "Training Accuracy for epoch 66:  0.741506646971935\n",
      "Validation Accuracy for epoch 66:  0.3669950738916256\n",
      "LOSS train 1.4798009532690048 valid 1.8672821521759033\n",
      "EPOCH 68:\n",
      "  batch 1000 loss: 1.4776594686508178\n",
      "Training Accuracy for epoch 67:  0.7451994091580503\n",
      "Validation Accuracy for epoch 67:  0.3694581280788177\n",
      "LOSS train 1.4776594686508178 valid 1.8669168949127197\n",
      "EPOCH 69:\n",
      "  batch 1000 loss: 1.4755231466293335\n",
      "Training Accuracy for epoch 68:  0.7481536189069424\n",
      "Validation Accuracy for epoch 68:  0.3694581280788177\n",
      "LOSS train 1.4755231466293335 valid 1.8665649890899658\n",
      "EPOCH 70:\n",
      "  batch 1000 loss: 1.473394053220749\n",
      "Training Accuracy for epoch 69:  0.7533234859675036\n",
      "Validation Accuracy for epoch 69:  0.3706896551724138\n",
      "LOSS train 1.473394053220749 valid 1.8662266731262207\n",
      "EPOCH 71:\n",
      "  batch 1000 loss: 1.4712758840322495\n",
      "Training Accuracy for epoch 70:  0.7548005908419497\n",
      "Validation Accuracy for epoch 70:  0.37192118226600984\n",
      "LOSS train 1.4712758840322495 valid 1.8659006357192993\n",
      "EPOCH 72:\n",
      "  batch 1000 loss: 1.4691737245321275\n",
      "Training Accuracy for epoch 71:  0.757754800590842\n",
      "Validation Accuracy for epoch 71:  0.37192118226600984\n",
      "LOSS train 1.4691737245321275 valid 1.865587830543518\n",
      "EPOCH 73:\n",
      "  batch 1000 loss: 1.467093838095665\n",
      "Training Accuracy for epoch 72:  0.758493353028065\n",
      "Validation Accuracy for epoch 72:  0.3706896551724138\n",
      "LOSS train 1.467093838095665 valid 1.865286946296692\n",
      "EPOCH 74:\n",
      "  batch 1000 loss: 1.4650429971218109\n",
      "Training Accuracy for epoch 73:  0.7621861152141802\n",
      "Validation Accuracy for epoch 73:  0.37438423645320196\n",
      "LOSS train 1.4650429971218109 valid 1.8649938106536865\n",
      "EPOCH 75:\n",
      "  batch 1000 loss: 1.4630280386209489\n",
      "Training Accuracy for epoch 74:  0.7614475627769571\n",
      "Validation Accuracy for epoch 74:  0.37561576354679804\n",
      "LOSS train 1.4630280386209489 valid 1.8647081851959229\n",
      "EPOCH 76:\n",
      "  batch 1000 loss: 1.4610551469326019\n",
      "Training Accuracy for epoch 75:  0.7621861152141802\n",
      "Validation Accuracy for epoch 75:  0.3768472906403941\n",
      "LOSS train 1.4610551469326019 valid 1.8644262552261353\n",
      "EPOCH 77:\n",
      "  batch 1000 loss: 1.4591294345855712\n",
      "Training Accuracy for epoch 76:  0.7629246676514032\n",
      "Validation Accuracy for epoch 76:  0.3793103448275862\n",
      "LOSS train 1.4591294345855712 valid 1.864150047302246\n",
      "EPOCH 78:\n",
      "  batch 1000 loss: 1.4572545057535171\n",
      "Training Accuracy for epoch 77:  0.7644017725258493\n",
      "Validation Accuracy for epoch 77:  0.3805418719211823\n",
      "LOSS train 1.4572545057535171 valid 1.863871455192566\n",
      "EPOCH 79:\n",
      "  batch 1000 loss: 1.4554324530363083\n",
      "Training Accuracy for epoch 78:  0.7658788774002954\n",
      "Validation Accuracy for epoch 78:  0.3817733990147783\n",
      "LOSS train 1.4554324530363083 valid 1.863596796989441\n",
      "EPOCH 80:\n",
      "  batch 1000 loss: 1.4536638391017913\n",
      "Training Accuracy for epoch 79:  0.7666174298375185\n",
      "Validation Accuracy for epoch 79:  0.3830049261083744\n",
      "LOSS train 1.4536638391017913 valid 1.8633170127868652\n",
      "EPOCH 81:\n",
      "  batch 1000 loss: 1.4519479391574859\n",
      "Training Accuracy for epoch 80:  0.7673559822747416\n",
      "Validation Accuracy for epoch 80:  0.3830049261083744\n",
      "LOSS train 1.4519479391574859 valid 1.8630348443984985\n",
      "EPOCH 82:\n",
      "  batch 1000 loss: 1.4502830238342286\n",
      "Training Accuracy for epoch 81:  0.7688330871491876\n",
      "Validation Accuracy for epoch 81:  0.3854679802955665\n",
      "LOSS train 1.4502830238342286 valid 1.8627474308013916\n",
      "EPOCH 83:\n",
      "  batch 1000 loss: 1.4486666921377183\n",
      "Training Accuracy for epoch 82:  0.7703101920236337\n",
      "Validation Accuracy for epoch 82:  0.3866995073891626\n",
      "LOSS train 1.4486666921377183 valid 1.8624578714370728\n",
      "EPOCH 84:\n",
      "  batch 1000 loss: 1.4470960818529128\n",
      "Training Accuracy for epoch 83:  0.7725258493353028\n",
      "Validation Accuracy for epoch 83:  0.3879310344827586\n",
      "LOSS train 1.4470960818529128 valid 1.8621629476547241\n",
      "EPOCH 85:\n",
      "  batch 1000 loss: 1.4455681231021882\n",
      "Training Accuracy for epoch 84:  0.7725258493353028\n",
      "Validation Accuracy for epoch 84:  0.3891625615763547\n",
      "LOSS train 1.4455681231021882 valid 1.861863613128662\n",
      "EPOCH 86:\n",
      "  batch 1000 loss: 1.444079732656479\n",
      "Training Accuracy for epoch 85:  0.7732644017725259\n",
      "Validation Accuracy for epoch 85:  0.3891625615763547\n",
      "LOSS train 1.444079732656479 valid 1.8615609407424927\n",
      "EPOCH 87:\n",
      "  batch 1000 loss: 1.4426278961896897\n",
      "Training Accuracy for epoch 86:  0.776218611521418\n",
      "Validation Accuracy for epoch 86:  0.3916256157635468\n",
      "LOSS train 1.4426278961896897 valid 1.8612524271011353\n",
      "EPOCH 88:\n",
      "  batch 1000 loss: 1.4412098203897477\n",
      "Training Accuracy for epoch 87:  0.7776957163958641\n",
      "Validation Accuracy for epoch 87:  0.39285714285714285\n",
      "LOSS train 1.4412098203897477 valid 1.8609411716461182\n",
      "EPOCH 89:\n",
      "  batch 1000 loss: 1.4398228789567948\n",
      "Training Accuracy for epoch 88:  0.7791728212703102\n",
      "Validation Accuracy for epoch 88:  0.39532019704433496\n",
      "LOSS train 1.4398228789567948 valid 1.8606284856796265\n",
      "EPOCH 90:\n",
      "  batch 1000 loss: 1.4384647195339202\n",
      "Training Accuracy for epoch 89:  0.7813884785819794\n",
      "Validation Accuracy for epoch 89:  0.39655172413793105\n",
      "LOSS train 1.4384647195339202 valid 1.8603101968765259\n",
      "EPOCH 91:\n",
      "  batch 1000 loss: 1.4371332346200942\n",
      "Training Accuracy for epoch 90:  0.7836041358936484\n",
      "Validation Accuracy for epoch 90:  0.39655172413793105\n",
      "LOSS train 1.4371332346200942 valid 1.859989881515503\n",
      "EPOCH 92:\n",
      "  batch 1000 loss: 1.4358265606164933\n",
      "Training Accuracy for epoch 91:  0.7850812407680945\n",
      "Validation Accuracy for epoch 91:  0.39655172413793105\n",
      "LOSS train 1.4358265606164933 valid 1.8596677780151367\n",
      "EPOCH 93:\n",
      "  batch 1000 loss: 1.4345430685281753\n",
      "Training Accuracy for epoch 92:  0.7858197932053176\n",
      "Validation Accuracy for epoch 92:  0.39655172413793105\n",
      "LOSS train 1.4345430685281753 valid 1.859342336654663\n",
      "EPOCH 94:\n",
      "  batch 1000 loss: 1.4332813801765443\n",
      "Training Accuracy for epoch 93:  0.7865583456425406\n",
      "Validation Accuracy for epoch 93:  0.39655172413793105\n",
      "LOSS train 1.4332813801765443 valid 1.8590188026428223\n",
      "EPOCH 95:\n",
      "  batch 1000 loss: 1.4320402537584305\n",
      "Training Accuracy for epoch 94:  0.7872968980797637\n",
      "Validation Accuracy for epoch 94:  0.3977832512315271\n",
      "LOSS train 1.4320402537584305 valid 1.858693242073059\n",
      "EPOCH 96:\n",
      "  batch 1000 loss: 1.4308186526298523\n",
      "Training Accuracy for epoch 95:  0.7880354505169868\n",
      "Validation Accuracy for epoch 95:  0.3977832512315271\n",
      "LOSS train 1.4308186526298523 valid 1.8583670854568481\n",
      "EPOCH 97:\n",
      "  batch 1000 loss: 1.4296156879663466\n",
      "Training Accuracy for epoch 96:  0.7887740029542097\n",
      "Validation Accuracy for epoch 96:  0.4002463054187192\n",
      "LOSS train 1.4296156879663466 valid 1.858039379119873\n",
      "EPOCH 98:\n",
      "  batch 1000 loss: 1.4284305970668794\n",
      "Training Accuracy for epoch 97:  0.7909896602658789\n",
      "Validation Accuracy for epoch 97:  0.4014778325123153\n",
      "LOSS train 1.4284305970668794 valid 1.8577117919921875\n",
      "EPOCH 99:\n",
      "  batch 1000 loss: 1.4272626945972442\n",
      "Training Accuracy for epoch 98:  0.7917282127031019\n",
      "Validation Accuracy for epoch 98:  0.4027093596059113\n",
      "LOSS train 1.4272626945972442 valid 1.8573821783065796\n",
      "EPOCH 100:\n",
      "  batch 1000 loss: 1.426111394405365\n",
      "Training Accuracy for epoch 99:  0.793205317577548\n",
      "Validation Accuracy for epoch 99:  0.4039408866995074\n",
      "LOSS train 1.426111394405365 valid 1.8570557832717896\n",
      "EPOCH 101:\n",
      "  batch 1000 loss: 1.424976176738739\n",
      "Training Accuracy for epoch 100:  0.7939438700147711\n",
      "Validation Accuracy for epoch 100:  0.4051724137931034\n",
      "LOSS train 1.424976176738739 valid 1.856726884841919\n",
      "EPOCH 102:\n",
      "  batch 1000 loss: 1.4238565294742584\n",
      "Training Accuracy for epoch 101:  0.7939438700147711\n",
      "Validation Accuracy for epoch 101:  0.4051724137931034\n",
      "LOSS train 1.4238565294742584 valid 1.8564000129699707\n",
      "EPOCH 103:\n",
      "  batch 1000 loss: 1.4227520076036453\n",
      "Training Accuracy for epoch 102:  0.794682422451994\n",
      "Validation Accuracy for epoch 102:  0.4051724137931034\n",
      "LOSS train 1.4227520076036453 valid 1.856072187423706\n",
      "EPOCH 104:\n",
      "  batch 1000 loss: 1.4216621664762497\n",
      "Training Accuracy for epoch 103:  0.7961595273264401\n",
      "Validation Accuracy for epoch 103:  0.4064039408866995\n",
      "LOSS train 1.4216621664762497 valid 1.8557454347610474\n",
      "EPOCH 105:\n",
      "  batch 1000 loss: 1.4205865794420243\n",
      "Training Accuracy for epoch 104:  0.7968980797636632\n",
      "Validation Accuracy for epoch 104:  0.40763546798029554\n",
      "LOSS train 1.4205865794420243 valid 1.8554164171218872\n",
      "EPOCH 106:\n",
      "  batch 1000 loss: 1.4195248186588287\n",
      "Training Accuracy for epoch 105:  0.7983751846381093\n",
      "Validation Accuracy for epoch 105:  0.4088669950738916\n",
      "LOSS train 1.4195248186588287 valid 1.8550904989242554\n",
      "EPOCH 107:\n",
      "  batch 1000 loss: 1.4184764566421508\n",
      "Training Accuracy for epoch 106:  0.7991137370753324\n",
      "Validation Accuracy for epoch 106:  0.4088669950738916\n",
      "LOSS train 1.4184764566421508 valid 1.8547616004943848\n",
      "EPOCH 108:\n",
      "  batch 1000 loss: 1.4174410425424575\n",
      "Training Accuracy for epoch 107:  0.7991137370753324\n",
      "Validation Accuracy for epoch 107:  0.4088669950738916\n",
      "LOSS train 1.4174410425424575 valid 1.8544343709945679\n",
      "EPOCH 109:\n",
      "  batch 1000 loss: 1.4164181510210037\n",
      "Training Accuracy for epoch 108:  0.7991137370753324\n",
      "Validation Accuracy for epoch 108:  0.4100985221674877\n",
      "LOSS train 1.4164181510210037 valid 1.8541063070297241\n",
      "EPOCH 110:\n",
      "  batch 1000 loss: 1.41540731215477\n",
      "Training Accuracy for epoch 109:  0.8005908419497785\n",
      "Validation Accuracy for epoch 109:  0.4100985221674877\n",
      "LOSS train 1.41540731215477 valid 1.85378098487854\n",
      "EPOCH 111:\n",
      "  batch 1000 loss: 1.4144080587625503\n",
      "Training Accuracy for epoch 110:  0.8013293943870015\n",
      "Validation Accuracy for epoch 110:  0.4100985221674877\n",
      "LOSS train 1.4144080587625503 valid 1.8534519672393799\n",
      "EPOCH 112:\n",
      "  batch 1000 loss: 1.413419909477234\n",
      "Training Accuracy for epoch 111:  0.8020679468242246\n",
      "Validation Accuracy for epoch 111:  0.4100985221674877\n",
      "LOSS train 1.413419909477234 valid 1.8531237840652466\n",
      "EPOCH 113:\n",
      "  batch 1000 loss: 1.412442326068878\n",
      "Training Accuracy for epoch 112:  0.8035450516986706\n",
      "Validation Accuracy for epoch 112:  0.41133004926108374\n",
      "LOSS train 1.412442326068878 valid 1.852797508239746\n",
      "EPOCH 114:\n",
      "  batch 1000 loss: 1.4114747661352158\n",
      "Training Accuracy for epoch 113:  0.8050221565731167\n",
      "Validation Accuracy for epoch 113:  0.4100985221674877\n",
      "LOSS train 1.4114747661352158 valid 1.8524689674377441\n",
      "EPOCH 115:\n",
      "  batch 1000 loss: 1.4105166726112366\n",
      "Training Accuracy for epoch 114:  0.8094534711964549\n",
      "Validation Accuracy for epoch 114:  0.4125615763546798\n",
      "LOSS train 1.4105166726112366 valid 1.8521393537521362\n",
      "EPOCH 116:\n",
      "  batch 1000 loss: 1.409567434310913\n",
      "Training Accuracy for epoch 115:  0.8116691285081241\n",
      "Validation Accuracy for epoch 115:  0.41133004926108374\n",
      "LOSS train 1.409567434310913 valid 1.8518118858337402\n",
      "EPOCH 117:\n",
      "  batch 1000 loss: 1.408626421689987\n",
      "Training Accuracy for epoch 116:  0.8131462333825702\n",
      "Validation Accuracy for epoch 116:  0.4125615763546798\n",
      "LOSS train 1.408626421689987 valid 1.8514796495437622\n",
      "EPOCH 118:\n",
      "  batch 1000 loss: 1.4076930122375488\n",
      "Training Accuracy for epoch 117:  0.8146233382570163\n",
      "Validation Accuracy for epoch 117:  0.4125615763546798\n",
      "LOSS train 1.4076930122375488 valid 1.851150393486023\n",
      "EPOCH 119:\n",
      "  batch 1000 loss: 1.4067665821313857\n",
      "Training Accuracy for epoch 118:  0.8153618906942393\n",
      "Validation Accuracy for epoch 118:  0.41379310344827586\n",
      "LOSS train 1.4067665821313857 valid 1.8508198261260986\n",
      "EPOCH 120:\n",
      "  batch 1000 loss: 1.4058465886116027\n",
      "Training Accuracy for epoch 119:  0.8183161004431314\n",
      "Validation Accuracy for epoch 119:  0.41502463054187194\n",
      "LOSS train 1.4058465886116027 valid 1.8504842519760132\n",
      "EPOCH 121:\n",
      "  batch 1000 loss: 1.4049325535297394\n",
      "Training Accuracy for epoch 120:  0.8190546528803545\n",
      "Validation Accuracy for epoch 120:  0.41502463054187194\n",
      "LOSS train 1.4049325535297394 valid 1.8501509428024292\n",
      "EPOCH 122:\n",
      "  batch 1000 loss: 1.4040241981744765\n",
      "Training Accuracy for epoch 121:  0.8197932053175776\n",
      "Validation Accuracy for epoch 121:  0.41502463054187194\n",
      "LOSS train 1.4040241981744765 valid 1.8498133420944214\n",
      "EPOCH 123:\n",
      "  batch 1000 loss: 1.4031214768886566\n",
      "Training Accuracy for epoch 122:  0.8205317577548006\n",
      "Validation Accuracy for epoch 122:  0.41502463054187194\n",
      "LOSS train 1.4031214768886566 valid 1.8494728803634644\n",
      "EPOCH 124:\n",
      "  batch 1000 loss: 1.4022246301174164\n",
      "Training Accuracy for epoch 123:  0.8212703101920237\n",
      "Validation Accuracy for epoch 123:  0.41625615763546797\n",
      "LOSS train 1.4022246301174164 valid 1.8491321802139282\n",
      "EPOCH 125:\n",
      "  batch 1000 loss: 1.4013341572284699\n",
      "Training Accuracy for epoch 124:  0.8220088626292467\n",
      "Validation Accuracy for epoch 124:  0.41748768472906406\n",
      "LOSS train 1.4013341572284699 valid 1.8487865924835205\n",
      "EPOCH 126:\n",
      "  batch 1000 loss: 1.4004507845640182\n",
      "Training Accuracy for epoch 125:  0.8220088626292467\n",
      "Validation Accuracy for epoch 125:  0.41995073891625617\n",
      "LOSS train 1.4004507845640182 valid 1.848439335823059\n",
      "EPOCH 127:\n",
      "  batch 1000 loss: 1.3995752764940261\n",
      "Training Accuracy for epoch 126:  0.8227474150664698\n",
      "Validation Accuracy for epoch 126:  0.41995073891625617\n",
      "LOSS train 1.3995752764940261 valid 1.8480874300003052\n",
      "EPOCH 128:\n",
      "  batch 1000 loss: 1.3987083595991134\n",
      "Training Accuracy for epoch 127:  0.8242245199409158\n",
      "Validation Accuracy for epoch 127:  0.4224137931034483\n",
      "LOSS train 1.3987083595991134 valid 1.8477331399917603\n",
      "EPOCH 129:\n",
      "  batch 1000 loss: 1.3978505084514619\n",
      "Training Accuracy for epoch 128:  0.8242245199409158\n",
      "Validation Accuracy for epoch 128:  0.4224137931034483\n",
      "LOSS train 1.3978505084514619 valid 1.847375750541687\n",
      "EPOCH 130:\n",
      "  batch 1000 loss: 1.3970019507408142\n",
      "Training Accuracy for epoch 129:  0.8242245199409158\n",
      "Validation Accuracy for epoch 129:  0.42610837438423643\n",
      "LOSS train 1.3970019507408142 valid 1.8470147848129272\n",
      "EPOCH 131:\n",
      "  batch 1000 loss: 1.3961626549959183\n",
      "Training Accuracy for epoch 130:  0.8249630723781388\n",
      "Validation Accuracy for epoch 130:  0.42610837438423643\n",
      "LOSS train 1.3961626549959183 valid 1.8466520309448242\n",
      "EPOCH 132:\n",
      "  batch 1000 loss: 1.3953323594331741\n",
      "Training Accuracy for epoch 131:  0.8249630723781388\n",
      "Validation Accuracy for epoch 131:  0.42610837438423643\n",
      "LOSS train 1.3953323594331741 valid 1.846286416053772\n",
      "EPOCH 133:\n",
      "  batch 1000 loss: 1.3945107151269913\n",
      "Training Accuracy for epoch 132:  0.8257016248153619\n",
      "Validation Accuracy for epoch 132:  0.4248768472906404\n",
      "LOSS train 1.3945107151269913 valid 1.84591805934906\n",
      "EPOCH 134:\n",
      "  batch 1000 loss: 1.3936972445249558\n",
      "Training Accuracy for epoch 133:  0.827178729689808\n",
      "Validation Accuracy for epoch 133:  0.4248768472906404\n",
      "LOSS train 1.3936972445249558 valid 1.8455471992492676\n",
      "EPOCH 135:\n",
      "  batch 1000 loss: 1.3928914787769318\n",
      "Training Accuracy for epoch 134:  0.827917282127031\n",
      "Validation Accuracy for epoch 134:  0.42610837438423643\n",
      "LOSS train 1.3928914787769318 valid 1.8451752662658691\n",
      "EPOCH 136:\n",
      "  batch 1000 loss: 1.3920929518938066\n",
      "Training Accuracy for epoch 135:  0.827917282127031\n",
      "Validation Accuracy for epoch 135:  0.4273399014778325\n",
      "LOSS train 1.3920929518938066 valid 1.8448015451431274\n",
      "EPOCH 137:\n",
      "  batch 1000 loss: 1.391301259994507\n",
      "Training Accuracy for epoch 136:  0.829394387001477\n",
      "Validation Accuracy for epoch 136:  0.4273399014778325\n",
      "LOSS train 1.391301259994507 valid 1.8444262742996216\n",
      "EPOCH 138:\n",
      "  batch 1000 loss: 1.390516011953354\n",
      "Training Accuracy for epoch 137:  0.829394387001477\n",
      "Validation Accuracy for epoch 137:  0.4273399014778325\n",
      "LOSS train 1.390516011953354 valid 1.8440495729446411\n",
      "EPOCH 139:\n",
      "  batch 1000 loss: 1.3897368861436843\n",
      "Training Accuracy for epoch 138:  0.8301329394387001\n",
      "Validation Accuracy for epoch 138:  0.4273399014778325\n",
      "LOSS train 1.3897368861436843 valid 1.8436682224273682\n",
      "EPOCH 140:\n",
      "  batch 1000 loss: 1.3889635943174363\n",
      "Training Accuracy for epoch 139:  0.8308714918759232\n",
      "Validation Accuracy for epoch 139:  0.42610837438423643\n",
      "LOSS train 1.3889635943174363 valid 1.8432860374450684\n",
      "EPOCH 141:\n",
      "  batch 1000 loss: 1.3881958904266358\n",
      "Training Accuracy for epoch 140:  0.8316100443131462\n",
      "Validation Accuracy for epoch 140:  0.42610837438423643\n",
      "LOSS train 1.3881958904266358 valid 1.842903733253479\n",
      "EPOCH 142:\n",
      "  batch 1000 loss: 1.3874335551261903\n",
      "Training Accuracy for epoch 141:  0.8316100443131462\n",
      "Validation Accuracy for epoch 141:  0.42610837438423643\n",
      "LOSS train 1.3874335551261903 valid 1.8425171375274658\n",
      "EPOCH 143:\n",
      "  batch 1000 loss: 1.3866763809919358\n",
      "Training Accuracy for epoch 142:  0.8316100443131462\n",
      "Validation Accuracy for epoch 142:  0.42610837438423643\n",
      "LOSS train 1.3866763809919358 valid 1.8421279191970825\n",
      "EPOCH 144:\n",
      "  batch 1000 loss: 1.3859242167472838\n",
      "Training Accuracy for epoch 143:  0.8316100443131462\n",
      "Validation Accuracy for epoch 143:  0.42610837438423643\n",
      "LOSS train 1.3859242167472838 valid 1.8417378664016724\n",
      "EPOCH 145:\n",
      "  batch 1000 loss: 1.3851768944263458\n",
      "Training Accuracy for epoch 144:  0.8316100443131462\n",
      "Validation Accuracy for epoch 144:  0.42610837438423643\n",
      "LOSS train 1.3851768944263458 valid 1.841344952583313\n",
      "EPOCH 146:\n",
      "  batch 1000 loss: 1.3844342353343964\n",
      "Training Accuracy for epoch 145:  0.8323485967503693\n",
      "Validation Accuracy for epoch 145:  0.42610837438423643\n",
      "LOSS train 1.3844342353343964 valid 1.8409507274627686\n",
      "EPOCH 147:\n",
      "  batch 1000 loss: 1.3836961147785187\n",
      "Training Accuracy for epoch 146:  0.8353028064992615\n",
      "Validation Accuracy for epoch 146:  0.42610837438423643\n",
      "LOSS train 1.3836961147785187 valid 1.840552806854248\n",
      "EPOCH 148:\n",
      "  batch 1000 loss: 1.38296235704422\n",
      "Training Accuracy for epoch 147:  0.8360413589364845\n",
      "Validation Accuracy for epoch 147:  0.4273399014778325\n",
      "LOSS train 1.38296235704422 valid 1.8401525020599365\n",
      "EPOCH 149:\n",
      "  batch 1000 loss: 1.3822328104972839\n",
      "Training Accuracy for epoch 148:  0.8382570162481536\n",
      "Validation Accuracy for epoch 148:  0.4273399014778325\n",
      "LOSS train 1.3822328104972839 valid 1.8397504091262817\n",
      "EPOCH 150:\n",
      "  batch 1000 loss: 1.381507295012474\n",
      "Training Accuracy for epoch 149:  0.8382570162481536\n",
      "Validation Accuracy for epoch 149:  0.42857142857142855\n",
      "LOSS train 1.381507295012474 valid 1.8393441438674927\n",
      "EPOCH 151:\n",
      "  batch 1000 loss: 1.380785647392273\n",
      "Training Accuracy for epoch 150:  0.8389955686853766\n",
      "Validation Accuracy for epoch 150:  0.42857142857142855\n",
      "LOSS train 1.380785647392273 valid 1.83893620967865\n",
      "EPOCH 152:\n",
      "  batch 1000 loss: 1.38006766474247\n",
      "Training Accuracy for epoch 151:  0.8389955686853766\n",
      "Validation Accuracy for epoch 151:  0.42857142857142855\n",
      "LOSS train 1.38006766474247 valid 1.8385266065597534\n",
      "EPOCH 153:\n",
      "  batch 1000 loss: 1.3793531495332718\n",
      "Training Accuracy for epoch 152:  0.8389955686853766\n",
      "Validation Accuracy for epoch 152:  0.42857142857142855\n",
      "LOSS train 1.3793531495332718 valid 1.8381116390228271\n",
      "EPOCH 154:\n",
      "  batch 1000 loss: 1.3786418914794922\n",
      "Training Accuracy for epoch 153:  0.8397341211225997\n",
      "Validation Accuracy for epoch 153:  0.42857142857142855\n",
      "LOSS train 1.3786418914794922 valid 1.8376942873001099\n",
      "EPOCH 155:\n",
      "  batch 1000 loss: 1.3779336900711059\n",
      "Training Accuracy for epoch 154:  0.8412112259970458\n",
      "Validation Accuracy for epoch 154:  0.42857142857142855\n",
      "LOSS train 1.3779336900711059 valid 1.837274193763733\n",
      "EPOCH 156:\n",
      "  batch 1000 loss: 1.3772282935380935\n",
      "Training Accuracy for epoch 155:  0.8412112259970458\n",
      "Validation Accuracy for epoch 155:  0.42857142857142855\n",
      "LOSS train 1.3772282935380935 valid 1.836850643157959\n",
      "EPOCH 157:\n",
      "  batch 1000 loss: 1.3765254992246627\n",
      "Training Accuracy for epoch 156:  0.8426883308714919\n",
      "Validation Accuracy for epoch 156:  0.42857142857142855\n",
      "LOSS train 1.3765254992246627 valid 1.836423635482788\n",
      "EPOCH 158:\n",
      "  batch 1000 loss: 1.3758250963687897\n",
      "Training Accuracy for epoch 157:  0.844903988183161\n",
      "Validation Accuracy for epoch 157:  0.42980295566502463\n",
      "LOSS train 1.3758250963687897 valid 1.8359932899475098\n",
      "EPOCH 159:\n",
      "  batch 1000 loss: 1.3751268594264985\n",
      "Training Accuracy for epoch 158:  0.844903988183161\n",
      "Validation Accuracy for epoch 158:  0.42980295566502463\n",
      "LOSS train 1.3751268594264985 valid 1.8355575799942017\n",
      "EPOCH 160:\n",
      "  batch 1000 loss: 1.3744305835962296\n",
      "Training Accuracy for epoch 159:  0.8471196454948301\n",
      "Validation Accuracy for epoch 159:  0.43226600985221675\n",
      "LOSS train 1.3744305835962296 valid 1.8351190090179443\n",
      "EPOCH 161:\n",
      "  batch 1000 loss: 1.3737360898256301\n",
      "Training Accuracy for epoch 160:  0.8471196454948301\n",
      "Validation Accuracy for epoch 160:  0.43226600985221675\n",
      "LOSS train 1.3737360898256301 valid 1.8346744775772095\n",
      "EPOCH 162:\n",
      "  batch 1000 loss: 1.3730431777238845\n",
      "Training Accuracy for epoch 161:  0.8478581979320532\n",
      "Validation Accuracy for epoch 161:  0.43349753694581283\n",
      "LOSS train 1.3730431777238845 valid 1.8342270851135254\n",
      "EPOCH 163:\n",
      "  batch 1000 loss: 1.372351702928543\n",
      "Training Accuracy for epoch 162:  0.8478581979320532\n",
      "Validation Accuracy for epoch 162:  0.43349753694581283\n",
      "LOSS train 1.372351702928543 valid 1.8337748050689697\n",
      "EPOCH 164:\n",
      "  batch 1000 loss: 1.3716615425348282\n",
      "Training Accuracy for epoch 163:  0.8500738552437223\n",
      "Validation Accuracy for epoch 163:  0.43349753694581283\n",
      "LOSS train 1.3716615425348282 valid 1.8333172798156738\n",
      "EPOCH 165:\n",
      "  batch 1000 loss: 1.370972584247589\n",
      "Training Accuracy for epoch 164:  0.8508124076809453\n",
      "Validation Accuracy for epoch 164:  0.43349753694581283\n",
      "LOSS train 1.370972584247589 valid 1.832851529121399\n",
      "EPOCH 166:\n",
      "  batch 1000 loss: 1.370284716129303\n",
      "Training Accuracy for epoch 165:  0.8508124076809453\n",
      "Validation Accuracy for epoch 165:  0.43349753694581283\n",
      "LOSS train 1.370284716129303 valid 1.8323814868927002\n",
      "EPOCH 167:\n",
      "  batch 1000 loss: 1.3695979145765305\n",
      "Training Accuracy for epoch 166:  0.8545051698670606\n",
      "Validation Accuracy for epoch 166:  0.43472906403940886\n",
      "LOSS train 1.3695979145765305 valid 1.8319065570831299\n",
      "EPOCH 168:\n",
      "  batch 1000 loss: 1.36891213285923\n",
      "Training Accuracy for epoch 167:  0.8552437223042836\n",
      "Validation Accuracy for epoch 167:  0.43472906403940886\n",
      "LOSS train 1.36891213285923 valid 1.8314250707626343\n",
      "EPOCH 169:\n",
      "  batch 1000 loss: 1.3682273468971253\n",
      "Training Accuracy for epoch 168:  0.8559822747415067\n",
      "Validation Accuracy for epoch 168:  0.43472906403940886\n",
      "LOSS train 1.3682273468971253 valid 1.8309379816055298\n",
      "EPOCH 170:\n",
      "  batch 1000 loss: 1.367543587565422\n",
      "Training Accuracy for epoch 169:  0.8574593796159528\n",
      "Validation Accuracy for epoch 169:  0.43472906403940886\n",
      "LOSS train 1.367543587565422 valid 1.8304431438446045\n",
      "EPOCH 171:\n",
      "  batch 1000 loss: 1.3668608392477035\n",
      "Training Accuracy for epoch 170:  0.8581979320531757\n",
      "Validation Accuracy for epoch 170:  0.43472906403940886\n",
      "LOSS train 1.3668608392477035 valid 1.8299424648284912\n",
      "EPOCH 172:\n",
      "  batch 1000 loss: 1.3661791616678238\n",
      "Training Accuracy for epoch 171:  0.8589364844903988\n",
      "Validation Accuracy for epoch 171:  0.43596059113300495\n",
      "LOSS train 1.3661791616678238 valid 1.8294364213943481\n",
      "EPOCH 173:\n",
      "  batch 1000 loss: 1.3654985655546188\n",
      "Training Accuracy for epoch 172:  0.8596750369276218\n",
      "Validation Accuracy for epoch 172:  0.43596059113300495\n",
      "LOSS train 1.3654985655546188 valid 1.8289220333099365\n",
      "EPOCH 174:\n",
      "  batch 1000 loss: 1.3648191124200821\n",
      "Training Accuracy for epoch 173:  0.8604135893648449\n",
      "Validation Accuracy for epoch 173:  0.43596059113300495\n",
      "LOSS train 1.3648191124200821 valid 1.8284037113189697\n",
      "EPOCH 175:\n",
      "  batch 1000 loss: 1.36414082634449\n",
      "Training Accuracy for epoch 174:  0.8611521418020679\n",
      "Validation Accuracy for epoch 174:  0.437192118226601\n",
      "LOSS train 1.36414082634449 valid 1.8278778791427612\n",
      "EPOCH 176:\n",
      "  batch 1000 loss: 1.363463742852211\n",
      "Training Accuracy for epoch 175:  0.8626292466765141\n",
      "Validation Accuracy for epoch 175:  0.437192118226601\n",
      "LOSS train 1.363463742852211 valid 1.8273478746414185\n",
      "EPOCH 177:\n",
      "  batch 1000 loss: 1.3627878679037093\n",
      "Training Accuracy for epoch 176:  0.8641063515509602\n",
      "Validation Accuracy for epoch 176:  0.43842364532019706\n",
      "LOSS train 1.3627878679037093 valid 1.8268115520477295\n",
      "EPOCH 178:\n",
      "  batch 1000 loss: 1.3621132276058197\n",
      "Training Accuracy for epoch 177:  0.8641063515509602\n",
      "Validation Accuracy for epoch 177:  0.43842364532019706\n",
      "LOSS train 1.3621132276058197 valid 1.8262699842453003\n",
      "EPOCH 179:\n",
      "  batch 1000 loss: 1.3614398174285889\n",
      "Training Accuracy for epoch 178:  0.8655834564254062\n",
      "Validation Accuracy for epoch 178:  0.43842364532019706\n",
      "LOSS train 1.3614398174285889 valid 1.825723648071289\n",
      "EPOCH 180:\n",
      "  batch 1000 loss: 1.360767632007599\n",
      "Training Accuracy for epoch 179:  0.8655834564254062\n",
      "Validation Accuracy for epoch 179:  0.4396551724137931\n",
      "LOSS train 1.360767632007599 valid 1.8251765966415405\n",
      "EPOCH 181:\n",
      "  batch 1000 loss: 1.3600966373682022\n",
      "Training Accuracy for epoch 180:  0.8677991137370753\n",
      "Validation Accuracy for epoch 180:  0.4408866995073892\n",
      "LOSS train 1.3600966373682022 valid 1.8246253728866577\n",
      "EPOCH 182:\n",
      "  batch 1000 loss: 1.3594268312454223\n",
      "Training Accuracy for epoch 181:  0.8685376661742984\n",
      "Validation Accuracy for epoch 181:  0.4421182266009852\n",
      "LOSS train 1.3594268312454223 valid 1.8240727186203003\n",
      "EPOCH 183:\n",
      "  batch 1000 loss: 1.3587581832408906\n",
      "Training Accuracy for epoch 182:  0.8692762186115214\n",
      "Validation Accuracy for epoch 182:  0.4421182266009852\n",
      "LOSS train 1.3587581832408906 valid 1.8235180377960205\n",
      "EPOCH 184:\n",
      "  batch 1000 loss: 1.3580906864404678\n",
      "Training Accuracy for epoch 183:  0.8700147710487445\n",
      "Validation Accuracy for epoch 183:  0.4421182266009852\n",
      "LOSS train 1.3580906864404678 valid 1.8229619264602661\n",
      "EPOCH 185:\n",
      "  batch 1000 loss: 1.3574243681430818\n",
      "Training Accuracy for epoch 184:  0.8700147710487445\n",
      "Validation Accuracy for epoch 184:  0.4433497536945813\n",
      "LOSS train 1.3574243681430818 valid 1.8224095106124878\n",
      "EPOCH 186:\n",
      "  batch 1000 loss: 1.3567592706680298\n",
      "Training Accuracy for epoch 185:  0.8707533234859675\n",
      "Validation Accuracy for epoch 185:  0.4445812807881773\n",
      "LOSS train 1.3567592706680298 valid 1.8218556642532349\n",
      "EPOCH 187:\n",
      "  batch 1000 loss: 1.3560955229997635\n",
      "Training Accuracy for epoch 186:  0.8729689807976366\n",
      "Validation Accuracy for epoch 186:  0.4458128078817734\n",
      "LOSS train 1.3560955229997635 valid 1.8213034868240356\n",
      "EPOCH 188:\n",
      "  batch 1000 loss: 1.3554332860708236\n",
      "Training Accuracy for epoch 187:  0.8729689807976366\n",
      "Validation Accuracy for epoch 187:  0.4458128078817734\n",
      "LOSS train 1.3554332860708236 valid 1.8207566738128662\n",
      "EPOCH 189:\n",
      "  batch 1000 loss: 1.354772816181183\n",
      "Training Accuracy for epoch 188:  0.8729689807976366\n",
      "Validation Accuracy for epoch 188:  0.4458128078817734\n",
      "LOSS train 1.354772816181183 valid 1.8202120065689087\n",
      "EPOCH 190:\n",
      "  batch 1000 loss: 1.3541144417524338\n",
      "Training Accuracy for epoch 189:  0.8729689807976366\n",
      "Validation Accuracy for epoch 189:  0.44704433497536944\n",
      "LOSS train 1.3541144417524338 valid 1.8196710348129272\n",
      "EPOCH 191:\n",
      "  batch 1000 loss: 1.3534585584402083\n",
      "Training Accuracy for epoch 190:  0.8737075332348597\n",
      "Validation Accuracy for epoch 190:  0.4482758620689655\n",
      "LOSS train 1.3534585584402083 valid 1.8191345930099487\n",
      "EPOCH 192:\n",
      "  batch 1000 loss: 1.3528056353330613\n",
      "Training Accuracy for epoch 191:  0.8744460856720827\n",
      "Validation Accuracy for epoch 191:  0.44704433497536944\n",
      "LOSS train 1.3528056353330613 valid 1.8186030387878418\n",
      "EPOCH 193:\n",
      "  batch 1000 loss: 1.3521561385393144\n",
      "Training Accuracy for epoch 192:  0.8774002954209749\n",
      "Validation Accuracy for epoch 192:  0.4482758620689655\n",
      "LOSS train 1.3521561385393144 valid 1.8180761337280273\n",
      "EPOCH 194:\n",
      "  batch 1000 loss: 1.351510553598404\n",
      "Training Accuracy for epoch 193:  0.8788774002954209\n",
      "Validation Accuracy for epoch 193:  0.44950738916256155\n",
      "LOSS train 1.351510553598404 valid 1.8175551891326904\n",
      "EPOCH 195:\n",
      "  batch 1000 loss: 1.3508692988157271\n",
      "Training Accuracy for epoch 194:  0.879615952732644\n",
      "Validation Accuracy for epoch 194:  0.44950738916256155\n",
      "LOSS train 1.3508692988157271 valid 1.8170421123504639\n",
      "EPOCH 196:\n",
      "  batch 1000 loss: 1.3502327325344086\n",
      "Training Accuracy for epoch 195:  0.879615952732644\n",
      "Validation Accuracy for epoch 195:  0.44950738916256155\n",
      "LOSS train 1.3502327325344086 valid 1.8165326118469238\n",
      "EPOCH 197:\n",
      "  batch 1000 loss: 1.349601131439209\n",
      "Training Accuracy for epoch 196:  0.879615952732644\n",
      "Validation Accuracy for epoch 196:  0.45073891625615764\n",
      "LOSS train 1.349601131439209 valid 1.8160347938537598\n",
      "EPOCH 198:\n",
      "  batch 1000 loss: 1.3489746917486192\n",
      "Training Accuracy for epoch 197:  0.8788774002954209\n",
      "Validation Accuracy for epoch 197:  0.45073891625615764\n",
      "LOSS train 1.3489746917486192 valid 1.8155395984649658\n",
      "EPOCH 199:\n",
      "  batch 1000 loss: 1.3483534842729568\n",
      "Training Accuracy for epoch 198:  0.8788774002954209\n",
      "Validation Accuracy for epoch 198:  0.44950738916256155\n",
      "LOSS train 1.3483534842729568 valid 1.8150482177734375\n",
      "EPOCH 200:\n",
      "  batch 1000 loss: 1.3477375297546386\n",
      "Training Accuracy for epoch 199:  0.880354505169867\n",
      "Validation Accuracy for epoch 199:  0.44950738916256155\n",
      "LOSS train 1.3477375297546386 valid 1.8145668506622314\n"
     ]
    }
   ],
   "source": [
    "# define one training epoch \n",
    "def train_one_epoch(model, train_nodes, train_labels, epoch_index, tb_writer, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        epoch_index: index of the epoch \n",
    "        tb_writer: a writer object that can record all the training statistics \n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "    true_pairs = 0\n",
    "    # Work thought the entire dataset\n",
    "    for i, (inputs, labels) in enumerate(zip(train_nodes, train_labels)):\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        # print(\"Output: \", outputs)\n",
    "        # print(\"Label: \", labels)\n",
    "        loss = loss_fn(outputs.unsqueeze(0), labels)\n",
    "        loss.backward()\n",
    "        if torch.argmax(outputs) == labels: \n",
    "            true_pairs += 1\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_nodes) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    print(f\"Training Accuracy for epoch {epoch_index}: \", true_pairs/len(train_nodes))\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/FCN_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(model=model, \n",
    "        train_nodes=train_nodes, \n",
    "        train_labels=train_labels, \n",
    "        epoch_index=epoch_number, \n",
    "        tb_writer=writer, \n",
    "        loss_fn=loss, \n",
    "        optimizer=optimizer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    true_pairs = 0\n",
    "    for i, (vdata, vlabel) in enumerate(zip(validation_nodes, validation_labels)):\n",
    "        voutputs = model(vdata)\n",
    "        vloss = loss(voutputs.unsqueeze(0), vlabel)\n",
    "        running_vloss += vloss\n",
    "        if torch.argmax(voutputs) == vlabel: \n",
    "            true_pairs += 1\n",
    "\n",
    "    print(f\"Validation Accuracy for epoch {epoch}: \", true_pairs/len(validation_nodes))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'FCNs/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xueze\\AppData\\Local\\Temp\\ipykernel_23424\\959668787.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.46494464944649444\n",
      "LOSS test 1.8706679344177246 \n"
     ]
    }
   ],
   "source": [
    "# testing evaluation \n",
    "model.train(False) # turn training off for all parameters\n",
    "\n",
    "running_tloss = 0.0\n",
    "true_pairs = 0\n",
    "for i, (tdata, tlabel) in enumerate(zip(test_nodes, test_labels)):\n",
    "    toutputs = model(tdata)\n",
    "    tloss = loss(toutputs.unsqueeze(0), tlabel)\n",
    "    running_tloss += tloss\n",
    "    if torch.argmax(toutputs) == tlabel: \n",
    "        true_pairs += 1\n",
    "print(f\"Test Accuracy: \", true_pairs/len(test_nodes))\n",
    "\n",
    "\n",
    "avg_tloss = running_tloss / len(test_nodes)\n",
    "print('LOSS test {} '.format(avg_tloss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ee9abc2f8ed4c2bec59380a88532f6c4409e3142704504d9be0d180fda6aa0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
