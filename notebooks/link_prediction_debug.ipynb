{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard pacakges \n",
    "import torch \n",
    "from torch import nn, utils\n",
    "import networkx as nx \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Pytroch Geometric \n",
    "from torch_geometric import utils as gutils\n",
    "from torch_geometric import nn as gnn # import layers \n",
    "from torch_geometric.datasets import Planetoid # import dataset CORA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_cora_torch(filepath=\"../data/raw/Planetoid\", transform=None, batch_size=1, shuffle=False, device='cuda:0' if torch.cuda.is_available() else 'mps'):\n",
    "    \"\"\"Return the CORA dataset\"\"\"\n",
    "    dataset = Planetoid(root=filepath, name='Cora', split='public', num_train_per_class=20, num_val=500, num_test=1000, transform=transform) # return a class of datasets\n",
    "    data = dataset[0]\n",
    "    # print some dataset statistics \n",
    "    print(f'Loads Cora dataset, at root location: {filepath}')\n",
    "    print(f'Number of nodes: {data.num_nodes}')\n",
    "    print(f'Number of edges: {data.num_edges}')\n",
    "    print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "    print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "    print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "    print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "    print(f'Has self-loops: {data.has_self_loops()}')\n",
    "    print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "    return data.edge_index.to(device), data.x.to(device), data.y.to(device)\n",
    "\n",
    "\n",
    "# edge_index, node_features, labels = loader_cora_torch(shuffle=True) # load the cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConvCustom(nn.Module):\n",
    "    def __init__(self, \n",
    "                edge_index,\n",
    "                # node_batch,\n",
    "                input_dim, \n",
    "                output_dim, \n",
    "                scale=1, \n",
    "                random_init=True, \n",
    "                with_bias=True,\n",
    "                device='cuda:0' if torch.cuda.is_available() else 'mps'):\n",
    "        super(GCNConvCustom, self).__init__()\n",
    "        # print(\"layer initialized\")\n",
    "\n",
    "        \"\"\"Metadata\"\"\"\n",
    "        self.device = device # initialize the hosting device\n",
    "        self.with_bias = with_bias\n",
    "        self.scale = scale\n",
    "\n",
    "        \"\"\"Calculate Matrices\"\"\"\n",
    "        # the adjacency matrix with self-loop \n",
    "        \n",
    "        self.A = gutils.to_dense_adj(edge_index).to(self.device)[0]\n",
    "        self.A_self = self.A + torch.diag(torch.ones(self.A.shape[0], device=self.device))\n",
    "        # print(\"Adj Matrix with self loop: \", self.A)\n",
    "        \n",
    "        # calculate the degree matrix with A after added self loop\n",
    "        self.D = torch.sum(self.A_self, dim=0).to(self.device)  # Note: these are the elements along the diagonal of D\n",
    "        # print(\"Degree Matrix: \", self.D)\n",
    "\n",
    "        # for diagonal matrix, raising it to any power is the same as raising its diagonal elements to that power\n",
    "        # we can just apply the -1/2 power to all element of this degree matrix \n",
    "        # self.D_half_norm = torch.reciprocal(torch.sqrt(self.D)) \n",
    "        # self.D_half_norm = torch.from_numpy(fractional_matrix_power(self.D, -0.5)).to(self.device)\n",
    "        self.D_half_norm = torch.diag(torch.pow(self.D, -0.5))\n",
    "        # print(\"Normalization Matrix: \", self.D_half_norm)\n",
    "\n",
    "        # normalized adjacency matrix\n",
    "        # self.A_s = torch.mm(torch.mm(self.D_half_norm, self.A), self.D_half_norm) \n",
    "        self.A_s = self.D_half_norm @ self.A_self @ self.D_half_norm\n",
    "        self.A_s = self.A_s.to(self.device)\n",
    "        # print(self.A_s.shape)\n",
    "        \n",
    "        # initialize learnable weights\n",
    "        # the weight should have shape of (N , F) where N is the size of the input, and F is the output dimension\n",
    "        self.W, self.b = None, None\n",
    "        if random_init: \n",
    "            \n",
    "            self.W = torch.nn.Parameter(\n",
    "                data=(2 * torch.rand(input_dim, output_dim, device=self.device)-1)*self.scale,  \n",
    "                requires_grad=True\n",
    "            )\n",
    "            # create trainable a bias term for the layer\n",
    "            self.b = torch.nn.Parameter(\n",
    "                data=(2 * torch.rand(output_dim, 1, device=self.device)-1)*self.scale,\n",
    "                requires_grad=True\n",
    "            )\n",
    "        else: \n",
    "            self.W = torch.nn.Parameter(\n",
    "                data=torch.zeros(input_dim, output_dim, device=self.device), \n",
    "                requires_grad=True\n",
    "            )\n",
    "            self.b = torch.nn.Parameter(\n",
    "                data=torch.zeros(output_dim, 1, device=self.device), \n",
    "                requires_grad=True\n",
    "            )\n",
    "\n",
    "    def forward(self, H):\n",
    "        if self.with_bias: \n",
    "            return self.A_s @ (H @ self.W) + self.b.T\n",
    "        else: \n",
    "            return self.A_s @ (H @ self.W)\n",
    "\n",
    "    def get_adj_matrix(self, with_self=False):\n",
    "        if with_self: \n",
    "            return self.A_self \n",
    "        return self.A\n",
    "    \n",
    "    def get_normalized_adj_matrix(self): \n",
    "        return self.A_s\n",
    "\n",
    "    def get_degree_matrix(self, normalization=False): \n",
    "        if normalization: \n",
    "            return self.D_half_norm\n",
    "        return self.D\n",
    "\n",
    "\n",
    "class GCN_AE(nn.Module): \n",
    "    \"\"\"\n",
    "    Graph Convolutional Auto-Encoder\n",
    "\n",
    "    GCN layer implementation from: \n",
    "        https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                edge_index, \n",
    "                input_size, \n",
    "                hidden_size_1,\n",
    "                hidden_size_2,\n",
    "                encoding_size,\n",
    "                random_init = True, \n",
    "                with_bias = True,\n",
    "                device= 'cuda:0' if torch.cuda.is_available() else 'mps'): \n",
    "        super().__init__()\n",
    "        # meta information\n",
    "        self.device = device\n",
    "        self.edge_index = edge_index # same as edge list, replace adjacency matrix \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_1 = hidden_size_1\n",
    "        self.hidden_size_2 = hidden_size_2\n",
    "        self.encoding_size = encoding_size\n",
    "        self.random_init = random_init\n",
    "        self.with_bias = with_bias\n",
    "\n",
    "        # training utilities \n",
    "        # self.criterion = None \n",
    "        # self.optimizer = None \n",
    "        \n",
    "        # layers \n",
    "        self.GCN_1 = GCNConvCustom(edge_index=self.edge_index, input_dim=self.input_size, output_dim=self.hidden_size_1, random_init=self.random_init, with_bias=self.with_bias, device=self.device) \n",
    "        self.GCN_2 = GCNConvCustom(edge_index=self.edge_index, input_dim=self.hidden_size_1, output_dim=self.encoding_size, random_init=self.random_init, with_bias=self.with_bias, device=self.device)\n",
    "        # self.FC = nn.Linear(in_features=self.hidden_size_2, out_features=self.encoding_size, device=self.device)\n",
    "\n",
    "        # activations \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encoder(self, X): \n",
    "        X_hat = self.GCN_1(X) # first layer: lower dimension feature matrix\n",
    "        # X_hat = self.relu(X_hat) \n",
    "        H = self.GCN_2(X_hat) # second layer: mean matrix\n",
    "        # Z = self.relu(H)\n",
    "        # Z = self.FC(H)\n",
    "        # Z = self.relu(Z) # this activation may or may not be here, doesn't make a difference  \n",
    "        return H\n",
    "\n",
    "    def decoder(self, Z): \n",
    "        Y_inner = torch.mm(Z, Z.T) # calculate inner product of matrix \n",
    "        # Y_inner = Y_inner.reshape((-1)) # flatten the tensor \n",
    "        Y = self.sigmoid(Y_inner) # apply activation \n",
    "        return Y\n",
    "\n",
    "    def forward(self, X): \n",
    "        Z = self.encoder(X)\n",
    "        output = self.decoder(Z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads Cora dataset, at root location: ../data/raw/Planetoid\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 140\n",
      "Training node label rate: 0.05\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "\n",
      "            There are a total of 10556. \n",
      "\n",
      "            Train dataset has 5278 edges. \n",
      "\n",
      "            Validation dataset has 2111 edges. \n",
      "\n",
      "            Test dataset has 3167 edges. \n",
      "          \n"
     ]
    }
   ],
   "source": [
    "# data preparation \n",
    "\"\"\"\n",
    "Strategy: \n",
    "* Randomly sample 50% edges (training), 20% edges (validation), and 30% edges (testing). \n",
    "* We want to evaluate against the edges that are uniformly distributed connected to each nodes, \n",
    "    so create a couple of random sampled train, validation, test datasets, evaluate the performance against each. \n",
    "\"\"\"\n",
    "def extract_train_test_data(edge_index, node_features=None, labels=None, device='cuda:0' if torch.cuda.is_available() else 'mps'): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "        edge_index: torch.Tensor, shape should be (2, # of edges in graph)\n",
    "        node_features: troch.Tensor, shape should be (# of nodes, feature dimension)\n",
    "        labels: torch.Tensor, shape should eb (# of nodes, )\n",
    "        \n",
    "    Return\n",
    "        datasets: list, contain randomly sampled (train_matrix, validation_matrix, test_matix) dataset; same length as the number of trials specified. \n",
    "            train_matrix: sub-graph created by sampling 50% of the edges from the original graph \n",
    "            validation_matix: sub-graph created by sampling 20% of the edges from the original graph \n",
    "            test_matrix: sub-graph of the rest of the 30% of the original graph\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.shape[1] \n",
    "    train_size, val_size = int(0.5*num_edges), int(0.2*num_edges)\n",
    "    test_size = num_edges - train_size - val_size\n",
    "    print(f\"\"\"\n",
    "            There are a total of {num_edges}. \\n\n",
    "            Train dataset has {train_size} edges. \\n\n",
    "            Validation dataset has {val_size} edges. \\n\n",
    "            Test dataset has {test_size} edges. \n",
    "          \"\"\")\n",
    "   \n",
    "    randomize_indices = torch.randperm(num_edges)\n",
    "    train_indices, val_indices, test_indices = randomize_indices[:train_size], randomize_indices[train_size:train_size+val_size], randomize_indices[train_size+val_size:]\n",
    "    train_matrix, validation_matrix, test_matrix = edge_index[:, train_indices], edge_index[:, val_indices], edge_index[:, test_indices] \n",
    "   \n",
    "    return train_matrix.to(device), validation_matrix.to(device), test_matrix.to(device)\n",
    "    \n",
    "edge_index, node_features, labels = loader_cora_torch(shuffle=True, device='cuda:0' if torch.cuda.is_available() else 'mps') # load the cora dataset\n",
    "train_subset, validation_subset, test_subset = extract_train_test_data(edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN_AE(edge_index=train_subset, input_size=node_features.shape[1], hidden_size_1=1000, hidden_size_2=200, encoding_size=100, random_init=True, with_bias=False)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "# format train_subset (edge list) to train_matrix (adjacency matrix)\n",
    "train_matrix = gutils.to_dense_adj(train_subset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a120b31c19842ceaaafb433e7a19a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss:  17.638368606567383\n"
     ]
    }
   ],
   "source": [
    "# train model \n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "epochs = 1500\n",
    "train_running_loss = 0.0\n",
    "with tqdm(range(epochs), unit=\"batch\") as tepoch:\n",
    "    for epoch in tepoch: \n",
    "        # train \n",
    "        model.train(True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output and run backprop \n",
    "        output = model(node_features)\n",
    "        loss = criterion(output, train_matrix)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy \n",
    "        # print(output)\n",
    "        accuracy = torch.mean(output == train_matrix, dtype=torch.float).item()\n",
    "\n",
    "        # print results \n",
    "        train_running_loss += loss\n",
    "        tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy)\n",
    "\n",
    "print(\"Final Training Loss: \", (train_running_loss/epochs).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def saveModel():\n",
    "    path = \"./GCNs/GCN_AE_linkprediction.pth\"\n",
    "    torch.save(model.state_dict(), path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('gcn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5c63958b8fbdae5d372fc0dd404e4e10eddae4b1263fc1fb5bccd426a7de9b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
