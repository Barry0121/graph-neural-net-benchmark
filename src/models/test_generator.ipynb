{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import numpy as np\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "from generator_utils import *\n",
    "from discriminator import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_plain(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, has_input=True, has_output=False, output_size=None):\n",
    "        super(GRU_plain, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.has_input = has_input\n",
    "        self.has_output = has_output\n",
    "\n",
    "        if has_input:\n",
    "            self.input = nn.Linear(input_size, embedding_size)\n",
    "            self.rnn = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        if has_output:\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, output_size)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        # initialize\n",
    "        self.hidden = None  # need initialize before forward run\n",
    "\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.25)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param,gain=nn.init.calculate_gain('sigmoid'))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight = init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(choose_device())\n",
    "\n",
    "    def forward(self, input_raw, pack=False, input_len=None):\n",
    "        if self.has_input:\n",
    "            input = self.input(input_raw)\n",
    "            input = self.relu(input)\n",
    "        else:\n",
    "            input = input_raw\n",
    "        if pack:\n",
    "            input = pack_padded_sequence(input, input_len, batch_first=True)\n",
    "        output_raw, self.hidden = self.rnn(input, self.hidden)\n",
    "        if pack:\n",
    "            output_raw = pad_packed_sequence(output_raw, batch_first=True)[0]\n",
    "        if self.has_output:\n",
    "            output_raw = self.output(output_raw)\n",
    "        # return hidden state at each time step\n",
    "        return output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_adj(adj_output):\n",
    "    '''\n",
    "    From GraphRNN codebase\n",
    "        recover to adj from adj_output\n",
    "        note: here adj_output have shape (n-1)*m\n",
    "    '''\n",
    "    max_prev_node = adj_output.shape[1]\n",
    "    adj = torch.zeros((adj_output.shape[0], adj_output.shape[0]))\n",
    "    print(adj.size())\n",
    "    reverse_adj = torch.flip(adj_output, dims=(1,))\n",
    "    for i in range(adj_output.shape[0]):\n",
    "        input_start = max(0, i - max_prev_node + 1)\n",
    "        input_end = i + 1\n",
    "        output_start = max_prev_node + max(0, i - max_prev_node + 1) - (i + 1)\n",
    "        output_end = max_prev_node\n",
    "        # adj[i, input_start:input_end] = adj_output[i,::-1][output_start:output_end]\n",
    "        # print(adj[i, input_start:input_end].size())\n",
    "        # print(reverse_adj[i,output_start:output_end][:, 0].size())\n",
    "        adj[i, input_start:input_end] = reverse_adj[i,output_start:output_end][:, 0]\n",
    "    adj_full = torch.zeros((adj_output.shape[0]+1, adj_output.shape[0]+1))\n",
    "    n = adj_full.shape[0]\n",
    "    adj_full[1:n, 0:n-1] = torch.tril(adj, 0)\n",
    "    adj_full = adj_full + adj_full.T\n",
    "\n",
    "    return adj_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRNN(nn.Module):\n",
    "    def __init__(self, args, device=choose_device()) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.rnn = GRU_plain(input_size=self.args.max_prev_node, embedding_size=self.args.embedding_size_rnn,\n",
    "                        hidden_size=self.args.hidden_size_rnn, num_layers=self.args.num_layers, has_input=True,\n",
    "                        has_output=True, output_size=self.args.hidden_size_rnn_output).to(self.device)\n",
    "        self.output = GRU_plain(input_size=1, embedding_size=self.args.embedding_size_rnn_output,\n",
    "                            hidden_size=self.args.hidden_size_rnn_output, num_layers=self.args.num_layers, has_input=True,\n",
    "                            has_output=True, output_size=1).to(self.device)\n",
    "\n",
    "        # load data state\n",
    "        if args.load:\n",
    "            fname = args.model_save_path + args.fname + 'lstm_' + str(args.load_epoch) + '.dat'\n",
    "            self.rnn.load_state_dict(torch.load(fname))\n",
    "            fname = args.model_save_path + args.fname + 'output_' + str(args.load_epoch) + '.dat'\n",
    "            self.output.load_state_dict(torch.load(fname))\n",
    "\n",
    "            args.lr = 0.00001\n",
    "            epoch = args.load_epoch\n",
    "            print('model loaded!, lr: {}'.format(args.lr))\n",
    "        else:\n",
    "            epoch = 1\n",
    "\n",
    "    # ====Call these in training loop====\n",
    "    def init_optimizer(self, lr):\n",
    "        \"\"\"Initialize optimizers and schedular for both RNNs\"\"\"\n",
    "        self.optimizer_rnn = optim.Adam(list(self.rnn.parameters()), lr=lr)\n",
    "        self.optimizer_output = optim.Adam(list(self.output.parameters()), lr=lr)\n",
    "        self.scheduler_rnn = MultiStepLR(self.optimizer_rnn, milestones=self.args.milestones)\n",
    "        self.scheduler_output = MultiStepLR(self.optimizer_output, milestones=self.args.milestones)\n",
    "        return self.optimizer_rnn, self.optimizer_output, self.scheduler_rnn, self.scheduler_output\n",
    "\n",
    "    def clear_gradient_models(self):\n",
    "        self.rnn.zero_grad()\n",
    "        self.output.zero_grad()\n",
    "\n",
    "    def train(self, flag):\n",
    "        if flag:\n",
    "            self.rnn.train(True)\n",
    "            self.output.train(True)\n",
    "        else:\n",
    "            self.rnn.train(False)\n",
    "            self.output.train(False)\n",
    "\n",
    "    def clear_gradient_opts(self):\n",
    "        self.optimizer_rnn.zero_grad()\n",
    "        self.optimizer_output.zero_grad()\n",
    "\n",
    "    def all_steps(self):\n",
    "        self.optimizer_rnn.step()\n",
    "        self.optimizer_output.step()\n",
    "        self.scheduler_rnn.step()\n",
    "        self.scheduler_output.step()\n",
    "\n",
    "    def sort_data_per_epoch(self, X, Y, length):\n",
    "        x_unsorted = X.float()\n",
    "        y_unsorted = Y.float()\n",
    "        y_len_unsorted = length\n",
    "        y_len_max = max(y_len_unsorted)\n",
    "        x_unsorted = x_unsorted[:, 0:y_len_max, :]\n",
    "        y_unsorted = y_unsorted[:, 0:y_len_max, :]\n",
    "        y_len,sort_index = torch.sort(y_len_unsorted,0,descending=True)\n",
    "        y_len = y_len.numpy().tolist()\n",
    "        x = torch.index_select(x_unsorted,0,sort_index)\n",
    "        y = torch.index_select(y_unsorted,0,sort_index)\n",
    "        y_reshape = pack_padded_sequence(y,y_len,batch_first=True).data\n",
    "        idx = [i for i in range(y_reshape.size(0)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx)\n",
    "        y_reshape = y_reshape.index_select(0, idx)\n",
    "        y_reshape = y_reshape.view(y_reshape.size(0),y_reshape.size(1),1)\n",
    "        output_x = torch.cat((torch.ones(y_reshape.size(0),1,1),y_reshape[:,0:-1,0:1]),dim=1) # x's shape is determined by y's shape\n",
    "        output_y = y_reshape\n",
    "        output_y_len = []\n",
    "        output_y_len_bin = np.bincount(np.array(y_len))\n",
    "        for i in range(len(output_y_len_bin)-1,0,-1):\n",
    "            count_temp = np.sum(output_y_len_bin[i:]) # count how many y_len is above i\n",
    "            output_y_len.extend([min(i,y.size(2))]*count_temp) # put them in output_y_len; max value should not exceed y.size(2)\n",
    "\n",
    "        # pack into variable\n",
    "        x = Variable(x).to(self.device)\n",
    "        y = Variable(y).to(self.device)\n",
    "        output_x = Variable(output_x).to(self.device)\n",
    "        output_y = Variable(output_y).to(self.device)\n",
    "        batch_size = x_unsorted.size(0)\n",
    "        return x, y, output_x, output_y, y_len, output_y_len, batch_size\n",
    "    \n",
    "    # ======================================\n",
    "\n",
    "    def forward(self, noise, X, Y, length):\n",
    "        \"\"\"\n",
    "        X: noise/latent vector\n",
    "        args: arguments dictionary\n",
    "        test_batch_size: number of graphs you want to generate\n",
    "        \"\"\"\n",
    "        # provide a option to change number of graphs generated\n",
    "        output_batch_size = self.args.test_batch_size\n",
    "        input_hidden = torch.stack(self.rnn.num_layers*[noise]).to(self.device)\n",
    "        self.rnn.hidden = input_hidden # expected shape: (num_layer, batch_size, hidden_size)\n",
    "\n",
    "        x, y, output_x, output_y, y_len, output_y_len, _ = self.sort_data_per_epoch(X, Y, length)\n",
    "        \n",
    "        h = self.rnn(x, pack=True, input_len=y_len)\n",
    "        h = pack_padded_sequence(h,y_len,batch_first=True).data # get packed hidden vector\n",
    "        # reverse h\n",
    "        idx = [i for i in range(h.size(0) - 1, -1, -1)]\n",
    "        idx = Variable(torch.LongTensor(idx)).cuda()\n",
    "        h = h.index_select(0, idx)\n",
    "        hidden_null = Variable(torch.zeros(self.rnn.num_layers-1, h.size(0), h.size(1))).cuda()\n",
    "        self.output.hidden = torch.cat((h.view(1,h.size(0),h.size(1)),hidden_null),dim=0) # num_layers, batch_size, hidden_size\n",
    "        y_pred = self.output(output_x, pack=True, input_len=output_y_len)\n",
    "        y_pred = F.sigmoid(y_pred)\n",
    "        # clean\n",
    "        y_pred = pack_padded_sequence(y_pred, output_y_len, batch_first=True)\n",
    "        y_pred = pad_packed_sequence(y_pred, batch_first=True)[0]\n",
    "\n",
    "        out = decode_adj(y_pred)\n",
    "        print(out.size())\n",
    "        # # TODO: change this part to noise vector might need resizing\n",
    "        # # y_pred_long = Variable(torch.zeros(output_batch_size, self.args.max_num_node, self.args.max_prev_node)).to(self.device) # discrete prediction\n",
    "        # y_pred_long = torch.zeros(output_batch_size, self.args.max_num_node, self.args.max_prev_node).to(self.device) # discrete prediction\n",
    "        # # x_step = X.to(self.device) # shape:(batch_size, 1, self.args.max_prev_node)\n",
    "\n",
    "        # x_step = torch.ones(output_batch_size, 1, self.args.max_prev_node).to(self.device)\n",
    "\n",
    "        # # iterative graph generation\n",
    "        # for i in range(self.args.max_num_node):\n",
    "        #     # for each node\n",
    "        #     # 1. we use rnn to create new node embedding\n",
    "        #     # 2. we use output to create new edges\n",
    "\n",
    "        #     # (1)\n",
    "        #     h = self.rnn(x_step)\n",
    "        #     hidden_null = torch.zeros(self.args.num_layers - 1, h.size(0), h.size(2)).to(self.device)\n",
    "        #     x_step = torch.zeros(output_batch_size, 1, self.args.max_prev_node).to(self.device)\n",
    "        #     output_x_step = torch.ones(output_batch_size, 1, 1).to(self.device)\n",
    "        #     # (2)\n",
    "        #     # self.output.hidden = torch.cat((h.permute(1,0,2), hidden_null), dim=0).to(self.device)\n",
    "        #     # for j in range(min(self.args.max_prev_node,i+1)):\n",
    "        #     #     output_y_pred_step = self.output(output_x_step)\n",
    "        #     #     # print(output_y_pred_step.requires_grad)\n",
    "        #     #     output_x_step = sample_sigmoid(output_y_pred_step, sample=True, sample_time=1, device=self.device)\n",
    "        #     #     x_step[:,:,j:j+1] = output_x_step\n",
    "        #     #     # self.output.hidden = Variable(self.output.hidden.data).to(self.device)\n",
    "        #     # y_pred_long[:, i:i + 1, :] = x_step\n",
    "        #     self.rnn.hidden = Variable(self.rnn.hidden.data).to(self.device)\n",
    "\n",
    "        # y_pred_long_data = y_pred_long.long()\n",
    "\n",
    "        return out\n",
    "\n",
    "        # print(1)\n",
    "        init_adj_pred = decode_adj(y_pred_long_data[0].cpu())\n",
    "        adj_pred_list = torch.zeros((output_batch_size, init_adj_pred.size(0), init_adj_pred.size(1)))\n",
    "        for i in range(output_batch_size):\n",
    "            # adj_pred = decode_adj(y_pred_long_data[i].cpu().numpy())\n",
    "            # adj_pred_list = np.append(adj_pred_list, adj_pred)\n",
    "            # adj_pred_list.append(adj_pred)\n",
    "            adj_pred_list[i, :, :] = decode_adj(y_pred_long_data[i].cpu())\n",
    "\n",
    "        # return torch.Tensor(np.array(adj_pred_list))\n",
    "        return adj_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_inverter=False, num_layers=4, clamp_lower=-0.1, clamp_upper=0.1, lr=1e-3, betas=1e-5, lamb=0.1, loss_func='MSE', device=choose_device()):\n",
    "    # get the dataset\n",
    "    lr = 1e-4\n",
    "    train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "    train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "    train_loader = get_dataloader_labels(train_dataset, args)\n",
    "    noise_dim = args.hidden_size_rnn\n",
    "    # print('noise dimension is: ', noise_dim)\n",
    "\n",
    "    # initialize noise, optimizer and loss\n",
    "    netG = GraphRNN(args=args)\n",
    "    netD = NetD(stat_input_dim=128, stat_hidden_dim=64, num_stat=2)\n",
    "    hg = list(netG.parameters())[2].register_hook(lambda grad: print(f\"NetG parameter Update with gradient {grad}\"))\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # check model parameters\n",
    "    # for param in netD.parameters():\n",
    "    #     print(param.name, param.data, param.requires_grad)\n",
    "    # for param in netG.parameters():\n",
    "    #     print(param.name, param.data, param.requires_grad)\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=[betas for _ in range(2)])\n",
    "    G_optimizer_rnn, G_optimizer_output, G_scheduler_rnn, G_scheduler_output = netG.init_optimizer(lr=0.1) # initialize optimizers\n",
    "\n",
    "\n",
    "    noise = torch.randn(args.batch_size, noise_dim).to(device)\n",
    "\n",
    "    gen_iterations = 0\n",
    "    for e in range(args.epochs):\n",
    "        # for now, treat the input as adj matrices\n",
    "        for i, data in tqdm(enumerate(train_loader), desc=f\"Training epoch#{e+1}\", total=len(train_loader)):\n",
    "            X = data['x']\n",
    "            Y = data['y']\n",
    "            adj_mat = data['adj_mat']\n",
    "            Y_len = data['len']\n",
    "\n",
    "            # zero grad\n",
    "            optimizerD.zero_grad()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "\n",
    "            # skip uneven batch\n",
    "            if adj_mat.size(0) != args.batch_size:\n",
    "                continue\n",
    "            \n",
    "            # ========== Train Generator ==================\n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            # netG.clear_gradient_models()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "            # in case our last batch was the tail batch of the dataloader,\n",
    "            # make sure we feed a full batch of noise\n",
    "            # noisev = noise.normal_(0,1)\n",
    "            noisev = torch.randn(args.batch_size, noise_dim)\n",
    "            # print(f\"noise size: {noisev.size()}\")\n",
    "            fake = netG(noisev, X, Y, Y_len)\n",
    "            # fake_tensor = netD(fake)\n",
    "            # output = Variable(loss(fake, torch.ones((32, 6200)).cuda()), requires_grad=True)\n",
    "            # print(fake.size())\n",
    "            # target = torch.((2405, 2405)).to('cuda')\n",
    "            output = torch.mean(fake.cuda())\n",
    "            output.backward()\n",
    "            print(output.grad)\n",
    "\n",
    "            # errG = fake_tensor\n",
    "            # errG.backward()\n",
    "            G_optimizer_rnn.step()\n",
    "            G_optimizer_output.step()\n",
    "            # # for p in netG.parameters()[0]:\n",
    "            # # netG.all_steps()\n",
    "            # gen_iterations += 1\n",
    "\n",
    "            print(f\"errG for generator: {output.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating max previous node, total iteration: 12000\n",
      "iter 0 times\n",
      "iter 2400 times\n",
      "iter 4800 times\n",
      "iter 7200 times\n",
      "iter 9600 times\n",
      "max previous node: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:   0%|          | 0/35 [00:00<?, ?it/s]/home/jgeng/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2404, 2404])\n",
      "torch.Size([2405, 2405])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jgeng/.conda/envs/dev180/lib/python3.9/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n",
      "Training epoch#1:   3%|▎         | 1/35 [00:09<05:35,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 2.0595e-10,  8.9589e-12,  2.0067e-10,  ...,  4.7315e-10,\n",
      "          3.5230e-10,  4.7792e-10],\n",
      "        [ 7.8863e-10,  5.2105e-11,  5.9448e-10,  ...,  1.8314e-09,\n",
      "          5.4963e-10,  1.2023e-09],\n",
      "        [-1.8632e-10, -7.7048e-12, -1.5629e-10,  ..., -3.9626e-11,\n",
      "         -2.0188e-10, -1.1530e-10],\n",
      "        ...,\n",
      "        [ 6.4004e-08,  3.2564e-09,  7.2966e-08,  ...,  2.7935e-07,\n",
      "          1.1535e-07,  2.2295e-07],\n",
      "        [ 1.8974e-07,  8.9252e-09,  2.4370e-07,  ...,  9.2353e-07,\n",
      "          3.8537e-07,  7.3718e-07],\n",
      "        [-8.9927e-09, -8.3062e-10, -1.0176e-08,  ..., -3.4090e-08,\n",
      "         -1.7478e-08, -2.9322e-08]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 0.004558607004582882\n",
      "torch.Size([1382, 1382])\n",
      "torch.Size([1383, 1383])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:   6%|▌         | 2/35 [00:11<02:50,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[-2.4963e-09, -4.7767e-10, -2.9195e-09,  ..., -2.6322e-08,\n",
      "         -4.1873e-08, -1.5409e-08],\n",
      "        [-7.7858e-09, -1.1108e-09, -6.6949e-09,  ..., -7.3920e-08,\n",
      "         -9.0500e-08, -4.1059e-08],\n",
      "        [ 2.0892e-10,  4.5747e-11,  2.6799e-10,  ...,  2.4096e-09,\n",
      "          3.4898e-09,  1.4116e-09],\n",
      "        ...,\n",
      "        [ 5.6885e-09,  9.4744e-10,  7.5777e-09,  ...,  6.2607e-08,\n",
      "          8.9827e-08,  3.2734e-08],\n",
      "        [ 2.7659e-09,  6.2927e-10,  4.2778e-09,  ...,  3.0808e-08,\n",
      "          4.4166e-08,  1.7126e-08],\n",
      "        [ 9.0999e-10,  1.8117e-10,  1.1023e-09,  ...,  9.5203e-09,\n",
      "          1.4409e-08,  5.4537e-09]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 0.006219551432877779\n",
      "torch.Size([1868, 1868])\n",
      "torch.Size([1869, 1869])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:   9%|▊         | 3/35 [00:16<02:36,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 3.7298e-19,  1.6447e-20,  1.2956e-17,  ..., -3.0166e-14,\n",
      "         -1.1413e-13,  3.2924e-16],\n",
      "        [ 1.2058e-19, -3.3435e-19,  8.2476e-18,  ..., -3.2648e-15,\n",
      "         -1.3714e-14,  5.0281e-16],\n",
      "        [ 2.1612e-13,  1.4217e-13,  1.7947e-12,  ..., -1.2123e-11,\n",
      "         -1.5749e-11, -9.3801e-12],\n",
      "        ...,\n",
      "        [-5.2800e-13, -4.5570e-13, -8.1489e-12,  ..., -1.6548e-10,\n",
      "         -2.5369e-10, -7.5274e-11],\n",
      "        [-6.0460e-11, -4.6536e-11, -6.2041e-10,  ..., -3.8537e-09,\n",
      "         -6.0499e-09, -1.2458e-09],\n",
      "        [-1.2023e-10,  9.0138e-11, -2.9118e-10,  ...,  9.5565e-09,\n",
      "          2.3274e-08,  8.1924e-09]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 0.0012082959292456508\n",
      "torch.Size([1611, 1611])\n",
      "torch.Size([1612, 1612])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  11%|█▏        | 4/35 [00:19<02:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[-2.7858e-23, -4.3726e-21,  8.0379e-20,  ..., -8.2847e-21,\n",
      "         -2.3796e-18,  1.0938e-19],\n",
      "        [ 0.0000e+00,  1.1908e-22,  5.5784e-20,  ..., -7.5729e-20,\n",
      "         -6.3210e-19,  0.0000e+00],\n",
      "        [-1.4629e-15, -2.4484e-15, -1.2725e-13,  ..., -4.9593e-13,\n",
      "         -1.4729e-12, -1.4533e-13],\n",
      "        ...,\n",
      "        [ 1.8142e-13,  8.9272e-13,  3.1484e-11,  ..., -1.1343e-11,\n",
      "          1.0107e-10, -2.0742e-11],\n",
      "        [-1.1986e-10,  1.7830e-10,  3.3210e-09,  ..., -3.5750e-09,\n",
      "         -1.0021e-09, -1.9392e-09],\n",
      "        [ 7.3380e-15,  2.3276e-14,  1.1769e-13,  ..., -5.9490e-11,\n",
      "         -2.2905e-10,  4.0283e-13]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 0.0002687618543859571\n",
      "torch.Size([1590, 1590])\n",
      "torch.Size([1591, 1591])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  14%|█▍        | 5/35 [00:21<01:39,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.2327e-20,\n",
      "         -1.3627e-20, -7.8875e-22],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.1107e-20,\n",
      "         -1.9448e-20,  3.2930e-22],\n",
      "        [ 3.4740e-19,  3.8095e-19,  2.8388e-16,  ...,  6.0155e-15,\n",
      "          1.2698e-14,  1.0613e-15],\n",
      "        ...,\n",
      "        [-9.5077e-16,  1.0633e-12,  3.5649e-12,  ...,  4.2870e-11,\n",
      "          6.5003e-11,  7.6742e-12],\n",
      "        [ 2.2756e-12, -5.2824e-11, -7.5207e-11,  ...,  1.4582e-10,\n",
      "          1.2254e-09,  1.0600e-09],\n",
      "        [-7.7126e-18,  4.1503e-17,  1.0052e-16,  ...,  6.4926e-12,\n",
      "          2.4737e-11, -2.3302e-15]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 4.734957838081755e-05\n",
      "torch.Size([2159, 2159])\n",
      "torch.Size([2160, 2160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  17%|█▋        | 6/35 [00:27<02:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -5.2815e-21,\n",
      "         -8.6422e-21, -1.1554e-22],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.0086e-20,\n",
      "          4.9231e-20,  6.5817e-22],\n",
      "        [ 2.1752e-24,  1.3560e-22,  3.4811e-21,  ..., -2.7997e-17,\n",
      "         -4.5749e-17, -6.0353e-19],\n",
      "        ...,\n",
      "        [-3.7402e-18, -2.4932e-15, -6.6019e-14,  ...,  1.2423e-12,\n",
      "          8.6935e-13, -1.9191e-13],\n",
      "        [-5.6405e-13, -5.9297e-12, -2.0916e-11,  ...,  1.5486e-11,\n",
      "          2.0652e-10, -5.0533e-12],\n",
      "        [-8.4061e-21,  2.3644e-19,  6.8101e-19,  ...,  1.5683e-13,\n",
      "          2.5689e-13,  3.5651e-15]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 4.488212653086521e-06\n",
      "torch.Size([1550, 1550])\n",
      "torch.Size([1551, 1551])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  20%|██        | 7/35 [00:29<01:38,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  2.7005e-25,  1.0419e-23,  ...,  9.4659e-20,\n",
      "          1.5928e-19,  1.0746e-23],\n",
      "        ...,\n",
      "        [-2.6470e-19,  8.6779e-19, -1.0362e-16,  ...,  1.1080e-12,\n",
      "          1.8748e-12, -2.2478e-16],\n",
      "        [-9.3395e-13,  1.2368e-13, -1.1546e-12,  ..., -1.0686e-10,\n",
      "         -2.4678e-10, -1.0460e-11],\n",
      "        [ 2.4172e-22,  0.0000e+00,  0.0000e+00,  ..., -1.2704e-20,\n",
      "         -1.0008e-19, -2.5974e-33]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 9.6201426913467e-07\n",
      "torch.Size([2254, 2254])\n",
      "torch.Size([2255, 2255])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  23%|██▎       | 8/35 [00:35<01:59,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.2012e-22,\n",
      "          7.3043e-22, -5.3221e-26],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  5.2551e-23, -1.9930e-21,  ...,  5.1140e-17,\n",
      "          9.0975e-17,  4.5350e-19],\n",
      "        [ 1.3102e-16, -1.4110e-13,  1.8636e-13,  ...,  1.4266e-11,\n",
      "          4.3216e-11,  9.8570e-13],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.0446e-20,\n",
      "          3.5581e-20,  0.0000e+00]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 1.0834463637365843e-07\n",
      "torch.Size([2069, 2069])\n",
      "torch.Size([2070, 2070])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  26%|██▌       | 9/35 [00:40<01:59,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.0664e-24,\n",
      "         -3.6718e-24,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0180e-18,\n",
      "          2.0097e-18,  3.9953e-20],\n",
      "        [ 5.0527e-17, -3.3481e-16, -1.7340e-13,  ..., -1.1408e-12,\n",
      "         -2.9286e-12, -1.4902e-14],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.1018e-26,\n",
      "          1.9578e-26,  0.0000e+00]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 1.772522395526721e-08\n",
      "torch.Size([1908, 1908])\n",
      "torch.Size([1909, 1909])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  29%|██▊       | 10/35 [00:43<01:46,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -6.4871e-27,\n",
      "         -1.1723e-26,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00, -8.1796e-22,  ...,  5.2515e-20,\n",
      "          8.8798e-20,  0.0000e+00],\n",
      "        [-1.1245e-20, -1.3227e-17, -2.4596e-15,  ...,  7.4910e-15,\n",
      "         -6.4332e-15,  2.8890e-18],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n",
      "None\n",
      "errG for generator: 3.4749572108694338e-09\n",
      "torch.Size([1193, 1193])\n",
      "torch.Size([1194, 1194])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:  29%|██▊       | 10/35 [00:45<01:53,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetG parameter Update with gradient tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.2201e-29,\n",
      "         -7.7320e-29,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0569e-20,\n",
      "         -5.6007e-20,  0.0000e+00],\n",
      "        [-5.2518e-21, -5.8681e-17, -6.1292e-15,  ..., -2.8110e-14,\n",
      "         -1.0662e-13, -1.8786e-17],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args\u001b[39m=\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn[47], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, train_inverter, num_layers, clamp_lower, clamp_upper, lr, betas, lamb, loss_func, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# fake_tensor = netD(fake)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# output = Variable(loss(fake, torch.ones((32, 6200)).cuda()), requires_grad=True)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# print(fake.size())\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# target = torch.((2405, 2405)).to('cuda')\u001b[39;00m\n\u001b[1;32m     62\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(fake\u001b[39m.\u001b[39mcuda())\n\u001b[0;32m---> 63\u001b[0m output\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     64\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m     66\u001b[0m \u001b[39m# errG = fake_tensor\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m# errG.backward()\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
