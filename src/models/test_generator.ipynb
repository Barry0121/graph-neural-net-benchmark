{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import numpy as np\n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "from generator_utils import *\n",
    "from discriminator import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_plain(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, has_input=True, has_output=False, output_size=None):\n",
    "        super(GRU_plain, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.has_input = has_input\n",
    "        self.has_output = has_output\n",
    "\n",
    "        if has_input:\n",
    "            self.input = nn.Linear(input_size, embedding_size)\n",
    "            self.rnn = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        if has_output:\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(hidden_size, embedding_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(embedding_size, output_size)\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        # initialize\n",
    "        self.hidden = None  # need initialize before forward run\n",
    "\n",
    "        for name, param in self.rnn.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param, 0.25)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param,gain=nn.init.calculate_gain('sigmoid'))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight = init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(choose_device())\n",
    "\n",
    "    def forward(self, input_raw, pack=False, input_len=None):\n",
    "        if self.has_input:\n",
    "            input = self.input(input_raw)\n",
    "            input = self.relu(input)\n",
    "        else:\n",
    "            input = input_raw\n",
    "        if pack:\n",
    "            input = pack_padded_sequence(input, input_len, batch_first=True)\n",
    "        output_raw, self.hidden = self.rnn(input, self.hidden)\n",
    "        if pack:\n",
    "            output_raw = pad_packed_sequence(output_raw, batch_first=True)[0]\n",
    "        if self.has_output:\n",
    "            output_raw = self.output(output_raw)\n",
    "        # return hidden state at each time step\n",
    "        return output_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_adj(adj_output):\n",
    "    '''\n",
    "    From GraphRNN codebase\n",
    "        recover to adj from adj_output\n",
    "        note: here adj_output have shape (n-1)*m\n",
    "    '''\n",
    "    max_prev_node = adj_output.shape[1]\n",
    "    adj = torch.zeros((adj_output.shape[0], adj_output.shape[0]))\n",
    "    print(adj.size())\n",
    "    reverse_adj = torch.flip(adj_output, dims=(1,))\n",
    "    for i in range(adj_output.shape[0]):\n",
    "        input_start = max(0, i - max_prev_node + 1)\n",
    "        input_end = i + 1\n",
    "        output_start = max_prev_node + max(0, i - max_prev_node + 1) - (i + 1)\n",
    "        output_end = max_prev_node\n",
    "        # adj[i, input_start:input_end] = adj_output[i,::-1][output_start:output_end]\n",
    "        # print(adj[i, input_start:input_end].size())\n",
    "        # print(reverse_adj[i,output_start:output_end][:, 0].size())\n",
    "        adj[i, input_start:input_end] = reverse_adj[i,output_start:output_end][:, 0]\n",
    "    adj_full = torch.zeros((adj_output.shape[0]+1, adj_output.shape[0]+1))\n",
    "    n = adj_full.shape[0]\n",
    "    adj_full[1:n, 0:n-1] = torch.tril(adj, 0)\n",
    "    adj_full = adj_full + adj_full.T\n",
    "\n",
    "    return adj_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRNN(nn.Module):\n",
    "    def __init__(self, args, device=choose_device()) -> None:\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        self.rnn = GRU_plain(input_size=self.args.max_prev_node, embedding_size=self.args.embedding_size_rnn,\n",
    "                        hidden_size=self.args.hidden_size_rnn, num_layers=self.args.num_layers, has_input=True,\n",
    "                        has_output=True, output_size=self.args.hidden_size_rnn_output).to(self.device)\n",
    "        self.output = GRU_plain(input_size=1, embedding_size=self.args.embedding_size_rnn_output,\n",
    "                            hidden_size=self.args.hidden_size_rnn_output, num_layers=self.args.num_layers, has_input=True,\n",
    "                            has_output=True, output_size=1).to(self.device)\n",
    "\n",
    "        # load data state\n",
    "        if args.load:\n",
    "            fname = args.model_save_path + args.fname + 'lstm_' + str(args.load_epoch) + '.dat'\n",
    "            self.rnn.load_state_dict(torch.load(fname))\n",
    "            fname = args.model_save_path + args.fname + 'output_' + str(args.load_epoch) + '.dat'\n",
    "            self.output.load_state_dict(torch.load(fname))\n",
    "\n",
    "            args.lr = 0.00001\n",
    "            epoch = args.load_epoch\n",
    "            print('model loaded!, lr: {}'.format(args.lr))\n",
    "        else:\n",
    "            epoch = 1\n",
    "\n",
    "    # ====Call these in training loop====\n",
    "    def init_optimizer(self, lr):\n",
    "        \"\"\"Initialize optimizers and schedular for both RNNs\"\"\"\n",
    "        self.optimizer_rnn = optim.Adam(list(self.rnn.parameters()), lr=lr)\n",
    "        self.optimizer_output = optim.Adam(list(self.output.parameters()), lr=lr)\n",
    "        self.scheduler_rnn = MultiStepLR(self.optimizer_rnn, milestones=self.args.milestones)\n",
    "        self.scheduler_output = MultiStepLR(self.optimizer_output, milestones=self.args.milestones)\n",
    "        return self.optimizer_rnn, self.optimizer_output, self.scheduler_rnn, self.scheduler_output\n",
    "\n",
    "    def clear_gradient_models(self):\n",
    "        self.rnn.zero_grad()\n",
    "        self.output.zero_grad()\n",
    "\n",
    "    def train(self, flag):\n",
    "        if flag:\n",
    "            self.rnn.train(True)\n",
    "            self.output.train(True)\n",
    "        else:\n",
    "            self.rnn.train(False)\n",
    "            self.output.train(False)\n",
    "\n",
    "    def clear_gradient_opts(self):\n",
    "        self.optimizer_rnn.zero_grad()\n",
    "        self.optimizer_output.zero_grad()\n",
    "\n",
    "    def all_steps(self):\n",
    "        self.optimizer_rnn.step()\n",
    "        self.optimizer_output.step()\n",
    "        self.scheduler_rnn.step()\n",
    "        self.scheduler_output.step()\n",
    "\n",
    "    def sort_data_per_epoch(self, X, Y, length):\n",
    "        x_unsorted = X.float()\n",
    "        y_unsorted = Y.float()\n",
    "        y_len_unsorted = length\n",
    "        y_len_max = max(y_len_unsorted)\n",
    "        x_unsorted = x_unsorted[:, 0:y_len_max, :]\n",
    "        y_unsorted = y_unsorted[:, 0:y_len_max, :]\n",
    "        y_len,sort_index = torch.sort(y_len_unsorted,0,descending=True)\n",
    "        y_len = y_len.numpy().tolist()\n",
    "        x = torch.index_select(x_unsorted,0,sort_index)\n",
    "        y = torch.index_select(y_unsorted,0,sort_index)\n",
    "        y_reshape = pack_padded_sequence(y,y_len,batch_first=True).data\n",
    "        idx = [i for i in range(y_reshape.size(0)-1, -1, -1)]\n",
    "        idx = torch.LongTensor(idx)\n",
    "        y_reshape = y_reshape.index_select(0, idx)\n",
    "        y_reshape = y_reshape.view(y_reshape.size(0),y_reshape.size(1),1)\n",
    "        output_x = torch.cat((torch.ones(y_reshape.size(0),1,1),y_reshape[:,0:-1,0:1]),dim=1) # x's shape is determined by y's shape\n",
    "        output_y = y_reshape\n",
    "        output_y_len = []\n",
    "        output_y_len_bin = np.bincount(np.array(y_len))\n",
    "        for i in range(len(output_y_len_bin)-1,0,-1):\n",
    "            count_temp = np.sum(output_y_len_bin[i:]) # count how many y_len is above i\n",
    "            output_y_len.extend([min(i,y.size(2))]*count_temp) # put them in output_y_len; max value should not exceed y.size(2)\n",
    "\n",
    "        # pack into variable\n",
    "        x = Variable(x).to(self.device)\n",
    "        y = Variable(y).to(self.device)\n",
    "        output_x = Variable(output_x).to(self.device)\n",
    "        output_y = Variable(output_y).to(self.device)\n",
    "        batch_size = x_unsorted.size(0)\n",
    "        return x, y, output_x, output_y, y_len, output_y_len, batch_size\n",
    "    \n",
    "    # ======================================\n",
    "\n",
    "    def forward(self, noise, X, Y, length):\n",
    "        \"\"\"\n",
    "        X: noise/latent vector\n",
    "        args: arguments dictionary\n",
    "        test_batch_size: number of graphs you want to generate\n",
    "        \"\"\"\n",
    "        # provide a option to change number of graphs generated\n",
    "        output_batch_size = self.args.test_batch_size\n",
    "        input_hidden = torch.stack(self.rnn.num_layers*[noise]).to(self.device)\n",
    "        self.rnn.hidden = input_hidden # expected shape: (num_layer, batch_size, hidden_size)\n",
    "\n",
    "        x, y, output_x, output_y, y_len, output_y_len, _ = self.sort_data_per_epoch(X, Y, length)\n",
    "        \n",
    "        h = self.rnn(x, pack=True, input_len=y_len)\n",
    "        h = pack_padded_sequence(h,y_len,batch_first=True).data # get packed hidden vector\n",
    "        # reverse h\n",
    "        idx = [i for i in range(h.size(0) - 1, -1, -1)]\n",
    "        idx = Variable(torch.LongTensor(idx)).cuda()\n",
    "        h = h.index_select(0, idx)\n",
    "        hidden_null = Variable(torch.zeros(self.rnn.num_layers-1, h.size(0), h.size(1))).cuda()\n",
    "        self.output.hidden = torch.cat((h.view(1,h.size(0),h.size(1)),hidden_null),dim=0) # num_layers, batch_size, hidden_size\n",
    "        y_pred = self.output(output_x, pack=True, input_len=output_y_len)\n",
    "        y_pred = F.sigmoid(y_pred)\n",
    "        # clean\n",
    "        y_pred = pack_padded_sequence(y_pred, output_y_len, batch_first=True)\n",
    "        y_pred = pad_packed_sequence(y_pred, batch_first=True)[0]\n",
    "\n",
    "        out = decode_adj(y_pred)\n",
    "        print(out.size())\n",
    "        # # TODO: change this part to noise vector might need resizing\n",
    "        # # y_pred_long = Variable(torch.zeros(output_batch_size, self.args.max_num_node, self.args.max_prev_node)).to(self.device) # discrete prediction\n",
    "        # y_pred_long = torch.zeros(output_batch_size, self.args.max_num_node, self.args.max_prev_node).to(self.device) # discrete prediction\n",
    "        # # x_step = X.to(self.device) # shape:(batch_size, 1, self.args.max_prev_node)\n",
    "\n",
    "        # x_step = torch.ones(output_batch_size, 1, self.args.max_prev_node).to(self.device)\n",
    "\n",
    "        # # iterative graph generation\n",
    "        # for i in range(self.args.max_num_node):\n",
    "        #     # for each node\n",
    "        #     # 1. we use rnn to create new node embedding\n",
    "        #     # 2. we use output to create new edges\n",
    "\n",
    "        #     # (1)\n",
    "        #     h = self.rnn(x_step)\n",
    "        #     hidden_null = torch.zeros(self.args.num_layers - 1, h.size(0), h.size(2)).to(self.device)\n",
    "        #     x_step = torch.zeros(output_batch_size, 1, self.args.max_prev_node).to(self.device)\n",
    "        #     output_x_step = torch.ones(output_batch_size, 1, 1).to(self.device)\n",
    "        #     # (2)\n",
    "        #     # self.output.hidden = torch.cat((h.permute(1,0,2), hidden_null), dim=0).to(self.device)\n",
    "        #     # for j in range(min(self.args.max_prev_node,i+1)):\n",
    "        #     #     output_y_pred_step = self.output(output_x_step)\n",
    "        #     #     # print(output_y_pred_step.requires_grad)\n",
    "        #     #     output_x_step = sample_sigmoid(output_y_pred_step, sample=True, sample_time=1, device=self.device)\n",
    "        #     #     x_step[:,:,j:j+1] = output_x_step\n",
    "        #     #     # self.output.hidden = Variable(self.output.hidden.data).to(self.device)\n",
    "        #     # y_pred_long[:, i:i + 1, :] = x_step\n",
    "        #     self.rnn.hidden = Variable(self.rnn.hidden.data).to(self.device)\n",
    "\n",
    "        # y_pred_long_data = y_pred_long.long()\n",
    "\n",
    "        return out\n",
    "\n",
    "        # print(1)\n",
    "        init_adj_pred = decode_adj(y_pred_long_data[0].cpu())\n",
    "        adj_pred_list = torch.zeros((output_batch_size, init_adj_pred.size(0), init_adj_pred.size(1)))\n",
    "        for i in range(output_batch_size):\n",
    "            # adj_pred = decode_adj(y_pred_long_data[i].cpu().numpy())\n",
    "            # adj_pred_list = np.append(adj_pred_list, adj_pred)\n",
    "            # adj_pred_list.append(adj_pred)\n",
    "            adj_pred_list[i, :, :] = decode_adj(y_pred_long_data[i].cpu())\n",
    "\n",
    "        # return torch.Tensor(np.array(adj_pred_list))\n",
    "        return adj_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_inverter=False, num_layers=4, clamp_lower=-0.1, clamp_upper=0.1, lr=1e-3, betas=1e-5, lamb=0.1, loss_func='MSE', device=choose_device()):\n",
    "    # get the dataset\n",
    "    lr = 1e-4\n",
    "    train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "    train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "    train_loader = get_dataloader_labels(train_dataset, args)\n",
    "    noise_dim = args.hidden_size_rnn\n",
    "    # print('noise dimension is: ', noise_dim)\n",
    "\n",
    "    # initialize noise, optimizer and loss\n",
    "    netG = GraphRNN(args=args)\n",
    "    netD = NetD(stat_input_dim=128, stat_hidden_dim=64, num_stat=2)\n",
    "    hg = list(netG.parameters())[2].register_hook(lambda grad: print(f\"NetG parameter Update with gradient {grad}\"))\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    # check model parameters\n",
    "    # for param in netD.parameters():\n",
    "    #     print(param.name, param.data, param.requires_grad)\n",
    "    # for param in netG.parameters():\n",
    "    #     print(param.name, param.data, param.requires_grad)\n",
    "\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=[betas for _ in range(2)])\n",
    "    G_optimizer_rnn, G_optimizer_output, G_scheduler_rnn, G_scheduler_output = netG.init_optimizer(lr=0.1) # initialize optimizers\n",
    "\n",
    "\n",
    "    noise = torch.randn(args.batch_size, noise_dim).to(device)\n",
    "\n",
    "    gen_iterations = 0\n",
    "    for e in range(args.epochs):\n",
    "        # for now, treat the input as adj matrices\n",
    "        for i, data in tqdm(enumerate(train_loader), desc=f\"Training epoch#{e+1}\", total=len(train_loader)):\n",
    "            X = data['x']\n",
    "            Y = data['y']\n",
    "            adj_mat = data['adj_mat']\n",
    "            Y_len = data['len']\n",
    "\n",
    "            # zero grad\n",
    "            optimizerD.zero_grad()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "\n",
    "            # skip uneven batch\n",
    "            if adj_mat.size(0) != args.batch_size:\n",
    "                continue\n",
    "            \n",
    "            # ========== Train Generator ==================\n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            # netG.clear_gradient_models()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "            # in case our last batch was the tail batch of the dataloader,\n",
    "            # make sure we feed a full batch of noise\n",
    "            # noisev = noise.normal_(0,1)\n",
    "            noisev = torch.randn(args.batch_size, noise_dim)\n",
    "            # print(f\"noise size: {noisev.size()}\")\n",
    "            fake = netG(noisev, X, Y, Y_len)\n",
    "            # fake_tensor = netD(fake)\n",
    "            # output = Variable(loss(fake, torch.ones((32, 6200)).cuda()), requires_grad=True)\n",
    "            # print(fake.size())\n",
    "            target = torch.ones((2405, 2405)).to('cuda')\n",
    "            output = loss(fake.cuda(), target)\n",
    "            output.backward()\n",
    "            print(output.grad)\n",
    "\n",
    "            # errG = fake_tensor\n",
    "            # errG.backward()\n",
    "            G_optimizer_rnn.step()\n",
    "            G_optimizer_output.step()\n",
    "            # # for p in netG.parameters()[0]:\n",
    "            # # netG.all_steps()\n",
    "            # gen_iterations += 1\n",
    "\n",
    "            print(f\"errG for generator: {output.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating max previous node, total iteration: 12000\n",
      "iter 0 times\n",
      "iter 2400 times\n",
      "iter 4800 times\n",
      "iter 7200 times\n",
      "iter 9600 times\n",
      "max previous node: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:   0%|          | 0/35 [00:00<?, ?it/s]/home/jgeng/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2404, 2404])\n",
      "torch.Size([2405, 2405])\n",
      "torch.Size([2405, 2405])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch#1:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args\u001b[39m=\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn[42], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, train_inverter, num_layers, clamp_lower, clamp_upper, lr, betas, lamb, loss_func, device)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(fake\u001b[39m.\u001b[39msize())\n\u001b[1;32m     61\u001b[0m target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((\u001b[39m2405\u001b[39m, \u001b[39m2405\u001b[39m))\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m output \u001b[39m=\u001b[39m loss(fake, target)\n\u001b[1;32m     63\u001b[0m output\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/loss.py:530\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/functional.py:3280\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3279\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m-> 3280\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train(args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
