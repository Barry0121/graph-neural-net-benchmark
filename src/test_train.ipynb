{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.dataset import *\n",
    "from models.discriminator import *\n",
    "from models.generator import *\n",
    "from models.inverter import *\n",
    "from gw_loss import *\n",
    "\n",
    "from models.GAM.src.param_parser import *\n",
    "from models.GAM.src.gam import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from ot.gromov import gromov_wasserstein\n",
    "from models.args import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", UserWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating max previous node, total iteration: 12000\n",
      "iter 0 times\n",
      "iter 2400 times\n",
      "iter 4800 times\n",
      "iter 7200 times\n",
      "iter 9600 times\n",
      "max previous node: 10\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "train_loader, adj_shape = get_dataloader_labels(train_dataset, args)\n",
    "noise_dim = args.hidden_size_rnn\n",
    "\n",
    "GAMachineTrainer = GAMTrainer(args, args.graph_type) # maps from graphs to latent space (of embeddings)\n",
    "gam_optimizer = torch.optim.Adam(GAMachineTrainer.model.parameters(),\n",
    "                                lr=args.learning_rate,\n",
    "                                weight_decay=args.weight_decay)\n",
    "netG = GraphRNN(args=args)\n",
    "optimizer_trainer, G_optimizer_output, G_scheduler_rnn, G_scheduler_output = netG.init_optimizer(lr=0.1)\n",
    "optimizer_generator = None\n",
    "\n",
    "all_data = []\n",
    "for d in train_loader:\n",
    "    all_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "simple = nn.Linear(29, 29)\n",
    "optimizer = optim.SGD(simple.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(args, data):\n",
    "    # TODO: figure out what is \"data\"\n",
    "\n",
    "    ### from Barry's code\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    adj_mat = data['adj_mat']\n",
    "    label = data['label']\n",
    "    Y_len = data['len']\n",
    "    ###\n",
    "\n",
    "    ###############################################################################################\n",
    "    # Embedding (GAMTrainer) Update\n",
    "    ###############################################################################################\n",
    "    \n",
    "    GAMachineTrainer.model.train()\n",
    "    netG.eval()\n",
    "    for e in range(args.embedding_iters):\n",
    "        noise = torch.randn(args.batch_size, args.hidden_size_rnn)\n",
    "        fake_graphs = []\n",
    "        # for b in range(args.batch_size):\n",
    "        fake_graph = netG(noise, X, Y, Y_len) # noise[b, :]\n",
    "        fake_graphs = fake_graph\n",
    "\n",
    "        # print(fake_graph.size())\n",
    "        # print(len(fake_graphs))\n",
    "        # TODO: unpad (a.k.a. pack) fake_graphs\n",
    "\n",
    "        true_graphs = adj_mat # TODO: sample from true dataset, e.g. MUTAG\n",
    "        # print(true_graphs.size())\n",
    "\n",
    "        # print(adj_mat[0])\n",
    "\n",
    "        # from process_batch\n",
    "        optimizer_trainer.zero_grad()\n",
    "        batch_loss = 0\n",
    "        counter = 0\n",
    "        for adj in true_graphs:\n",
    "            batch_loss = GAMachineTrainer.process_graph(\n",
    "                batch_loss=batch_loss, \n",
    "                already_matrix=True, \n",
    "                adj=adj[0:Y_len[counter], 0:Y_len[counter]], target=0 # may be worth passing classes instead of generic \"true\" label\n",
    "            )\n",
    "            counter += 1\n",
    "        counter = 0\n",
    "        for adj in fake_graphs:\n",
    "            # print(adj)\n",
    "            batch_loss = GAMachineTrainer.process_graph(\n",
    "                batch_loss=batch_loss, \n",
    "                already_matrix=True, \n",
    "                adj=adj[0:Y_len[counter], 0:Y_len[counter]], target=1\n",
    "            )\n",
    "            counter += 1\n",
    "        batch_loss.backward(retain_graph=True)\n",
    "        # print(batch_loss)\n",
    "        gam_optimizer.step()\n",
    "        \n",
    "    # ###############################################################################################\n",
    "    # # Generator (GraphRNN) Update\n",
    "    # ###############################################################################################\n",
    "\n",
    "    GAMachineTrainer.model.eval()\n",
    "    netG.train(True)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ### generate fake graphs from noise\n",
    "    noise = torch.randn(args.batch_size, args.hidden_size_rnn)\n",
    "    fake_graphs = []\n",
    "    # for b in range(args.batch_size):\n",
    "    fake_graph = netG(noise, X, Y, Y_len)\n",
    "    fake_graphs = fake_graph\n",
    "    # fake_graphs.append(fake_graph) # 1x8x29x29\n",
    "\n",
    "    # print(len(fake_graphs))\n",
    "    # print(fake_graphs[0].size())\n",
    "    # # TODO: unpad (a.k.a. pack) fake_graphs\n",
    "\n",
    "    ### compute embeddings of fake_graphs, then pass embeddings to generator for reconstruction\n",
    "    # recon_fake_graphs = []\n",
    "    tem_embeddings = []\n",
    "    for fake_adj in fake_graphs:\n",
    "        datadict, features, node = GAMachineTrainer.get_datadict_features_node(fake_adj, target=1) # target unimportant\n",
    "        fake_embedding = GAMachineTrainer.model(\n",
    "            datadict, fake_adj, features, node, get_embedding=True\n",
    "        )\n",
    "        \n",
    "        fake_embedding = fake_embedding.view(-1)\n",
    "        tem_embeddings.append(fake_embedding)\n",
    "    fake_embeddings = torch.stack(tem_embeddings)\n",
    "    recon_fake_graphs = netG(fake_embeddings, X, Y, Y_len)\n",
    "    # recon_fake_graphs.append(recon_graph)\n",
    "\n",
    "    ### do same for true graphs\n",
    "    true_graphs = adj_mat \n",
    "    \n",
    "    # compute embeddings of true_graphs, then pass embeddings to generator for reconstruction\n",
    "    recon_true_graphs = []\n",
    "    true_embnd = []\n",
    "    for true_adj in true_graphs:\n",
    "        datadict, features, node = GAMachineTrainer.get_datadict_features_node(adj, target=0)\n",
    "        true_adj = true_adj + torch.eye(true_adj.size(0), true_adj.size(1))\n",
    "        true_embedding = GAMachineTrainer.model(\n",
    "            datadict, true_adj, features, node, get_embedding=True\n",
    "        )\n",
    "        true_embedding = true_embedding.view(-1)\n",
    "        true_embnd.append(true_embedding)\n",
    "    ftrue_embeddings = torch.stack(true_embnd)\n",
    "    recon_true_graphs = netG(ftrue_embeddings, X, Y, Y_len)\n",
    "    # recon_true_graphs = simple(true_graphs.float()) # TODO\n",
    "    # recon_true_graphs.append(recon_graph)\n",
    "    # print(recon_true_graphs.size())\n",
    "\n",
    "    # output = loss(true_graphs.float(), recon_true_graphs) # TODO\n",
    "    # output.backward() # TODO\n",
    "    # optimizer.step() # TODO\n",
    "\n",
    "    batch_gw_loss = 0\n",
    "    # compute GW distance on true_graphs/fake_graphs and recon_true_graphs/recon_fake_graphs\n",
    "    for adj_o, adj_r in zip(true_graphs, recon_true_graphs):\n",
    "        batch_gw_loss += GWLoss(adj_o, adj_r)\n",
    "    for adj_o, adj_r in zip(fake_graphs, recon_fake_graphs):\n",
    "        batch_gw_loss += GWLoss(adj_o, adj_r)\n",
    "    batch_gw_loss = torch.tensor(batch_gw_loss, requires_grad=True)\n",
    "    batch_gw_loss.backward()\n",
    "    optimizer_trainer.step()\n",
    "    G_optimizer_output.step()\n",
    "\n",
    "    # print(list(netG.parameters()), list(GAMachineTrainer.model.parameters())[0])\n",
    "    print(output.item())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.MSELoss()\n",
    "# simple = nn.Linear(2, 2)\n",
    "# optimizer = optim.SGD(simple.parameters(), lr=0.01, momentum=0.9)\n",
    "# input = torch.randn(2, 2, 2, requires_grad=True)\n",
    "# target = torch.randn(2, 2, 2)\n",
    "# for _ in range(5):\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = simple(input)\n",
    "#     output = loss(pred, target)\n",
    "#     output.backward()\n",
    "#     optimizer.step()\n",
    "#     print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06757685542106628\n",
      "0.06752537935972214\n",
      "0.0674520954489708\n",
      "0.06735934317111969\n",
      "0.06724926829338074\n",
      "0.06712380051612854\n",
      "0.06698474287986755\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m all_data:\n\u001b[0;32m----> 2\u001b[0m     trainAE(args, all_data[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m \u001b[39m# trainAE(args, all_data[0])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 82\u001b[0m, in \u001b[0;36mtrainAE\u001b[0;34m(args, data)\u001b[0m\n\u001b[1;32m     80\u001b[0m tem_embeddings \u001b[39m=\u001b[39m []\n\u001b[1;32m     81\u001b[0m \u001b[39mfor\u001b[39;00m fake_adj \u001b[39min\u001b[39;00m fake_graphs:\n\u001b[0;32m---> 82\u001b[0m     datadict, features, node \u001b[39m=\u001b[39m GAMachineTrainer\u001b[39m.\u001b[39;49mget_datadict_features_node(fake_adj, target\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# target unimportant\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     fake_embedding \u001b[39m=\u001b[39m GAMachineTrainer\u001b[39m.\u001b[39mmodel(\n\u001b[1;32m     84\u001b[0m         datadict, fake_adj, features, node, get_embedding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     87\u001b[0m     fake_embedding \u001b[39m=\u001b[39m fake_embedding\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/DSC180B-PlayGround/graph-neural-net-benchmark/src/models/GAM/src/gam.py:254\u001b[0m, in \u001b[0;36mGAMTrainer.get_datadict_features_node\u001b[0;34m(self, adj, target)\u001b[0m\n\u001b[1;32m    247\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m    248\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m: target, \u001b[39m# label of graph's adj\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: degrees, \u001b[39m# node degree of each node\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minverse_labels\u001b[39m\u001b[39m'\u001b[39m: inv_degrees \u001b[39m# what node degrees correspond to what nodes\u001b[39;00m\n\u001b[1;32m    251\u001b[0m }\n\u001b[1;32m    253\u001b[0m _, features \u001b[39m=\u001b[39m create_features(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39midentifiers, use_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, adj\u001b[39m=\u001b[39madj)\n\u001b[0;32m--> 254\u001b[0m node \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mchoice(nodes)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    255\u001b[0m \u001b[39mreturn\u001b[39;00m data, features, node\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/random.py:346\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_randbelow(\u001b[39mlen\u001b[39;49m(seq))]\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/random.py:243\u001b[0m, in \u001b[0;36mRandom._randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    242\u001b[0m getrandbits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetrandbits\n\u001b[0;32m--> 243\u001b[0m k \u001b[39m=\u001b[39m n\u001b[39m.\u001b[39;49mbit_length()  \u001b[39m# don't use (n-1) here because n can be 1\u001b[39;00m\n\u001b[1;32m    244\u001b[0m r \u001b[39m=\u001b[39m getrandbits(k)  \u001b[39m# 0 <= r < 2**k\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mwhile\u001b[39;00m r \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m n:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for d in all_data:\n",
    "    trainAE(args, all_data[0])\n",
    "# trainAE(args, all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.from_numpy(np.array([[[1,0], [0,0]], [[0,0], [0,0]], [[0,0], [0,0]]]))\n",
    "# i = torch.eye(2,2)\n",
    "# i = torch.unsqueeze(i, 0)\n",
    "# addition = i.expand(3, 2, 2)  \n",
    "# a + addition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
