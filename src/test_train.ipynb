{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.args import *\n",
    "from models.dataset import *\n",
    "from models.discriminator import *\n",
    "from models.generator import *\n",
    "from models.inverter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, num_layers=4, clamp_lower=-0.01, clamp_upper=0.01, epochs=10, lr=1e-3, betas=1e-5, batch_size=1, lamb=0.1, loss_func='MSE', device=choose_device()):\n",
    "    # get the dataset\n",
    "    train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "    train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "    train_loader = get_dataloader_labels(train_dataset, args)\n",
    "    noise_dim = args.hidden_size_rnn\n",
    "    print('noise dimension is: ', noise_dim)\n",
    "\n",
    "    # initialize noise, optimizer and loss\n",
    "    I = Inverter(input_dim=128, output_dim=noise_dim, hidden_dim=64)\n",
    "    G = GraphRNN(args=args)\n",
    "    D = NetD(stat_input_dim=128, stat_hidden_dim=64, num_stat=2)\n",
    "\n",
    "    graph2vec = get_graph2vec(args.graph_type) # use infer() to generate new graph embedding\n",
    "    optimizerI = optim.Adam(I.parameters(), lr=lr)\n",
    "    optimizerD = optim.Adam(D.parameters(), lr=lr, betas=[betas for _ in range(2)])\n",
    "    lossI = WGAN_ReconLoss(lamb, loss_func).to(device)\n",
    "    G.init_optimizer() # initialize optimizers\n",
    "\n",
    "\n",
    "    noise = torch.randn(args.batch_size, noise_dim).to(device)\n",
    "    one = torch.FloatTensor([1])\n",
    "    mone = one * -1\n",
    "\n",
    "    start_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        # for now, treat the input as adj matrices\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            X = data['x']\n",
    "            Y = data['y']\n",
    "            adj_mat = data['adj_mat']\n",
    "            label = data['label']\n",
    "            Y_len = data['len']\n",
    "\n",
    "            # start=time.time()\n",
    "\n",
    "            # print(\"====Start Discriminator \", end='')\n",
    "\n",
    "            # # enable training\n",
    "            # for p in D.parameters(): # reset requires_grad\n",
    "            #     p.requires_grad = True # they are set to False below in netG update\n",
    "\n",
    "            # Diters = 1 # number of iterations to train discriminator TODO: change to 10\n",
    "            # j = 0 # counter for 1, 2, ... Diters\n",
    "            # while j < Diters and i < len(train_loader):\n",
    "            #     j += 1\n",
    "            #     # weight clipping: clamp parameters to a cube\n",
    "            #     for p in D.parameters():\n",
    "            #         p.data.clamp_(clamp_lower, clamp_upper)\n",
    "            #     D.zero_grad()\n",
    "\n",
    "            #     # train with real\n",
    "            #     inputs = torch.empty_like(adj_mat).copy_(adj_mat)\n",
    "            #     # print(inputs.shape)\n",
    "            #     input_graphs = [nx.from_numpy_matrix(i) for i in inputs.detach().numpy()] # TODO: Error raise NetworkXError(f\"Edge tuple {e} must be a 2-tuple or 3-tuple.\")\n",
    "            #     D_pred = torch.Tensor([D(graph) for graph in input_graphs])\n",
    "            #     errD_real = torch.mean(D_pred)\n",
    "            #     # print(errD_real)\n",
    "            #     # errD_real.backward(one) # discriminator should assign 1's to true samples\n",
    "            #     # errD_real.backward()\n",
    "\n",
    "            #     # train with fake\n",
    "            #     input = noise.resize_(args.num_layers, args.batch_size, args.hidden_size_rnn).normal_(0, 1)\n",
    "            #     # insert data processing\n",
    "            #     fake = G.generate(input, args, test_batch_size=args.batch_size)\n",
    "            #     fake_tensor = torch.Tensor([D(nx.from_numpy_matrix(f)) for f in fake])\n",
    "            #     errD_fake = torch.mean(fake_tensor)\n",
    "\n",
    "            #     # print(f\"errD_fake is {errD_fake} for {j}\")\n",
    "            #     # errD_fake.backward(mone) # discriminator should assign -1's to fake samples??\n",
    "            #     # errD_fake.backward()\n",
    "\n",
    "            #     # compute Wasserstein distance and update parameters\n",
    "            #     errD = errD_real - errD_fake\n",
    "            #     errD = Variable(errD, requires_grad = True)\n",
    "\n",
    "            #     # print(f\"errD is {errD} for {j}\")\n",
    "\n",
    "            #     errD.backward()\n",
    "            #     optimizerD.step()\n",
    "\n",
    "            # print(f\"{(time.time()-start)%60}s Finished====\")\n",
    "\n",
    "\n",
    "            print(\"====Start Inverter and Generator \", end='')\n",
    "            G.train()\n",
    "            G.clear_gradient_opts()\n",
    "            G.clear_gradient_models()\n",
    "            I.zero_grad()\n",
    "            istart = time.time()\n",
    "            # graphs\n",
    "            original_graphs = adj_mat.to(device) # shape: (batch_size, padded_size, padded_size); in the case for MUTAG, padded_size is 29\n",
    "            graphs = [nx.from_numpy_matrix(adj.numpy()) for adj in original_graphs]\n",
    "            graphVec = torch.from_numpy(graph2vec.infer(graphs))\n",
    "            I_output = I(graphVec) # TODO: expected shape: (batch_size, 1, max_prev_node)\n",
    "\n",
    "            # print(I_output.size())\n",
    "            \n",
    "            G_pred_graphs = G.generate(X=I_output, args=args, test_batch_size=args.batch_size)\n",
    "            reconst_graphs = torch.from_numpy(np.array(G_pred_graphs)).to(device)  # 0 for prediction, 1 for sorted output\n",
    "            # noise\n",
    "            G_pred_noise = G.generate(X=noise, args=args, test_batch_size=args.batch_size) # shape: (batch_size, padded_size, padded_size)\n",
    "            G_pred_noise = torch.from_numpy(np.array(G_pred_noise))\n",
    "            noise_graphs = [nx.from_numpy_matrix(adj.numpy()) for adj in G_pred_noise]\n",
    "            noise_graphVec = torch.from_numpy(graph2vec.infer(noise_graphs))\n",
    "            reconst_noise = I(noise_graphVec).to(device)\n",
    "            # compute loss and update inverter loss\n",
    "\n",
    "            print(original_graphs.size())\n",
    "            print(reconst_graphs.size())\n",
    "            print(noise.size())\n",
    "            print(reconst_noise.size())\n",
    "\n",
    "            loss = lossI(original_graphs, reconst_graphs, noise, reconst_noise)\n",
    "            optimizerI.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizerI.step()\n",
    "            # compute loss and update generator loss\n",
    "            errG = torch.mean(G(reconst_graphs))\n",
    "            errG.backward()\n",
    "            G.all_steps()\n",
    "            print(f\"{(time.time()-istart)%60}s Finished====\")\n",
    "\n",
    "        # Print out training information.\n",
    "        if (e+1) % 1 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print('Elapsed time [{:.4f}], Iteration [{}/{}], I Loss: {:.4f}, D Loss: {:.4f}, G Loss {:.4f}'.format(\n",
    "                elapsed_time, e+1, epochs, lossI.item(), errD, errG))\n",
    "\n",
    "        # save training loss across\n",
    "    print(\"====End of Training====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "# train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "# train_loader = get_dataloader_labels(train_dataset, args)\n",
    "# adjs = []\n",
    "# for i, data in enumerate(train_loader):\n",
    "#     X = data['x']\n",
    "#     Y = data['y']\n",
    "#     adj_mat = data['adj_mat']\n",
    "#     break\n",
    "# inputs = torch.empty_like(adj_mat).copy_(adj_mat)\n",
    "# inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating max previous node, total iteration: 12000\n",
      "iter 0 times\n",
      "iter 2400 times\n",
      "iter 4800 times\n",
      "iter 7200 times\n",
      "iter 9600 times\n",
      "max previous node: 10\n",
      "noise dimension is:  128\n",
      "======Generating Embedding======\n",
      "======Embedding Created (used 0.6722350120544434 sec)======\n",
      "====Start Inverter and Generator torch.Size([8, 621, 621])\n",
      "torch.Size([8, 621, 621])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args\u001b[39m=\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn[35], line 115\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, num_layers, clamp_lower, clamp_upper, epochs, lr, betas, batch_size, lamb, loss_func, device)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(noise\u001b[39m.\u001b[39msize())\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(reconst_noise\u001b[39m.\u001b[39msize())\n\u001b[0;32m--> 115\u001b[0m loss \u001b[39m=\u001b[39m lossI(original_graphs, reconst_graphs, noise, reconst_noise)\n\u001b[1;32m    116\u001b[0m optimizerI\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    117\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DSC180B-PlayGround/graph-neural-net-benchmark/src/models/inverter.py:74\u001b[0m, in \u001b[0;36mWGAN_ReconLoss.forward\u001b[0;34m(self, x_original, x_reconst, z_original, z_reconst, use_gw1)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mFrom the Generating Natural Adv. Example Paper:\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    x_original: original image/graph example\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    z_reconst: reconstruction of the noise, output of the inverter\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m use_gw1:\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL(x_original, x_reconst) \\\n\u001b[0;32m---> 74\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlamb \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mL(z_original, z_reconst))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:            \n\u001b[1;32m     76\u001b[0m     \u001b[39m# initialize measures for each metric measure network\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     p_o \u001b[39m=\u001b[39m ot\u001b[39m.\u001b[39munif(x_original\u001b[39m.\u001b[39mnumber_of_nodes())\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/loss.py:530\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 530\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/functional.py:3280\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3279\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(\u001b[39minput\u001b[39m, target)\n\u001b[0;32m-> 3280\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train(args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
