{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from models.args import *\n",
    "from models.dataset import *\n",
    "from models.discriminator import *\n",
    "from models.generator import *\n",
    "from models.inverter import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", UserWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_inverter=False, num_layers=4, clamp_lower=-0.1, clamp_upper=0.1, lr=1e-3, betas=1e-5, lamb=0.1, loss_func='MSE', device=choose_device()):\n",
    "    # save losses\n",
    "    iloss_lst = []\n",
    "    dloss_lst = []\n",
    "    gloss_lst = []\n",
    "\n",
    "    # get the dataset\n",
    "    lr = 1e-4\n",
    "    train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "    train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "    train_loader = get_dataloader_labels(train_dataset, args)\n",
    "    noise_dim = args.hidden_size_rnn\n",
    "    print('noise dimension is: ', noise_dim)\n",
    "\n",
    "    # initialize noise, optimizer and loss\n",
    "    netI = Inverter(input_dim=128, output_dim=args.hidden_size_rnn, hidden_dim=64)\n",
    "    netG = GraphRNN(args=args)\n",
    "    netD = NetD(stat_input_dim=128, stat_hidden_dim=64, num_stat=2)\n",
    "    hg = list(netG.parameters())[5].register_hook(lambda grad: print(f\"NetG parameter Update with gradient {grad}\"))\n",
    "\n",
    "    # set up a register_hook to check parameter gradient\n",
    "    # for param in netD.parameters():\n",
    "    #     h = param.register_hook(lambda grad: print(\"Parameter Update with gradient {:.4f}\".format(grad)))\n",
    "\n",
    "    # check model parameters\n",
    "    # for param in netD.parameters():\n",
    "    #     print(param.name, param.data, param.requires_grad)\n",
    "    # for param in netG.parameters():\n",
    "        # print(param.name, param.data, param.requires_grad)\n",
    "\n",
    "    graph2vec = get_graph2vec(args.graph_type, dim=512) # use infer() to generate new graph embedding\n",
    "    optimizerI = optim.Adam(netI.parameters(), lr=lr)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=[betas for _ in range(2)])\n",
    "    lossI = WGAN_ReconLoss(device, lamb, loss_func)\n",
    "    G_optimizer_rnn, G_optimizer_output, G_scheduler_rnn, G_scheduler_output = netG.init_optimizer(lr=0.1) # initialize optimizers\n",
    "\n",
    "\n",
    "    noise = torch.randn(args.batch_size, noise_dim).to(device)\n",
    "    one = torch.tensor(1, dtype=torch.float)\n",
    "    mone = torch.tensor(-1, dtype=torch.float)\n",
    "\n",
    "    gen_iterations = 0\n",
    "    for e in range(args.epochs):\n",
    "        # for now, treat the input as adj matrices\n",
    "        start_time = time.time()\n",
    "        e_errI, e_errD, e_errG, count_batch = 0, 0, 0, 0\n",
    "        for i, data in tqdm(enumerate(train_loader), desc=f\"Training epoch#{e+1}\", total=len(train_loader)):\n",
    "            X = data['x']\n",
    "            Y = data['y']\n",
    "            adj_mat = data['adj_mat']\n",
    "            label = data['label']\n",
    "            Y_len = data['len']\n",
    "\n",
    "            # zero grad\n",
    "            optimizerI.zero_grad()\n",
    "            optimizerD.zero_grad()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "\n",
    "            # skip uneven batch\n",
    "            if adj_mat.size(0) != args.batch_size:\n",
    "                continue\n",
    "\n",
    "            ######################\n",
    "            # Discriminator Update\n",
    "            ######################\n",
    "            # number of iteration to train the discriminator\n",
    "            # if gen_iterations < 25 or gen_iterations % 500 == 0:\n",
    "            #     Diters = 20\n",
    "            # else:\n",
    "            #     Diters = 5\n",
    "            Diters = 1\n",
    "            j = 0 # counter for 1, 2, ... Diters\n",
    "\n",
    "            # enable training\n",
    "            netD.train(True)\n",
    "            netG.train(False)\n",
    "            b_errD = 0\n",
    "            while j < Diters:\n",
    "                j += 1\n",
    "                # TODO: commenting this part out for testing\n",
    "                # weight clipping: clamp parameters to a cube\n",
    "                # for p in netD.parameters():\n",
    "                #     p.data.clamp_(clamp_lower, clamp_upper)\n",
    "                netD.zero_grad()\n",
    "\n",
    "                # train with real\n",
    "                inputs = torch.empty_like(adj_mat).copy_(adj_mat)\n",
    "                D_pred = netD(inputs)\n",
    "                errD_real = D_pred\n",
    "                errD_real.backward(one) # discriminator should assign 1's to true samples\n",
    "                print(j, 'errD_real:', errD_real.item(), end='; ')\n",
    "\n",
    "                # train with fake\n",
    "                input = noise.normal_(0,1) # (batch_size, hidden_size)\n",
    "                # insert data processing\n",
    "                fake = netG(input)\n",
    "                fake_tensor = netD(fake)\n",
    "                errD_fake = fake_tensor\n",
    "                errD_fake.backward(mone) # discriminator should assign -1's to fake samples??\n",
    "\n",
    "                # # # compute Wasserstein distance and update parameters\n",
    "                errD = errD_real - errD_fake\n",
    "\n",
    "                # print(f\"Check if the model is training: iterative value at #{j}.\")\n",
    "                # for p in netD.parameters():\n",
    "                #     print(\"Parameters gradients? :\", p.requires_grad, end='')\n",
    "                #     print(\"Parameters grad: \", p.grad)\n",
    "\n",
    "                optimizerD.step()\n",
    "                print(j, 'errD_fake:', errD_fake.item())\n",
    "                # print(f\"errD_real {errD_real.item()} \")\n",
    "                # print(f\"Iterative errD {errD.item()}, errD_real {errD_real.item()}, errD_fake {errD_fake.item()}: \")\n",
    "                # b_errD += errD\n",
    "            \n",
    "            # ========== Train Generator ==================\n",
    "            netD.train(False)\n",
    "            netG.train(True)\n",
    "            # netG.clear_gradient_models()\n",
    "            G_optimizer_rnn.zero_grad()\n",
    "            G_optimizer_output.zero_grad()\n",
    "            # in case our last batch was the tail batch of the dataloader,\n",
    "            # make sure we feed a full batch of noise\n",
    "            noisev = Variable(noise.normal_(0,1))\n",
    "            fake = netG(noisev)\n",
    "            fake_tensor = netD(fake)\n",
    "            errG = fake_tensor\n",
    "            errG.backward(one)\n",
    "            G_optimizer_rnn.step()\n",
    "            G_optimizer_output.step()\n",
    "            for p in netG.parameters():\n",
    "                print(p[0,0], p.grad)\n",
    "                break\n",
    "            # for p in netG.parameters()[0]:\n",
    "            # netG.all_steps()\n",
    "            gen_iterations += 1\n",
    "\n",
    "            print(f\"errG for generator: {errG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.,  8.,  7.,  6.,  5.,  4.,  3.,  2.,  1.,  0.],\n",
       "        [19., 18., 17., 16., 15., 14., 13., 12., 11., 10.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((5,10))\n",
    "a[0, :] = torch.Tensor(np.array([i for i in range(10)]))\n",
    "a[1, :] = torch.Tensor(np.array([i for i in range(10, 20)]))\n",
    "torch.flip(a, dims=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, labels = get_dataset_with_label(args.graph_type) # entire dataset as train\n",
    "# train_dataset = Graph_sequence_sampler_pytorch(train, labels, args)\n",
    "# train_loader = get_dataloader_labels(train_dataset, args)\n",
    "# e = 0\n",
    "# for i, data in tqdm(enumerate(train_loader), desc=f\"Training epoch#{e+1}\", total=len(train_loader)):\n",
    "#     X = data['x']\n",
    "#     Y = data['y']\n",
    "\n",
    "#     print(X.size(), Y.size())\n",
    "#     # print(X)\n",
    "#     print('-'*50)\n",
    "#     # print(Y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(args\u001b[39m=\u001b[39margs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train(args=args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
