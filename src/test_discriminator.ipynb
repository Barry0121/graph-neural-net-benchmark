{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import *\n",
    "from models.discriminator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.inverter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.discriminator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = get_dataset(\"ENZYMES\")\n",
    "train_loader = get_dataloader(train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 126, 125])\n",
      "torch.Size([64, 126, 125])\n",
      "tensor([44, 46, 20,  8, 42, 88, 20, 36, 26, 18, 38, 23, 30, 47, 29, 19, 18, 33,\n",
      "        18, 21, 42, 14, 46, 32, 19, 36, 25, 11, 39, 33, 25, 40, 33, 15, 36, 74,\n",
      "        34, 22, 39, 42, 27, 39, 32, 17, 40, 41, 26, 20, 48, 42, 66, 20, 27, 20,\n",
      "        18, 42, 36, 19, 42, 28, 23, 14, 19, 12])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    # print(data.keys())\n",
    "    X = data['x']\n",
    "    Y = data['y']\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    print(data['len'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = Inverter(input_dim=512, output_dim=8, hidden_dim=256)\n",
    "D = NetD(stat_input_dim=128, stat_hidden_dim=64, num_stat=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerI = optim.Adam(I.parameters(), lr=1e-3)\n",
    "lossI = WGAN_ReconLoss(0.1, 'MSE')\n",
    "optimizerD = optim.Adam(D.parameters(), lr=1e-3, betas=[1e-5 for _ in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'degree'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m Y\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     22\u001b[0m     \u001b[39m# insert data processing\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m errD_real \u001b[39m=\u001b[39m D(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m errD_real\u001b[39m.\u001b[39mbackward(one) \u001b[39m# discriminator should assign 1's to true samples\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# train with fake\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DSC180B-PlayGround/graph-neural-net-benchmark/src/models/discriminator.py:62\u001b[0m, in \u001b[0;36mNetD.forward\u001b[0;34m(self, G)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39mparam G: the input graph\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m# when computing univariate statistics for x, pass each through an individual NN with\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m# tanh as final activation\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# use a NN with tanh as final activation to combine scores\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# possible modifications:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# 1) use torch.histogram instead of np.histogram?\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# 2) add lots more statistics, these probably aren't enough to characterize a graph\u001b[39;00m\n\u001b[1;32m     61\u001b[0m degree_hist, _ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhistogram(\n\u001b[0;32m---> 62\u001b[0m     np\u001b[39m.\u001b[39marray(nx\u001b[39m.\u001b[39;49mdegree_histogram(G)),\n\u001b[1;32m     63\u001b[0m     bins\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstat_input_dim, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m(\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m), density\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m degree_hist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstat_NNs[\u001b[39m0\u001b[39m](degree_hist)\n\u001b[1;32m     66\u001b[0m clustering_coefs, _ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhistogram(\n\u001b[1;32m     67\u001b[0m     \u001b[39mlist\u001b[39m(nx\u001b[39m.\u001b[39mclustering(G)\u001b[39m.\u001b[39mvalues()),\n\u001b[1;32m     68\u001b[0m     bins\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstat_input_dim, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m(\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m), density\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/dev180/lib/python3.9/site-packages/networkx/classes/function.py:147\u001b[0m, in \u001b[0;36mdegree_histogram\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdegree_histogram\u001b[39m(G):\n\u001b[1;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a list of the frequency of each degree value.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m    (Order(number_of_edges))\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     counts \u001b[39m=\u001b[39m Counter(d \u001b[39mfor\u001b[39;00m n, d \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39;49mdegree())\n\u001b[1;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m [counts\u001b[39m.\u001b[39mget(i, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mmax\u001b[39m(counts) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'degree'"
     ]
    }
   ],
   "source": [
    "noise = torch.randn(64, 8).to('cuda')\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1\n",
    "\n",
    "for i, data in enumerate(train_loader):\n",
    "    X = data['x'].numpy()\n",
    "    Y = data['y'].numpy()\n",
    "    for p in D.parameters(): # reset requires_grad\n",
    "        p.requires_grad = True\n",
    "    Diters = 10 # number of iterations to train discriminator\n",
    "    j = 0 # counter for 1, 2, ... Diters\n",
    "    while j < Diters and i < len(train_loader):\n",
    "        j += 1\n",
    "        # weight clipping: clamp parameters to a cube\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        D.zero_grad()\n",
    "\n",
    "        # train with real\n",
    "        print(type(Y))\n",
    "        input = Y.copy()\n",
    "            # insert data processing\n",
    "        errD_real = D(input)\n",
    "        errD_real.backward(one) # discriminator should assign 1's to true samples\n",
    "\n",
    "        # train with fake\n",
    "        input = noise.resize_(64, 1).normal_(0, 1)\n",
    "            # insert data processing\n",
    "        fake = D(input)\n",
    "        errD_fake = D(fake)\n",
    "        errD_fake.backward(mone) # discriminator should assign -1's to fake samples??\n",
    "\n",
    "        # compute Wasserstein distance and update parameters\n",
    "        errD = errD_real - errD_fake\n",
    "        optimizerD.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev180",
   "language": "python",
   "name": "dev180"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a3a2ecaabf27beca2883a9dba2b6875b07db1b2c8ad2f920ab70c4c2e7a16bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
